---
title: "5. How lidR functions map to lasR functions"
author: "Jean-Romain Roussel"
output:
  html_document:
    toc: true
    toc_float:
      collapsed: false
      smooth_scroll: false
    toc_depth: 2
vignette: >
  %\VignetteIndexEntry{5. How lidR functions map to lasR functions}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup, echo = FALSE}
library(lasR)
```


`lasR` contains exclusively low-level functions. It means that there are no functions such as `dtm()`, `chm()`, `clip()`, or any convenient functions with explicit names that perform one single expected task. The addition of, at least, a reader stage is mandatory. The concept of pipelines with low-level functions is highly versatile, and setting pre-built functions is sub-optimal and restricts usages (see later for more details).

This vignette presents a list of convenient functions that mimic `lidR` functions while not being strictly equivalent. Yet, it is usually more interesting to **NOT** use these functions and rather use pre-built pipelines. See also the pipelines vignettes with a list of possible pipelines. This vignette is more like a tutorial on how to develop and program tools with `lasR` than a tutorial on how to use `lasR` like `lidR`.

## add_attribute

In `lidR`, the function `add_attribute` enables adding a column to the `data.frame` so that this column is writable in a LAS file. Since there is no `data.frame` in `lasR`, and the package processes on-disk files and produces on-disk files, there is no one-to-one equivalent to `add_attribute()`. The closest equivalent function would be something like:


```{r}
add_dimension = function(ifile, data_type, name, description, ofile = tempfile(fileext = ".las"))
{
  pipeline <- reader(f) + add_extrabytes(data_type, name, description) + write_las(ofile)
  processor(pipeline)
}
```

```{r, echo = FALSE}
f <- system.file("extdata", "Example.las", package="lasR")
res = add_dimension(f, "int", "TEST", "A simple test")
```

It adds an `extrabytes` attribute, but the attribute is zeroed. The idea is that these attributes can be used in later stages, but as is, the pre-built function does not have other stages.

## catalog_apply

There is no equivalent since `catalog_apply()` and `catalog_map()` functions are designed to create pipelines in `lidR`. `lasR` *IS* a pipeline engine and does not need such functions. The pipeline mechanism in `lidR` is more difficult to use.

## catalog_retile

There is currently no full equivalent because `lasR` works by files and does not create arbitrary chunks. Yet, `catalog_retile()` is very versatile, and some behaviours can be reproduced, such as writing buffered tiles.

```{r}
buffer_tiles = function(f, buffer, ofiles = paste0(tempdir(), "/*_buffered.las"))
{
  read = reader(f, buffer = buffer)
  write = write_las(ofiles, keep_buffer = TRUE)
  processor(read+write)
}
```

```{r, echo = FALSE}
f <- system.file("extdata", "bcts/", package="lasR")
res <- buffer_tiles(f, 50)
ctg <- processor(reader(res) + boundaries())
#plot(ctg)
```

## classify_ground

`lasR` does not include any built-in ground classification algorithm yet, but the `callback` stage can be used to leverage functions from other R packages. `lidR` has `csf()` from the package `RCSF` and `mcc()` from the package `RMCC`.

```{r, eval=FALSE}
classify_ground_csf = function(
    files, 
    smooth = FALSE, 
    threshold = 0.5, 
    resolution = 0.5, 
    rigidness = 1L, 
    iterations = 500L, 
    step = 0.65, 
    ofiles = paste0(tempdir(), "/*_classified.las"))
{
  csf = function(data, smooth, threshold, resolution, rigidness, iterations, step)
  {
    id = RCSF::CSF(data, smooth, threshold, resolution, rigidness, iterations, step)
    class = integer(nrow(data))
    class[id] = 2L
    data$Classification <- class
    return(data)
  }
  
  read = reader(files, buffer = 25)
  classify = callback(csf, expose = "xyz", smooth = smooth, threshold = threshold, resolution = resolution, rigidness = rigidness, iterations = iterations, step = step)
  write = write_las(ofiles)
  pipeline = read + classify + write
  processor(pipeline)
}
```

```{r, eval=FALSE, echo=FALSE}
f <- system.file("extdata", "Topography.las", package="lasR")
ans = classify_ground_csf(f)
```

## classify_noise

```{r}
classify_noise_ivf = function(files, res =  5, n = 6, ofiles = paste0(tempdir(), "/*_classified.las"))
{
  processor(reader(files) + classify_isolated_points(res, n) +  write_las(ofile))
}
```

Here, it is worth noting that with pre-built functions, it is not possible to perform ground classification and noise classification, while with a self-built pipeline, it is possible to combine multiple stages in the pipeline.

```r
read = reader(f) 
ground = callback(csf, expose = "xyz", threshold = 0.4, iterations = 200)
noise = classify_isolated_points()
write = write_las()
pipeline = read + ground + noise + write  
```

## classify_poi

No current equivalent

## clip

Here, the function is implemented using the `sf` package and supports multiple options, such as returning in R memory or writing in files.

```{r}
clip_circle = function(files, geometry, radius, ofiles = paste0(tempdir(), "/*_clipped.las"))
{
  if (sf::st_geometry_type(geometry, FALSE) != "POINT") stop("Expected POINT geometry type")

  coordinates <- sf::st_coordinates(geometry)
  xcenter <- coordinates[,1]
  ycenter <- coordinates[,2]
  
  read = reader(files, xc = xcenter, yc = ycenter, r = radius)
  
  if (length(ofiles) == 1L && ofiles == "")
    stage = callback(function(data) { return(data) }, expose = "*", no_las_update = T)
  else
    stage = write_las(ofiles)
  
  ans = processor(read+stage)
  return(ans)
}
```

```{r, echo=FALSE, results='hide'}
f = paste0(system.file(package="lasR"), "/extdata/bcts/")
f = list.files(f, pattern = "(?i)\\.la(s|z)$", full.names = TRUE)
f = f[1:2]

x = c(885100, 885100)
y = c(629200, 629600)
df = data.frame(x,y)
geom = sf::st_as_sf(df, coords = c("x", "y"))
r = 20
clip_circle(f, geom, radius = 10)
clip_circle(f, geom, radius = 10, ofiles = "")
```

## decimate_points

```{r}
decimate_with_voxels = function(f, res = 2, ofiles = paste0(tempdir(), "/*_decimated.las"))
{
  read = reader(f)
  sample = sampling_voxel(res)
  write = write_las(ofiles)
  processor(read+sample+write)
}
```

```{r}
decimate_with_pixel = function(f, res = 2, ofiles = paste0(tempdir(), "/*_decimated.las"))
{
  read = reader(f)
  sample = sampling_pixel(res)
  write = write_las(ofiles)
  processor(read+sample+write)
}
```

## filter

`lasR` does not parse R expression such as `ReturnNumber != x` thus one must use `LASlib` filters. Each stage of the pipeline has it own filter. See `?lasR::filters`.

## filter_duplicates

`lasR` added a `-drop_duplicates` command to LASlib.

```{r}
read = reader(f, filter = drop_duplicates())
```

## las_check

There is currently no implementation but this will be added later

## locate_trees

```{r}
tree_tops = function(f, ...)
{
  read = reader(f)
  ttops = local_maximum(...)
  processor(read+ttops)
}
```

```{r, echo=FALSE}
f <- system.file("extdata", "Megaplot.las", package="lasR")
ttops = tree_tops(f, ws = 20)
```

## normalize_height

This implementation leaves the choice to use extrabytes attributes to store the height above ground.

```{r}
normalize_elevation = function(f, extrabytes = FALSE, ofiles = paste0(tempdir(), "/*_normalized.las"))
{
  read = reader(f)
  tri = triangulate()
  
  pipeline = read + tri
  
  if (extrabytes)
  {
    extra = add_extrabytes("int", "HAG", "Height Above Ground")
    norm = transform_with_triangulation(tri, store_in_attribute = "HAG")
    pipeline = pipeline + extra + norm
  }
  else
  {
    norm = transform_with_triangulation(tri)
    pipeline = pipeline + norm
  }
  
  pipeline = pipeline + write_las(ofiles)
  processor(pipeline)
}
```

## pixel_metrics

`lasR` has `rasterize()`, which is the exact equivalent to `pixel_metrics()`. Under the hood, it uses `aggregate()`. Due to non-standard evaluation, this function is a bit trickier to use inside a function.


```{r}
raster_metrics = function(f, res, fun)
{
  call = substitute(fun)
   env = new.env(parent=parent.frame())
  read = reader(f)
  rast = lasR:::aggregate_q(res, call, filter = "", ofile = tempfile(fileext = ".tif"), env = env)
  processor(read+rast)
}
```

```{r, echo=FALSE}
f <- system.file("extdata", "Megaplot.las", package="lasR")
ans <- raster_metrics(f, 2, mean(Z))
```

## plot_metrics

This computes custom metrics per inventory plots.

```{r}
circle_metrics = function(f, xcenter, ycenter, radius, fun)
{
  read = reader(f, xc = xcenter, yc = ycenter, r = radius)
  metrics = callback(fun, expose = "*")
  processor(read+metrics)
}
```


```{r, echo=FALSE, results='hide'}
f = paste0(system.file(package="lasR"), "/extdata/bcts/")
f = list.files(f, pattern = "(?i)\\.la(s|z)$", full.names = TRUE)
f = f[1:2]
x = c(885100, 885100)
y = c(629200, 629600)
r = 20
fun = function(data) { list(npoint = length(data$X), si = sum(data$Intensity)) }
circle_metrics(f, x,y,r,fun)
```

## rasterize_canopy

```{r}
rasterize_chm_p2r = function(f, res)
{
  read = reader(f)
  chm = rasterize(res, "max")
  return(processor(read+chm))
}

rasterize_chm_tin = function(f, res)
{
  read = reader(f)
  tin = triangulate()
  chm = rasterize(res, tin)
  return(processor(read+tin+chm))
}
```

Here, it is interesting to notice the limitation of a pre-built function. If the dataset is not normalized, this pipeline builds a Digital Elevation Model (similar to `lidR::rasterize_canopy()`). There is no way to obtain a Canopy Height Model, just like in `lidR`. However, by using only low-level functions to build a custom pipeline, we can normalize on-the-fly with a pipeline.

## rasterize_terrain

```{r}
rasterize_dtm = function(f, res)
{
  read = reader(f)
  tri = triangulate(filter = keep_ground())
  dtm = rasterize(res, tri)
  return(processor(read+tri+dtm))
}
```

## rasterize_density

```{r}
rasterize_chm_p2r = function(f, res)
{
  read = reader(f)
  den = rasterize(res, "count")
  return(processor(read+den))
}
```


## readLAS

```{r}
read_las = function(f, select = "xyzi", filter = "")
{
  load = function(data) { return(data) }
  read = reader(f)
  call = callback(load, expose = select, filter, no_las_update = TRUE)
  return(processor(read+call))
}
```





