---
title: "2. Tutorial"
author: "Jean-Romain Roussel"
output:
  html_document:
    toc: true
    toc_float:
      collapsed: false
      smooth_scroll: false
    toc_depth: 3
vignette: >
  %\VignetteIndexEntry{2. Tutorial}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup, echo = FALSE}
suppressPackageStartupMessages(library(lasR))
col = grDevices::colorRampPalette(c("blue", "cyan2", "yellow", "red"))(15)
print.list <- function(my_list, indent = 0) {
  if (is.null(names(my_list))) {
    for (element in my_list) {
      if (is.list(element)) {
        cat(rep(" ", indent), "- List:\n")
        print.list(element, indent + 1)
      } else {
        cat(rep(" ", indent), "-", element, "\n")
      }
    }
  } else {
    for (i in seq_along(my_list)) {
      name = names(my_list)[i]
      cat(rep("  ", indent), "-", name, ": ")
      if (is.list(my_list[[i]])) {
        cat("\n")
        print.list(my_list[[i]], indent + 1)
      } else {
        cat(my_list[[i]], "\n")
      }
    }
  }
}
```

This tutorial is intended to be read in order. It introduces the available tools of the package in a specific order so that the reader can discover all the features of the package organically.

<blockquote style="background-color: #d6e9f9; border-left: 5px solid #428bca; padding: 10px;font-size: 14px; border-radius: 5px;">
In the following tutorial, the variable `f` refers to one or several file paths stored in a vector. It can also be the path to a directory or the path of a  [virtual point cloud](https://www.lutraconsulting.co.uk/blog/2023/06/08/virtual-point-clouds/). In this tutorial, the output is rendered with one or two small LAS files, but every pipeline is designed to process numerous files covering a large extent.
</blockquote>

## Overall functionality

In `lasR`, the R functions provided to the user are not designed to process the data directly; instead, they are used to create a pipeline. A pipeline consists of stages that are applied to a point cloud. Each stage can either transform the point cloud within the pipeline without generating any output or process the point cloud to produce an output.

In the figure below, there are 4 LAS/LAZ files and a pipeline that (1) reads a file, (2) builds and writes a DTM on disk, (3) transforms the point cloud by normalizing the elevation, (4) builds a canopy height model using the transformed point cloud, and (5) transforms the point cloud by removing points below 5 m. The resulting version of the point cloud (points above 5m) is discarded and lost because there is no additional stage in this pipeline. However, other stages can be added, such as the application of a predictive model for points above 5 m or a stage that writes the point cloud to disk.

Once the first file completes the entire pipeline, the second file is used, and the pipeline is applied to fill in the missing parts of the geospatial rasters or vectors produced by the pipeline. Each file is loaded with a buffer from neighboring files if needed.

A pipeline created from the R interface does nothing initially. After building the pipeline, users must call the `processor()` function on it to initiate the computation.

![](pipeline.png)

## Reader

The `reader` stage MUST be the first stage of any pipeline (blue in the figure above). It takes as input the list of files to process. When creating a pipeline with only this stage, the header of the files are read, but no computation is actually applied. The points are not even read because there is no other stage in the pipeline that require to read the points. No result is returned.

```{r, echo = F}
f = paste0(system.file(package="lasR"), "/extdata/bcts")
```

```{r reader}
read = reader(f)
processor(read)
```

## Triangulate

The first stage we can try is `triangulate()`. This algorithm performs a Delaunay triangulation on the points of interest. Triangulating points is a very useful task that is employed in numerous processing tasks. Triangulating all points is not very interesting, so we usually want to use the `filter` argument to triangulate only specific points of interest.

In the following example, we triangulate the points classified as 2 (i.e., ground). This produces a meshed Digital Terrain Model.

In this example, the files are read sequentially, with points loaded one by one and stored to build a Delaunay triangulation. In `lasR`, only one file is stored in memory at a time. The program stores the point cloud and the Delaunay triangulation for the current processing file. Then the data are discarded to load a new file.

If the users do not provide a path to an output file to store the result, the result is lost. In the following pipeline, we are building a triangulation of the ground points, but we get no output because everything is lost.

```{r, echo = FALSE}
f <- system.file("extdata", "Topography.las", package="lasR")
```

```{r triangulate1}
read = reader(f)
del = triangulate(filter = keep_ground())
ans = processor(read+del)
ans
```

In the following pipeline the triangulation is stored in a geopackage file by providing an argument `ofile`:

```{r triangulate2, fig.width=5, fig.height=5}
read = reader(f)
del = triangulate(filter = keep_ground(), ofile = tempfile(fileext = ".gpkg"))
ans = processor(read+del)
ans

par(mar = c(2, 2, 1, 1))
plot(ans, axes = T, lwd = 0.5)
```

We can also triangulate the first returns. This produce a meshed Digital Surface Model.

```r
read = reader(f)
del = triangulate(filter = keep_first(), ofile = tempfile(fileext = ".gpkg"))
ans = processor(read+del)
```

We can also perform both triangulations in the same pipeline. The idea of `lasR` is to execute all the tasks in one pass using a pipeline:

```r
read = reader(f)
del1 = triangulate(filter = keep_ground(), ofile = tempfile(fileext = ".gpkg"))
del2 = triangulate(filter = keep_first(), ofile = tempfile(fileext = ".gpkg"))
pipeline = read + del1 + del2
ans = processor(pipeline)
```

Using `triangulate()` without any other stage in the pipeline is usually not very useful. Typically, `triangulate()` is employed without the `ofile` argument as an intermediate step. For instance, it can be used with `rasterize()`.

## Rasterize

`rasterize()` does exactly what users may expect from it and even more. There are three variations:

1. Rasterize with predefined operators. The operators are optimized internally, making the operations as fast as possible, but the number of registered operators is low.
2. Rasterize by injecting a user-defined R expression. This is equivalent to `pixel_metrics()` from the package `lidR`. Any user-defined function can be mapped, making it extremely versatile but slower and limited in speed by the quality of the R code injected.
3. Rasterize a Delaunay triangulation.

With these variations, users can build a CHM, a DTM, a predictive model, or anything else. 

### Rasterize - triangulation

Let's build a DTM using a triangulation of the ground points and the `rasterize()` stage. In the following pipeline, the LAS files are read, points are loaded for each LAS file **with a buffer**, a Delaunay triangulation of the ground points is built, and then the triangulation is interpolated and rasterized. By default, `rasterize()` writes the raster in a temporary file, so the result is not discarded.

```{r rasterize}
read = reader(f)
del = triangulate(filter = keep_ground())
dtm = rasterize(1, del)
pipeline = read + del + dtm
ans = processor(pipeline)
ans
```

Here, `processor()` returns a only a `SpatRaster` because `triangulate()` returns nothing (`NULL`). Therefore, the pipeline contains two elements, but only one returns something.

```{r plotdtm, fig.height=3.5}
terra::plot(ans, col = gray.colors(25,0,1), mar = c(1, 1, 1, 3))
```

Notice that, contrary to the lidR package, there is usually no high-level function with names like `rasterize_terrain()`. Instead, `lasR` is made up of low-level functions that are more versatile but also more challenging to use. 

### Rasterize - internal metrics

Let's build two CHMs: one based on the highest point per pixel with a resolution of 2 meters, and the second based on the triangulation of the first returns with a resolution of 50 cm.

In the following pipeline, we are using two variations of `rasterize()`: one capable of rasterizing a triangulation and the other capable of rasterizing the point cloud with a predefined operator (here `max`). The output is a named `list` with two `SpatRaster`.

```{r, echo = FALSE}
f <- system.file("extdata", "Megaplot.las", package="lasR")
```

```{r rasterize2, fig.show="hold", fig.height=6,  out.width="50%"}
read <- reader(f)
del <- triangulate(filter = keep_first())
chm1 <- rasterize(2, "max")
chm2 <- rasterize(0.5, del)
pipeline <- read + del + chm1 + chm2
ans <- processor(pipeline)

terra::plot(ans[[1]], mar = c(1, 1, 1, 3), col = col)
terra::plot(ans[[2]], mar = c(1, 1, 1, 3), col = col)
```

<blockquote style="background-color: #d6e9f9; border-left: 5px solid #428bca; padding: 10px;font-size: 14px; border-radius: 5px;">
For simplicity the package has pre-installed pipelines named `chm()` and `dtm()` that do what is explained above.
</blockquote>

### Rasterize - custom metrics

Last but not least, let's compute the map of the median intensity by injecting a user-defined expression. Like in `lidR`, the attributes of the point cloud are named: `X`, `Y`, `Z`, `Intensity`, `gpstime`, `ReturnNumber`, `NumberOfreturns`, `Classification`, `UserData`, `PointSourceID`, `R`, `G`, `B`, `NIR`. For users familiar with the `lidR` package, note that there is no `ScanAngleRank/ScanAngle`;  instead the scanner angle is always named `ScanAngle` and is numeric. Also flags are named `Withheld`, `Synthetic` and `Keypoint`.

```{r rasterize3, fig.height=3.5}
read = reader(f)
avgi = rasterize(10, median(Intensity))
pipeline = read + avgi
ans = processor(pipeline)

terra::plot(ans, mar = c(1, 1, 1, 3), col = heat.colors(15))
```

### Rasterize - buffered

<blockquote style="background-color: #d6e9f9; border-left: 5px solid #428bca; padding: 10px;font-size: 14px; border-radius: 5px;">
The `lasR` package introduced the concept of a buffered area-based approach to enhance the resolution of prediction maps. However, this concept is not covered in detail in this tutorial. For further information, readers can refer to the [dedicated article](baba.html)
</blockquote>

## Transform with

Another way to use a Delaunay triangulation is to transform the point cloud. Users can add or subtract the triangulation from the point cloud, effectively normalizing it. Unlike the `lidR` package, there is no high-level function with names like `normalize_points()`. Instead, `lasR` is composed of low-level functions that offer more versatility.

Let's normalize the point cloud using a triangulation of the ground points (meshed DTM).

In the following example, the triangulation is used by `transform_with()` that modifies the point cloud in the pipeline. Both `triangulate()` and `transform_with()` return nothing. The output is `NULL`.

```{r, echo = FALSE}
f <- system.file("extdata", "Topography.las", package="lasR")
```

```{r twt, warning=FALSE}
read = reader(f)
del = triangulate(filter = keep_ground())
norm = transform_with(del, "-")
pipeline = read + del + norm
ans = processor(pipeline)
ans
```

<blockquote style="background-color: #d6e9f9; border-left: 5px solid #428bca; padding: 10px;font-size: 14px; border-radius: 5px;">
For convenience this pipeline is pre-recorded in the package under the name `normalize()`.
</blockquote>

<blockquote style="background-color: #d6e9f9; border-left: 5px solid #428bca; padding: 10px;font-size: 14px; border-radius: 5px;">
`transform_with()` can also transform with a raster. This is not presented in this tutorial.
</blockquote>


To obtain a meaningful output, it is necessary to chain another stage. Here the point cloud has been modified but then, it is discarded because we did nothing with it. For instance, we can compute a Canopy Height Model (CHM) on the normalized point cloud. In the following pipeline, the first rasterization (`chm1`) is applied before normalization, while the second rasterization occurs after `transform_with()`, thus being applied to the transformed point cloud.

```{r twt2, warning=FALSE, fig.height=4, fig.width=8}
read = reader(f)
del = triangulate(filter = keep_ground())
norm = transform_with(del, "-")
chm1 = rasterize(2, "max")
chm2 = rasterize(2, "max")
pipeline = read + chm1 + del + norm + chm2
ans = processor(pipeline)

col = grDevices::colorRampPalette(c("blue", "cyan2", "yellow", "red"))(15)
terra::plot(c(ans[[1]], ans[[2]]), col = col)
```

After performing normalization, users may want to write the normalized point cloud to disk for later use. In this case, you can append the `write_las()` stage to the pipeline.

## Write LAS

`write_las()` can be called at any point in the pipeline. It writes one file per input file, using the name of the input files with added prefixes and suffixes. In the following pipeline, we read the files, write only the ground points to files named after the original files with the suffix `_ground`, perform a triangulation on the entire point cloud, followed by normalization. Finally, we write the normalized point cloud with the suffix `_normalized`.

```{r, echo = FALSE}
f = paste0(system.file(package="lasR"), "/extdata/bcts")
f = list.files(f, pattern = "(?i)\\.la(s|z)$", full.names = TRUE)
f = f[1:2]
```

```{r writelas, warning=FALSE}
read = reader(f)
write1 = write_las(paste0(tempdir(), "/*_ground.laz"), filter = keep_ground())
write2 = write_las(paste0(tempdir(), "/*_normalized.laz"), )
del = triangulate(filter = keep_ground())
norm = transform_with(del, "-")
pipeline = read + write1 + del + norm + write2
ans = processor(pipeline)
ans
```

It is crucial to include a wildcard `*` in the file path; otherwise, a single large file will be created. This behaviour may be intentional. Let's consider creating a file merge pipeline. In the following example, no wildcard `*` is used for the names of the LAS/LAZ files. The input files are read, and points are sequentially written to the single file `dataset_merged.laz`, naturally forming a merge pipeline.


```{r, echo = FALSE}
f <- system.file("extdata", "Example.las", package="lasR")
f = c(f,f)
```

```{r writelas2}
read = reader(f)
ofile = paste0(tempdir(), "/dataset_merged.laz")
merge = write_las(ofile)
ans = processor(read + merge)
ans
```

## Callback

The `callback` stage holds significant importance as the second and last entry point to inject R code into the pipeline, following `rasterize()`. For those familiar with the `lidR` package, the initial step often involves reading data with `lidR::readLAS()` to expose the point cloud as a `data.frame` object in R. In contrast, `lasR` loads the point cloud optimally in C++ without exposing it directly to R. However, with `callback`, it becomes possible to expose the point cloud as a `data.frame` for executing specific R functions.

Similar to `lidR`, the attributes of the point cloud in `lasR` are named: `X`, `Y`, `Z`, `Intensity`, `gpstime`, `ReturnNumber`, `NumberOfreturns`, `Classification`, `UserData`, `PointSourceID`, `R`, `G`, `B`, `NIR`. Notably, for users accustomed to the `lidR` package, the scanner angle is consistently named `ScanAngle` and is numeric, as opposed to `ScanAngleRank/ScanAngle`. Additionally, flags are named `Withheld`, `Synthetic`, and `Keypoint`.

Let's delve into a simple example. For each LAS file, the `callback` loads the point cloud as a `data.frame` and invokes the `meanz()` function on the `data.frame`.

```{r, echo = FALSE}
f <- c(system.file("extdata", "Topography.las", package="lasR"), system.file("extdata", "Megaplot.las", package="lasR"))
```

```{r callback}
meanz = function(data){ return(mean(data$Z)) }
read = reader(f)
call = callback(meanz, expose = "xyz")
ans = processor(read+call)
print(ans)
```
Here the output is a `list` with two elements because we processed two files (`f` is not displayed in this document). The average Z elevation are respectively 809.08 and 13.27 in each file.

<blockquote style="background-color: #fff3cd; border-left: 5px solid #ffc107; padding: 10px; font-size: 14px; border-radius: 5px;">
Be mindful that, for a given LAS/LAZ file, the point cloud may contain more points than the original file **if the file is loaded with a buffer**. Further clarification on this matter will be provided later.
</blockquote>

The `callback` function is versatile and can also be employed to edit the point cloud. When the user-defined function returns a `data.frame` with the same number of rows as the original one, the function edits the underlying C++ dataset. This enables users to perform tasks such as assigning a class to a specific point. While physically removing points is not possible, users can flag points as `Withheld`. In such cases, these points will not be processed in subsequent stages


```{r callback2}
edit_points = function(data)
{
  data$Classification[5:7] = c(2L,2L,2L)
  data$Withheld = FALSE
  data$Withheld[12] = TRUE
  return(data)
}

read = reader(f)
call = callback(edit_points, expose = "xyzc")
ans = processor(read+call)
ans
```

As observed, here, this time `callback` does not explicitly return anything; however, it edited the point cloud internally. To generate an output, users must use another stage such as `write_las()`. It's important to note that `write_las()` will **NOT** write the point number 12 which is flagged `withheld`. Neither any subsequent stage will process it. The point is still in memory but is discarded.

<blockquote style="background-color: #f8d7da; border-left: 5px solid #dc3545; padding: 10px; font-size: 14px; border-radius: 5px;">
For memory and efficiency reasons, it is not possible to physically remove a point from the underlying memory in `lasR`. Instead, the points flagged as `withheld` will **never be processed**. One consequence of this, is that points flagged as withheld in a LAS/LAZ file **will not** be processed in `lasR`. This aligns with the intended purpose of the flag according to the LAS specification but may differ from the default behaviour of many software on the market including `lidR`.
</blockquote>

Now, let's explore the capabilities of `callback` further. First, let's create a lidR-like `read_las()` function to expose the point cloud to R. In the following example, the user-defined function is employed to return the `data.frame` as is. When the user's function returns a `data.frame` with the same number of points as the original dataset, this updates the points at the C++ level. Here, we use `no_las_update = TRUE` to explicitly return the result.


```{r callback3}
read_las = function(f, select = "xyzi", filter = "")
{
  load = function(data) { return(data) }
  read = reader(f, filter = filter)
  call = callback(load, expose = select, no_las_update = TRUE)
  return (processor(read+call))
}

f <- system.file("extdata", "Topography.las", package="lasR")
las = read_las(f)
head(las)
```

Ground points can also be classified using an R function, such as the one provided by the `RCSF` package:

```r
csf = function(data)
{
  id = RCSF::CSF(data)
  class = integer(nrow(data))
  class[id] = 2L
  data$Classification <- class
  return(data)
}

read = reader(f)
classify = callback(csf, expose = "xyz")
write = write_las()
pipeline = read + classify + write
processor(pipeline)
```

<blockquote style="background-color: #fff3cd; border-left: 5px solid #ffc107; padding: 10px; font-size: 14px; border-radius: 5px;">
`callback()` exposes the point cloud as a `data.frame`. This is the only way to present point clouds to users in a manageable way. One of the reasons why `lasR` is more memory-efficient and faster than `lidR` is that it **does not** expose the point cloud as a `data.frame`. Thus, the pipelines using `callback()` are not significantly different from `lidR`. The advantage of using `lasR` here is the ability to pipe different stages.
</blockquote>

## Tree Segmentation

This section presents a complex pipeline for tree segmentation using `local_maximum()` to identify tree tops, `region_growing()` to segment the trees using the seeds produced by `local_maximum()`, and a Canopy Height Model (CHM) produced using Delaunay triangulation of first returns. The CHM is post-processed with `pit_fill()`, an algorithm designed to enhance the CHM by filling pits and NAs. In this tutorial, the pipeline is tested on one file to render the page faster. However, this pipeline can be applied to any number of files and will produce a contiguous output, managing the buffer between files. Every intermediate output can be exported, and in this tutorial, we export everything to display all the outputs.

```{r, echo = FALSE}
f <- system.file("extdata", "MixedConifer.las", package="lasR")
```

```{r its}
read = reader(f)
del = triangulate(filter = keep_first())
chm = rasterize(0.5, del)
chm2 = pit_fill(chm)
seed = local_maximum(3)
tree = region_growing(chm2, seed)
pipeline = read + del + chm + chm2 +  seed + tree
ans = processor(pipeline)
```

```{r, fig.show="hold", fig.width=4}
col = grDevices::colorRampPalette(c("blue", "cyan2", "yellow", "red"))(25)
terra::plot(ans$rasterize, col = col, mar = c(1, 1, 1, 3))
terra::plot(ans$pit_fill, col = col, mar = c(1, 1, 1, 3))
terra::plot(ans$region_growing, col = col[sample.int(25, 297, TRUE)], mar = c(1, 1, 1, 3))
plot(ans$local_maximum$geom, add = T, pch = 19, cex = 0.5)
```

## Buffer

Point clouds are typically stored in multiple contiguous files. To avoid edge artifacts, each file must be loaded with extra points coming from neighbouring files. Everything is handled automatically, except for the `callback()` stage. In `callback()`, the point cloud is exposed as a `data.frame` with the buffer, providing the user-defined function with some spatial context. If `callback` is used to edit the points, everything is handled internally. However, if an R object is returned, it is the responsibility of the user to handle the buffer.

For example, in the following pipeline, we are processing two files, and `callback()` is used to count the number of points. The presence of `triangulate()` implies that each file will be loaded with a buffer. Consequently, counting the points in `callback()` returns more points than `summarise()` because `summarise()` is an internal function that knows how to deal with the buffer.

```{r, echo = FALSE}
f = paste0(system.file(package="lasR"), "/extdata/bcts")
f = list.files(f, pattern = "(?i)\\.la(s|z)$", full.names = TRUE)
f = f[1:2]
```

```{r buffer}
count = function(data) { length(data$X) }
read = reader(f)
del = triangulate(filter = keep_ground())
npts = callback(count, expose = "x")
sum = summarise()
ans = processor(read + del + npts + sum)
print(ans$callback)
ans$callback[[1]]+ ans$callback[[2]]
ans$summary$npoints
```

We can compare this with the pipeline without `triangulate()`. In this case, there is no reason to use a buffer, and the files are not buffered. The counts are equal.


```{r buffer2}
ans = processor(read + npts + sum)
ans$callback[[1]]+ ans$callback[[2]]
ans$summary$npoints
```

To handle the buffer, the user can read the attribute `bbox` of the `data.frame`. It contains the bounding box of the point cloud without the buffer or use the column `Buffer` that contains `TRUE` or `FALSE` for each point. If `TRUE`, the point is in the buffer. The buffer is exposed only if the user includes the letter `'b'`.

```{r buffer3}
count_buffer_aware = function(data) {
  bbox = attr(data, "bbox")
  npoints = sum(!data$Buffer)
  return(list(bbox = bbox, npoints = npoints))
}

read = reader(f)
del = triangulate(filter = keep_ground())
npts = callback(count_buffer_aware, expose = "b") # b for buffer
sum = summarise()
ans = processor(read + del + npts + sum)
print(ans$callback)
ans$callback[[1]]$npoints+ ans$callback[[2]]$npoints
ans$summary$npoints
```

In conclusion, in the hypothesis that the user-defined function returns something complex, there are two ways to handle the buffer: either using the bounding box or using the `Buffer` flag. A third option is to use `drop_buffer`. In this case users ensure to receive a `data.frame` that does not include points from the buffer.

## Hulls

A Delaunay triangulation defines a convex polygon, which represents the convex hull of the points. However, in dense point clouds, removing triangles with large edges due to the absence of points results in a more complex structure.

```{r, echo = FALSE}
f <- system.file("extdata", "Topography.las", package="lasR")
```

```{r hulls, fig.width=5, fig.height=5}
read = reader(f)
del = triangulate(15, filter = keep_ground(), ofile = tempfile(fileext = ".gpkg"))
ans = processor(read+del)

par(mar = c(2, 2, 1, 1))
plot(ans, axes = T, lwd = 0.5)
```

The `hulls()` algorithm computes the contour of the mesh, producing a concave hull with holes:

```{r hulls2, fig.width=5, fig.height=5}
read = reader(f)
del = triangulate(15, filter = keep_ground())
bound = hulls(del)
ans = processor(read+del+bound)

par(mar = c(2, 2, 1, 1))
plot(ans, axes = T, lwd = 0.5, col = "gray")
```

However `hulls()` is more likely to be used without a triangulation. In this case it returns the bounding box of each LAS/LAZ file read from the header. And if it is used with `triangulate(0)` it returns the convex hull but this is a very inefficient way to get the convex hull.

## Sampling Voxel

The `sampling_voxel()` and `summarise()` functions operate similarly to other algorithms, and their output depends on their position in the pipeline:


```{r, echo = FALSE}
f <- c(system.file("extdata", "Topography.las", package="lasR"))
```

```{r sampling}
read = reader(f)
pipeline = read + summarise() + sampling_voxel(4) + summarise()
ans = processor(pipeline)
print(head(ans[[1]]))
print(head(ans[[2]]))
```

## Readers

There are several readers hidden behind `reader()`: 

- `reader_coverage()`: will read of the files and process the entire point cloud.
- `reader_rectangles()`: will read only some rectangular regions of interest of the coverage and process then sequentially.
- `reader_circles()`: will read only some circular regions of interest of the coverage and process then sequentially.

The following pipeline triangulates the ground points, normalizes the point cloud, and computes some metric of interest **for each file of the entire coverage**. Each file is loaded with a buffer so that triangulation is performed without edge artifacts. Notice the use of `drop_buffer = TRUE` to expose the `data.frame` without the buffer used to perform the triangulation and normalization.


```{r readers, eval = FALSE}
my_metric_fun = function(data) { mean(data$Z) }
tri <- triangulate(filter = keep_ground())
trans <- transform_with(tri)
norm <- tri + trans
metric <- callback(my_metric_fun, expose = "z", drop_buffer = TRUE)
pipeline = reader(f) + norm + metric
```

The following pipeline, on the contrary, works exactly the same but operates only on circular plots.


```{r, eval = FALSE}
pipeline = reader_circles(f, xcenter, ycenter, 11.28) + tri + norm + metrics
```

These readers allow building a ground inventory pipeline, for example.

## Plot inventory

This pipeline extracts a plot inventory using a shapefile from a non-normalized point cloud, normalizes each plot, computes metrics for each plot, and writes each normalized and non-normalized plot in separate files. Each circular plot is **loaded with a buffer** to perform a correct triangulation. The plots are exposed to R without the buffer using `drop_buffer = TRUE`.

```{r inventory, eval=FALSE}
ofiles_plot <- paste0(tempdir(), "/plot_*.las")
ofiles_plot_norm <- paste0(tempdir(), "/plot_*_norm.las")

my_metric_fun = function(data) { mean(data$Z) }

library(sf)
inventory <- st_read("shapefile.shp")
coordinates <- st_coordinates(inventory)
xcenter <- coordinates[,1]
ycenter <- coordinates[,2]

read <- reader(f, xc = xcenter, yc = ycenter, r = 11.28) 
tri <- triangulate(filter = keep_ground())
trans <- transform_with(tri)
norm <- tri + trans
metric <- callback(my_metric_fun, expose = "z", drop_buffer = TRUE)
write1 <- write_las(ofiles_plot)
write2 <- write_las(ofiles_plot_norm)

pipeline = read + trans + write1 + norm + write2
```

## Wildcard Usage

Usually, `write_las()` is used with a wildcard in the `ofile` argument (see above) to write one file per processed file. Otherwise, everything is written into a single massive LAS file (which might be the desired behaviour). On the contrary, `rasterize()` is used without a wildcard to write everything into a single raster file, but it also accepts a wildcard to write the results in multiple files, which is very useful with `reader_circles()` to avoid having one massive raster mostly empty. Compare this pipeline with and without the wildcard.


```{r, echo = FALSE}
f = paste0(system.file(package="lasR"), "/extdata/bcts/")
f = list.files(f, pattern = "(?i)\\.la(s|z)$", full.names = TRUE)
f = f[1:2]
```

Without a wildcard, the output is a single raster that covers the entire point cloud with two patches of populated pixels.


```{r nowildcard, fig.height=4, fig.width=8, fig.show="hold"}
ofile = paste0(tempdir(), "/chm.tif")   # no wildcard

x = c(885100, 885100)
y = c(629200, 629600)

pipeline = reader(f, xc = x, yc = y, r = 20) + rasterize(2, "max", ofile = ofile)
r0 = processor(pipeline)

terra::plot(r0, col = col) # covers the entire collection of files
```

With a wildcard, the output contains two rasters that cover regions of interest.


```{r wildcard, fig.height=6, fig.show="hold", out.width="50%"}
ofile = paste0(tempdir(), "/chm_*.tif") # wildcard

x = c(885100, 885100)
y = c(629200, 629600)

pipeline = reader(f, xc = x, yc = y, r = 20) + rasterize(2, "max", ofile = ofile)
ans = processor(pipeline)

r1 = terra::rast(ans[1])
r2 = terra::rast(ans[2])
terra::plot(r1, col = col)
terra::plot(r2, col = col)
```

## Compatibility with lidR

While `lasR` does not depends on `lidR` it has some compatibility with it. Instead of providing paths to files or folder it is possible to pass a `LAScatalog` or a `LAS` object to the readers.

```r
library(lasR)
library(lidR)

ctg = readLAScatalog(folder)
pipeline = reader(ctg) + normalize() + write_las()
ans = processor(pipeline)

las = readLAS(file)
pipeline = reader(las) + normalize() + write_las()
ans = processor(pipeline)
```