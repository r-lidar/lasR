[{"path":"/articles/baba.html","id":"concept","dir":"Articles","previous_headings":"","what":"Concept","title":"Buffered Area Based Approach","text":"area-based approach partitions study area cells computes derived metrics points lie within cell. method offers straightforward means map value interest across territory. However, one drawback mapping resolution typically coarse, cells typically measuring 20 x 20 meters, corresponding 400 square meters inventory plots used build predictive model. Achieving finer resolution possible. lasR, introduced concept Buffered Area Based Approach (BABA) Moving Windows Area Based Approach (MWABA). BABA, metrics computed using points within cell size s (typically 20 meters), akin classical ABA. However, resolution raster effectively r due moving window progresses steps size r. instance, can compute average Z elevation 400 square meter area resolution 5 meters, instead typical 20 meters.","code":""},{"path":"/articles/baba.html","id":"exemple-1","dir":"Articles","previous_headings":"","what":"Exemple 1","title":"Buffered Area Based Approach","text":"’s important emphasize BABA, although resolution set 5 meters, values pixel computed based 20-meter cell. metric remains valid concerning reference plot inventory, typically 400 square meters. Consequently, becomes feasible predict map values interest fine-grained resolution using regular plot inventory data.","code":"f <- system.file(\"extdata\", \"Megaplot.las\", package=\"lasR\") r = reader(f)  aba  = rasterize(20, \"zmean\")      # ABA baba = rasterize(c(5,20), \"zmean\") # BABA pipeline = r + aba + baba ans = processor(pipeline)  terra::plot(ans[[1]], col = col, main = \"ABA\") terra::plot(ans[[2]], col = col, main = \"BABA\")"},{"path":"/articles/baba.html","id":"exemple-2","dir":"Articles","previous_headings":"","what":"Exemple 2","title":"Buffered Area Based Approach","text":"approach also enables mapping point density finer resolution, among possibilities.","code":"f <- system.file(\"extdata\", \"Topography.las\", package=\"lasR\") r <- reader(f) c1 <- rasterize(1, \"count\") c2  <- rasterize(c(1,4), \"count\") pipeline = r + c1 + c2 res <- processor(pipeline) terra::plot(res[[1]]/4, col = gray.colors(15,0,1), main = \"Regular\")   # divide by 4 to get the density terra::plot(res[[2]]/25, col = gray.colors(15,0,1), main = \"Moving windows\")  # divide by 25 to get the density"},{"path":"/articles/baba.html","id":"naming-convention","dir":"Articles","previous_headings":"","what":"Naming convention","title":"Buffered Area Based Approach","text":"Buffered Area Based Approach (BABA), Moving Windows Area Based Approach (MWABA), Multi-Resolution Area Based Approach (MRABA) – feel free refer however prefer.","code":""},{"path":"/articles/lasR-lidR-mapping.html","id":"add_attribute","dir":"Articles","previous_headings":"","what":"add_attribute","title":"How lidR functions map to lasR functions","text":"lidR, function add_attribute enables adding column data.frame column writable LAS file. Since data.frame lasR, package processes -disk files produces -disk files, one--one equivalent add_attribute(). closest equivalent function something like: adds extrabytes attribute, attribute zeroed. idea attributes can used later stages, , pre-built function stages.","code":"add_dimension = function(ifile, data_type, name, description, ofile = tempfile(fileext = \".las\")) {   pipeline <- reader(f) + add_extrabytes(data_type, name, description) + write_las(ofile)   processor(pipeline) }"},{"path":"/articles/lasR-lidR-mapping.html","id":"catalog_apply","dir":"Articles","previous_headings":"","what":"catalog_apply","title":"How lidR functions map to lasR functions","text":"equivalent since catalog_apply() catalog_map() functions designed create pipelines lidR. lasR pipeline engine need functions. pipeline mechanism lidR difficult use.","code":""},{"path":"/articles/lasR-lidR-mapping.html","id":"catalog_retile","dir":"Articles","previous_headings":"","what":"catalog_retile","title":"How lidR functions map to lasR functions","text":"currently full equivalent lasR works files create arbitrary chunks. Yet, catalog_retile() versatile, behaviours can reproduced, writing buffered tiles.","code":"buffer_tiles = function(f, buffer, ofiles = paste0(tempdir(), \"/*_buffered.las\")) {   read = reader(f, buffer = buffer)   write = write_las(ofiles, keep_buffer = TRUE)   processor(read+write) }"},{"path":"/articles/lasR-lidR-mapping.html","id":"classify_ground","dir":"Articles","previous_headings":"","what":"classify_ground","title":"How lidR functions map to lasR functions","text":"lasR include built-ground classification algorithm yet, callback stage can used leverage functions R packages. lidR csf() package RCSF mcc() package RMCC.","code":"classify_ground_csf = function(     files,      smooth = FALSE,      threshold = 0.5,      resolution = 0.5,      rigidness = 1L,      iterations = 500L,      step = 0.65,      ofiles = paste0(tempdir(), \"/*_classified.las\")) {   csf = function(data, smooth, threshold, resolution, rigidness, iterations, step)   {     id = RCSF::CSF(data, smooth, threshold, resolution, rigidness, iterations, step)     class = integer(nrow(data))     class[id] = 2L     data$Classification <- class     return(data)   }      read = reader(files, buffer = 25)   classify = callback(csf, expose = \"xyz\", smooth = smooth, threshold = threshold, resolution = resolution, rigidness = rigidness, iterations = iterations, step = step)   write = write_las(ofiles)   pipeline = read + classify + write   processor(pipeline) }"},{"path":"/articles/lasR-lidR-mapping.html","id":"classify_noise","dir":"Articles","previous_headings":"","what":"classify_noise","title":"How lidR functions map to lasR functions","text":", worth noting pre-built functions, possible perform ground classification noise classification, self-built pipeline, possible combine multiple stages pipeline.","code":"classify_noise_ivf = function(files, res =  5, n = 6, ofiles = paste0(tempdir(), \"/*_classified.las\")) {   processor(reader(files) + classify_isolated_points(res, n) +  write_las(ofile)) } read = reader(f)  ground = callback(csf, expose = \"xyz\", threshold = 0.4, iterations = 200) noise = classify_isolated_points() write = write_las() pipeline = read + ground + noise + write"},{"path":"/articles/lasR-lidR-mapping.html","id":"classify_poi","dir":"Articles","previous_headings":"","what":"classify_poi","title":"How lidR functions map to lasR functions","text":"current equivalent","code":""},{"path":"/articles/lasR-lidR-mapping.html","id":"clip","dir":"Articles","previous_headings":"","what":"clip","title":"How lidR functions map to lasR functions","text":", function implemented using sf package supports multiple options, returning R memory writing files.","code":"clip_circle = function(files, geometry, radius, ofiles = paste0(tempdir(), \"/*_clipped.las\")) {   if (sf::st_geometry_type(geometry, FALSE) != \"POINT\") stop(\"Expected POINT geometry type\")    coordinates <- sf::st_coordinates(geometry)   xcenter <- coordinates[,1]   ycenter <- coordinates[,2]      read = reader(files, xc = xcenter, yc = ycenter, r = radius)      if (length(ofiles) == 1L && ofiles == \"\")     stage = callback(function(data) { return(data) }, expose = \"*\", no_las_update = T)   else     stage = write_las(ofiles)      ans = processor(read+stage)   return(ans) }"},{"path":"/articles/lasR-lidR-mapping.html","id":"decimate_points","dir":"Articles","previous_headings":"","what":"decimate_points","title":"How lidR functions map to lasR functions","text":"","code":"decimate_with_voxels = function(f, res = 2, ofiles = paste0(tempdir(), \"/*_decimated.las\")) {   read = reader(f)   sample = sampling_voxel(res)   write = write_las(ofiles)   processor(read+sample+write) } decimate_with_pixel = function(f, res = 2, ofiles = paste0(tempdir(), \"/*_decimated.las\")) {   read = reader(f)   sample = sampling_pixel(res)   write = write_las(ofiles)   processor(read+sample+write) }"},{"path":"/articles/lasR-lidR-mapping.html","id":"filter","dir":"Articles","previous_headings":"","what":"filter","title":"How lidR functions map to lasR functions","text":"lasR parse R expression ReturnNumber != x thus one must use LASlib filters. stage pipeline filter. See ?lasR::filters.","code":""},{"path":"/articles/lasR-lidR-mapping.html","id":"filter_duplicates","dir":"Articles","previous_headings":"","what":"filter_duplicates","title":"How lidR functions map to lasR functions","text":"lasR added -drop_duplicates command LASlib.","code":"read = reader(f, filter = drop_duplicates())"},{"path":"/articles/lasR-lidR-mapping.html","id":"las_check","dir":"Articles","previous_headings":"","what":"las_check","title":"How lidR functions map to lasR functions","text":"currently implementation added later","code":""},{"path":"/articles/lasR-lidR-mapping.html","id":"locate_trees","dir":"Articles","previous_headings":"","what":"locate_trees","title":"How lidR functions map to lasR functions","text":"","code":"tree_tops = function(f, ...) {   read = reader(f)   ttops = local_maximum(...)   processor(read+ttops) }"},{"path":"/articles/lasR-lidR-mapping.html","id":"normalize_height","dir":"Articles","previous_headings":"","what":"normalize_height","title":"How lidR functions map to lasR functions","text":"implementation leaves choice use extrabytes attributes store height ground.","code":"normalize_elevation = function(f, extrabytes = FALSE, ofiles = paste0(tempdir(), \"/*_normalized.las\")) {   read = reader(f)   tri = triangulate()      pipeline = read + tri      if (extrabytes)   {     extra = add_extrabytes(\"int\", \"HAG\", \"Height Above Ground\")     norm = transform_with(tri, store_in_attribute = \"HAG\")     pipeline = pipeline + extra + norm   }   else   {     norm = transform_with(tri)     pipeline = pipeline + norm   }      pipeline = pipeline + write_las(ofiles)   processor(pipeline) }"},{"path":"/articles/lasR-lidR-mapping.html","id":"pixel_metrics","dir":"Articles","previous_headings":"","what":"pixel_metrics","title":"How lidR functions map to lasR functions","text":"lasR rasterize(), exact equivalent pixel_metrics(). hood, uses aggregate(). Due non-standard evaluation, function bit trickier use inside function.","code":"raster_metrics = function(f, res, fun) {   call = substitute(fun)    env = new.env(parent=parent.frame())   read = reader(f)   rast = lasR:::aggregate_q(res, call, filter = \"\", ofile = tempfile(fileext = \".tif\"), env = env)   processor(read+rast) }"},{"path":"/articles/lasR-lidR-mapping.html","id":"plot_metrics","dir":"Articles","previous_headings":"","what":"plot_metrics","title":"How lidR functions map to lasR functions","text":"computes custom metrics per inventory plots.","code":"circle_metrics = function(f, xcenter, ycenter, radius, fun) {   read = reader(f, xc = xcenter, yc = ycenter, r = radius)   metrics = callback(fun, expose = \"*\")   processor(read+metrics) }"},{"path":"/articles/lasR-lidR-mapping.html","id":"rasterize_canopy","dir":"Articles","previous_headings":"","what":"rasterize_canopy","title":"How lidR functions map to lasR functions","text":", interesting notice limitation pre-built function. dataset normalized, pipeline builds Digital Elevation Model (similar lidR::rasterize_canopy()). way obtain Canopy Height Model, just like lidR. However, using low-level functions build custom pipeline, can normalize --fly pipeline.","code":"rasterize_chm_p2r = function(f, res) {   read = reader(f)   chm = rasterize(res, \"max\")   return(processor(read+chm)) }  rasterize_chm_tin = function(f, res) {   read = reader(f)   tin = triangulate()   chm = rasterize(res, tin)   return(processor(read+tin+chm)) }"},{"path":"/articles/lasR-lidR-mapping.html","id":"rasterize_terrain","dir":"Articles","previous_headings":"","what":"rasterize_terrain","title":"How lidR functions map to lasR functions","text":"","code":"rasterize_dtm = function(f, res) {   read = reader(f)   tri = triangulate(filter = keep_ground())   dtm = rasterize(res, tri)   return(processor(read+tri+dtm)) }"},{"path":"/articles/lasR-lidR-mapping.html","id":"rasterize_density","dir":"Articles","previous_headings":"","what":"rasterize_density","title":"How lidR functions map to lasR functions","text":"","code":"rasterize_chm_p2r = function(f, res) {   read = reader(f)   den = rasterize(res, \"count\")   return(processor(read+den)) }"},{"path":"/articles/lasR-lidR-mapping.html","id":"readlas","dir":"Articles","previous_headings":"","what":"readLAS","title":"How lidR functions map to lasR functions","text":"","code":"read_las = function(f, select = \"xyzi\", filter = \"\") {   load = function(data) { return(data) }   read = reader(f)   call = callback(load, expose = select, filter, no_las_update = TRUE)   return(processor(read+call)) }"},{"path":"/articles/lasR1.html","id":"rationnale-for-lasr-vs--lidr","dir":"Articles","previous_headings":"","what":"Rationnale for lasR vs. lidR","title":"1. Why lasR?","text":"need new package? short answer lies following graph. x-axis represents time perform three different rasterizations (CHM, DTM, density map), y-axis represents amount RAM memory used lidR lasR (details benchmark vignette). lasR intended much efficient lidR terms memory usage computation times.  second issue absence powerful pipeline engine lidR. Performing task simple extracting deriving metrics multiple inventory plots non-normalized collection files easy lidR. straightforward point cloud normalized, , users must write complex custom script. introduction real pipelines, lasR enables users complex tasks easier way (see tutorial vignette well pipeline vignette). Last least, almost decade additional experience R, C++, point cloud processing, lot feedback compared started creation lidR. simply technically capable writing lasR ten years ago!","code":""},{"path":[]},{"path":"/articles/lasR1.html","id":"pipeline","dir":"Articles","previous_headings":"Main differences between lasR and lidR","what":"Pipeline","title":"1. Why lasR?","text":"lasR introduces versatile pipeline engine, enabling creation complex processing pipelines. Users can simultaneously create ABA compute DTM one read pass, leading significant speed-.","code":""},{"path":"/articles/lasR1.html","id":"data-loading","dir":"Articles","previous_headings":"Main differences between lasR and lidR","what":"Data loading","title":"1. Why lasR?","text":"Unlike lidR, lasR load lidar data data.frame. designed efficient data processing, memory management C++ level. Consequently, read_las() function. Everything internally efficiently stored C++ structure keeps data compact memory. However, entry points available inject user-defined R code C++ pipeline.","code":""},{"path":"/articles/lasR1.html","id":"dependencies","dir":"Articles","previous_headings":"Main differences between lasR and lidR","what":"Dependencies","title":"1. Why lasR?","text":"lasR one dependency, boost. doesn’t even depend Rcpp. lasR use terra sf R level reading writing spatial data; instead, links GDAL. terra sf installed, output files read packages. Due absence dependency R package non-loading data R objects, also dependency rgl, resulting interactive 3D viewer like lidR.","code":""},{"path":"/articles/lasR1.html","id":"code","dir":"Articles","previous_headings":"Main differences between lasR and lidR","what":"Code","title":"1. Why lasR?","text":"lasR written 100% C++ contains R code. utilizes source code lidR significant improvements. major improvements observed benchmark much source code rather organization code, .e., longer using data.frame, memory management C++ rather R, processing R level, pipelines, .","code":""},{"path":"/articles/lasR1.html","id":"should-i-use-lidr-or-lasr","dir":"Articles","previous_headings":"","what":"Should I use lidR or lasR?","title":"1. Why lasR?","text":"question actually pretty simple answer. want explore, manipulate, test, try, retry, implement new ideas mind, use lidR. know want, want relatively common (raster metrics, DTM, CHM, tree location), especially want large coverage, use lasR.","code":""},{"path":"/articles/lasR1.html","id":"example-1","dir":"Articles","previous_headings":"Should I use lidR or lasR?","what":"Example 1","title":"1. Why lasR?","text":"received 500 km² data, want CHM DTM. → Use lasR compute fast possible.","code":""},{"path":"/articles/lasR1.html","id":"example-2","dir":"Articles","previous_headings":"Should I use lidR or lasR?","what":"Example 2","title":"1. Why lasR?","text":"want segment trees, explore different methods, test different parameters small plots. Maybe integrate custom step, ’s exploratory process. → Use lidR.","code":""},{"path":"/articles/lasR1.html","id":"example-3","dir":"Articles","previous_headings":"Should I use lidR or lasR?","what":"Example 3","title":"1. Why lasR?","text":"want extract circular ground inventories compute metrics plot. → dataset already normalized, can use either lasR lidR; pretty much equivalent. lidR easier use; lasR little bit efficient difficult use (yet pipeline vignette contains copy-pastable code ). dataset normalized, lasR much simpler case, thanks pipeline processor allows adding normalization stage computing metrics.","code":""},{"path":"/articles/lasR1.html","id":"example-4","dir":"Articles","previous_headings":"Should I use lidR or lasR?","what":"Example 4","title":"1. Why lasR?","text":"want create complex pipeline computes local shape points classify roofs wires point cloud. using shapefile, want classify water point cloud. finish, want write new classified LAS files. → Use lidR. lasR many tools. lasR lidR; much efficient less versatile fewer tools.","code":""},{"path":"/articles/lasR1.html","id":"example-5","dir":"Articles","previous_headings":"Should I use lidR or lasR?","what":"Example 5","title":"1. Why lasR?","text":"want find segment trees common algorithm. Nothing fancy. want 100 km² . → Use lasR. lidR probably fail .","code":""},{"path":"/articles/lasR2.html","id":"overall-functionality","dir":"Articles","previous_headings":"","what":"Overall functionality","title":"2. Tutorial","text":"lasR, R functions provided user designed process data directly; instead, used create pipeline. pipeline consists stages applied point cloud. stage can either transform point cloud within pipeline without generating output process point cloud produce output. figure , 4 LAS/LAZ files pipeline (1) reads file, (2) builds writes DTM disk, (3) transforms point cloud normalizing elevation, (4) builds canopy height model using transformed point cloud, (5) transforms point cloud removing points 5 m. resulting version point cloud (points 5m) discarded lost additional stage pipeline. However, stages can added, application predictive model points 5 m stage writes point cloud disk. first file completes entire pipeline, second file used, pipeline applied fill missing parts geospatial rasters vectors produced pipeline. file loaded buffer neighboring files needed. pipeline created R interface nothing initially. building pipeline, users must call processor() function initiate computation.","code":""},{"path":"/articles/lasR2.html","id":"reader","dir":"Articles","previous_headings":"","what":"Reader","title":"2. Tutorial","text":"reader stage MUST first stage pipeline (blue figure ). takes input list files process. creating pipeline stage, header files read, computation actually applied. points even read stage pipeline require read points. result returned.","code":"read = reader(f) processor(read) #> NULL"},{"path":"/articles/lasR2.html","id":"triangulate","dir":"Articles","previous_headings":"","what":"Triangulate","title":"2. Tutorial","text":"first stage can try triangulate(). algorithm performs Delaunay triangulation points interest. Triangulating points useful task employed numerous processing tasks. Triangulating points interesting, usually want use filter argument triangulate specific points interest. following example, triangulate points classified 2 (.e., ground). produces meshed Digital Terrain Model. example, files read sequentially, points loaded one one stored build Delaunay triangulation. lasR, one file stored memory time. program stores point cloud Delaunay triangulation current processing file. data discarded load new file. users provide path output file store result, result lost. following pipeline, building triangulation ground points, get output everything lost. following pipeline triangulation stored geopackage file providing argument ofile:  can also triangulate first returns. produce meshed Digital Surface Model. can also perform triangulations pipeline. idea lasR execute tasks one pass using pipeline: Using triangulate() without stage pipeline usually useful. Typically, triangulate() employed without ofile argument intermediate step. instance, can used rasterize().","code":"read = reader(f) del = triangulate(filter = keep_ground()) ans = processor(read+del) ans #> NULL read = reader(f) del = triangulate(filter = keep_ground(), ofile = tempfile(fileext = \".gpkg\")) ans = processor(read+del) ans #> Simple feature collection with 1 feature and 0 fields #> Geometry type: MULTIPOLYGON #> Dimension:     XYZ #> Bounding box:  xmin: 273357.2 ymin: 5274357 xmax: 273642.9 ymax: 5274643 #> z_range:       zmin: 788.9932 zmax: 814.8322 #> Projected CRS: NAD83(CSRS) / MTM zone 7 #> # A tibble: 1 × 1 #>                                                                             geom #>                                                               <MULTIPOLYGON [m]> #> 1 Z (((273357.5 5274626 805.9742, 273357.4 5274633 804.6458, 273357.4 5274634 8…  par(mar = c(2, 2, 1, 1)) plot(ans, axes = T, lwd = 0.5) read = reader(f) del = triangulate(filter = keep_first(), ofile = tempfile(fileext = \".gpkg\")) ans = processor(read+del) read = reader(f) del1 = triangulate(filter = keep_ground(), ofile = tempfile(fileext = \".gpkg\")) del2 = triangulate(filter = keep_first(), ofile = tempfile(fileext = \".gpkg\")) pipeline = read + del1 + del2 ans = processor(pipeline)"},{"path":"/articles/lasR2.html","id":"rasterize","dir":"Articles","previous_headings":"","what":"Rasterize","title":"2. Tutorial","text":"rasterize() exactly users may expect even . three variations: Rasterize predefined operators. operators optimized internally, making operations fast possible, number registered operators low. Rasterize injecting user-defined R expression. equivalent pixel_metrics() package lidR. user-defined function can mapped, making extremely versatile slower limited speed quality R code injected. Rasterize Delaunay triangulation. variations, users can build CHM, DTM, predictive model, anything else.","code":""},{"path":"/articles/lasR2.html","id":"rasterize---triangulation","dir":"Articles","previous_headings":"Rasterize","what":"Rasterize - triangulation","title":"2. Tutorial","text":"Let’s build DTM using triangulation ground points rasterize() stage. following pipeline, LAS files read, points loaded LAS file buffer, Delaunay triangulation ground points built, triangulation interpolated rasterized. default, rasterize() writes raster temporary file, result discarded. , processor() returns SpatRaster triangulate() returns nothing (NULL). Therefore, pipeline contains two elements, one returns something.  Notice , contrary lidR package, usually high-level function names like rasterize_terrain(). Instead, lasR made low-level functions versatile also challenging use.","code":"read = reader(f) del = triangulate(filter = keep_ground()) dtm = rasterize(1, del) pipeline = read + del + dtm ans = processor(pipeline) ans #> class       : SpatRaster  #> dimensions  : 286, 286, 1  (nrow, ncol, nlyr) #> resolution  : 1, 1  (x, y) #> extent      : 273357, 273643, 5274357, 5274643  (xmin, xmax, ymin, ymax) #> coord. ref. : NAD83(CSRS) / MTM zone 7 (EPSG:2949)  #> source      : file21d72366ea40.tif  #> name        : file21d72366ea40 terra::plot(ans, col = gray.colors(25,0,1), mar = c(1, 1, 1, 3))"},{"path":"/articles/lasR2.html","id":"rasterize---internal-metrics","dir":"Articles","previous_headings":"Rasterize","what":"Rasterize - internal metrics","title":"2. Tutorial","text":"Let’s build two CHMs: one based highest point per pixel resolution 2 meters, second based triangulation first returns resolution 50 cm. following pipeline, using two variations rasterize(): one capable rasterizing triangulation capable rasterizing point cloud predefined operator (max). output named list two SpatRaster.  simplicity package pre-installed pipelines named chm() dtm() explained .","code":"read <- reader(f) del <- triangulate(filter = keep_first()) chm1 <- rasterize(2, \"max\") chm2 <- rasterize(0.5, del) pipeline <- read + del + chm1 + chm2 ans <- processor(pipeline)  terra::plot(ans[[1]], mar = c(1, 1, 1, 3), col = col) terra::plot(ans[[2]], mar = c(1, 1, 1, 3), col = col)"},{"path":"/articles/lasR2.html","id":"rasterize---custom-metrics","dir":"Articles","previous_headings":"Rasterize","what":"Rasterize - custom metrics","title":"2. Tutorial","text":"Last least, let’s compute map median intensity injecting user-defined expression. Like lidR, attributes point cloud named: X, Y, Z, Intensity, gpstime, ReturnNumber, NumberOfreturns, Classification, UserData, PointSourceID, R, G, B, NIR. users familiar lidR package, note ScanAngleRank/ScanAngle; instead scanner angle always named ScanAngle numeric. Also flags named Withheld, Synthetic Keypoint.","code":"read = reader(f) avgi = rasterize(10, median(Intensity)) pipeline = read + avgi ans = processor(pipeline)  terra::plot(ans, mar = c(1, 1, 1, 3), col = heat.colors(15))"},{"path":"/articles/lasR2.html","id":"rasterize---buffered","dir":"Articles","previous_headings":"Rasterize","what":"Rasterize - buffered","title":"2. Tutorial","text":"lasR package introduced concept buffered area-based approach enhance resolution prediction maps. However, concept covered detail tutorial. information, readers can refer dedicated article","code":""},{"path":"/articles/lasR2.html","id":"transform-with","dir":"Articles","previous_headings":"","what":"Transform with","title":"2. Tutorial","text":"Another way use Delaunay triangulation transform point cloud. Users can add subtract triangulation point cloud, effectively normalizing . Unlike lidR package, high-level function names like normalize_points(). Instead, lasR composed low-level functions offer versatility. Let’s normalize point cloud using triangulation ground points (meshed DTM). following example, triangulation used transform_with() modifies point cloud pipeline. triangulate() transform_with() return nothing. output NULL. convenience pipeline pre-recorded package name normalize(). transform_with() can also transform raster. presented tutorial. obtain meaningful output, necessary chain another stage. point cloud modified , discarded nothing . instance, can compute Canopy Height Model (CHM) normalized point cloud. following pipeline, first rasterization (chm1) applied normalization, second rasterization occurs transform_with(), thus applied transformed point cloud.  performing normalization, users may want write normalized point cloud disk later use. case, can append write_las() stage pipeline.","code":"read = reader(f) del = triangulate(filter = keep_ground()) norm = transform_with(del, \"-\") pipeline = read + del + norm ans = processor(pipeline) ans #> NULL read = reader(f) del = triangulate(filter = keep_ground()) norm = transform_with(del, \"-\") chm1 = rasterize(2, \"max\") chm2 = rasterize(2, \"max\") pipeline = read + chm1 + del + norm + chm2 ans = processor(pipeline)  col = grDevices::colorRampPalette(c(\"blue\", \"cyan2\", \"yellow\", \"red\"))(15) terra::plot(c(ans[[1]], ans[[2]]), col = col)"},{"path":"/articles/lasR2.html","id":"write-las","dir":"Articles","previous_headings":"","what":"Write LAS","title":"2. Tutorial","text":"write_las() can called point pipeline. writes one file per input file, using name input files added prefixes suffixes. following pipeline, read files, write ground points files named original files suffix _ground, perform triangulation entire point cloud, followed normalization. Finally, write normalized point cloud suffix _normalized. crucial include wildcard * file path; otherwise, single large file created. behaviour may intentional. Let’s consider creating file merge pipeline. following example, wildcard * used names LAS/LAZ files. input files read, points sequentially written single file dataset_merged.laz, naturally forming merge pipeline.","code":"read = reader(f) write1 = write_las(paste0(tempdir(), \"/*_ground.laz\"), filter = keep_ground()) write2 = write_las(paste0(tempdir(), \"/*_normalized.laz\"), ) del = triangulate(filter = keep_ground()) norm = transform_with(del, \"-\") pipeline = read + write1 + del + norm + write2 ans = processor(pipeline) ans #>  - write_las : /tmp/RtmpzDbC9b/bcts_1_ground.laz /tmp/RtmpzDbC9b/bcts_2_ground.laz  #>  - write_las : /tmp/RtmpzDbC9b/bcts_1_normalized.laz /tmp/RtmpzDbC9b/bcts_2_normalized.laz read = reader(f) ofile = paste0(tempdir(), \"/dataset_merged.laz\") merge = write_las(ofile) ans = processor(read + merge) ans #> [1] \"/tmp/RtmpzDbC9b/dataset_merged.laz\""},{"path":"/articles/lasR2.html","id":"callback","dir":"Articles","previous_headings":"","what":"Callback","title":"2. Tutorial","text":"callback stage holds significant importance second last entry point inject R code pipeline, following rasterize(). familiar lidR package, initial step often involves reading data lidR::readLAS() expose point cloud data.frame object R. contrast, lasR loads point cloud optimally C++ without exposing directly R. However, callback, becomes possible expose point cloud data.frame executing specific R functions. Similar lidR, attributes point cloud lasR named: X, Y, Z, Intensity, gpstime, ReturnNumber, NumberOfreturns, Classification, UserData, PointSourceID, R, G, B, NIR. Notably, users accustomed lidR package, scanner angle consistently named ScanAngle numeric, opposed ScanAngleRank/ScanAngle. Additionally, flags named Withheld, Synthetic, Keypoint. Let’s delve simple example. LAS file, callback loads point cloud data.frame invokes meanz() function data.frame. output list two elements processed two files (f displayed document). average Z elevation respectively 809.08 13.27 file. mindful , given LAS/LAZ file, point cloud may contain points original file file loaded buffer. clarification matter provided later. callback function versatile can also employed edit point cloud. user-defined function returns data.frame number rows original one, function edits underlying C++ dataset. enables users perform tasks assigning class specific point. physically removing points possible, users can flag points Withheld. cases, points processed subsequent stages observed, , time callback explicitly return anything; however, edited point cloud internally. generate output, users must use another stage write_las(). ’s important note write_las() write point number 12 flagged withheld. Neither subsequent stage process . point still memory discarded. memory efficiency reasons, possible physically remove point underlying memory lasR. Instead, points flagged withheld never processed. One consequence , points flagged withheld LAS/LAZ file processed lasR. aligns intended purpose flag according LAS specification may differ default behaviour many software market including lidR. Now, let’s explore capabilities callback . First, let’s create lidR-like read_las() function expose point cloud R. following example, user-defined function employed return data.frame . user’s function returns data.frame number points original dataset, updates points C++ level. , use no_las_update = TRUE explicitly return result. Ground points can also classified using R function, one provided RCSF package: callback() exposes point cloud data.frame. way present point clouds users manageable way. One reasons lasR memory-efficient faster lidR expose point cloud data.frame. Thus, pipelines using callback() significantly different lidR. advantage using lasR ability pipe different stages.","code":"meanz = function(data){ return(mean(data$Z)) } read = reader(f) call = callback(meanz, expose = \"xyz\") ans = processor(read+call) print(ans) #>  - 809.0835  #>  - 13.27202 edit_points = function(data) {   data$Classification[5:7] = c(2L,2L,2L)   data$Withheld = FALSE   data$Withheld[12] = TRUE   return(data) }  read = reader(f) call = callback(edit_points, expose = \"xyzc\") ans = processor(read+call) ans #> NULL read_las = function(f, select = \"xyzi\", filter = \"\") {   load = function(data) { return(data) }   read = reader(f, filter = filter)   call = callback(load, expose = select, no_las_update = TRUE)   return (processor(read+call)) }  f <- system.file(\"extdata\", \"Topography.las\", package=\"lasR\") las = read_las(f) head(las) #>          X       Y        Z Intensity #> 1 273357.1 5274360 806.5340      1340 #> 2 273357.2 5274359 806.5635       728 #> 3 273357.2 5274358 806.0248      1369 #> 4 273357.2 5274510 809.6303       589 #> 5 273357.2 5274509 809.3880      1302 #> 6 273357.2 5274508 809.4847       123 csf = function(data) {   id = RCSF::CSF(data)   class = integer(nrow(data))   class[id] = 2L   data$Classification <- class   return(data) }  read = reader(f) classify = callback(csf, expose = \"xyz\") write = write_las() pipeline = read + classify + write processor(pipeline)"},{"path":"/articles/lasR2.html","id":"tree-segmentation","dir":"Articles","previous_headings":"","what":"Tree Segmentation","title":"2. Tutorial","text":"section presents complex pipeline tree segmentation using local_maximum() identify tree tops, region_growing() segment trees using seeds produced local_maximum(), Canopy Height Model (CHM) produced using Delaunay triangulation first returns. CHM post-processed pit_fill(), algorithm designed enhance CHM filling pits NAs. tutorial, pipeline tested one file render page faster. However, pipeline can applied number files produce contiguous output, managing buffer files. Every intermediate output can exported, tutorial, export everything display outputs.","code":"read = reader(f) del = triangulate(filter = keep_first()) chm = rasterize(0.5, del) chm2 = pit_fill(chm) seed = local_maximum(3) tree = region_growing(chm2, seed) pipeline = read + del + chm + chm2 +  seed + tree ans = processor(pipeline) col = grDevices::colorRampPalette(c(\"blue\", \"cyan2\", \"yellow\", \"red\"))(25) terra::plot(ans$rasterize, col = col, mar = c(1, 1, 1, 3)) terra::plot(ans$pit_fill, col = col, mar = c(1, 1, 1, 3)) terra::plot(ans$region_growing, col = col[sample.int(25, 297, TRUE)], mar = c(1, 1, 1, 3)) plot(ans$local_maximum$geom, add = T, pch = 19, cex = 0.5)"},{"path":"/articles/lasR2.html","id":"buffer","dir":"Articles","previous_headings":"","what":"Buffer","title":"2. Tutorial","text":"Point clouds typically stored multiple contiguous files. avoid edge artifacts, file must loaded extra points coming neighbouring files. Everything handled automatically, except callback() stage. callback(), point cloud exposed data.frame buffer, providing user-defined function spatial context. callback used edit points, everything handled internally. However, R object returned, responsibility user handle buffer. example, following pipeline, processing two files, callback() used count number points. presence triangulate() implies file loaded buffer. Consequently, counting points callback() returns points summarise() summarise() internal function knows deal buffer. can compare pipeline without triangulate(). case, reason use buffer, files buffered. counts equal. handle buffer, user can read attribute bbox data.frame. contains bounding box point cloud without buffer use column Buffer contains TRUE FALSE point. TRUE, point buffer. buffer exposed user includes letter 'b'. conclusion, hypothesis user-defined function returns something complex, two ways handle buffer: either using bounding box using Buffer flag. third option use drop_buffer. case users ensure receive data.frame include points buffer.","code":"count = function(data) { length(data$X) } read = reader(f) del = triangulate(filter = keep_ground()) npts = callback(count, expose = \"x\") sum = summarise() ans = processor(read + del + npts + sum) print(ans$callback) #>  - 682031  #>  - 931581 ans$callback[[1]]+ ans$callback[[2]] #> [1] 1613612 ans$summary$npoints #> [1] 1355607 ans = processor(read + npts + sum) ans$callback[[1]]+ ans$callback[[2]] #> [1] 1355607 ans$summary$npoints #> [1] 1355607 count_buffer_aware = function(data) {   bbox = attr(data, \"bbox\")   npoints = sum(!data$Buffer)   return(list(bbox = bbox, npoints = npoints)) }  read = reader(f) del = triangulate(filter = keep_ground()) npts = callback(count_buffer_aware, expose = \"b\") # b for buffer sum = summarise() ans = processor(read + del + npts + sum) print(ans$callback) #>  - List: #>    - bbox : 885022.4 629157.2 885210.2 629400  #>    - npoints : 531662  #>  - List: #>    - bbox : 885024.1 629400 885217.1 629700  #>    - npoints : 823945 ans$callback[[1]]$npoints+ ans$callback[[2]]$npoints #> [1] 1355607 ans$summary$npoints #> [1] 1355607"},{"path":"/articles/lasR2.html","id":"hulls","dir":"Articles","previous_headings":"","what":"Hulls","title":"2. Tutorial","text":"Delaunay triangulation defines convex polygon, represents convex hull points. However, dense point clouds, removing triangles large edges due absence points results complex structure.  hulls() algorithm computes contour mesh, producing concave hull holes:  However hulls() likely used without triangulation. case returns bounding box LAS/LAZ file read header. used triangulate(0) returns convex hull inefficient way get convex hull.","code":"read = reader(f) del = triangulate(15, filter = keep_ground(), ofile = tempfile(fileext = \".gpkg\")) ans = processor(read+del)  par(mar = c(2, 2, 1, 1)) plot(ans, axes = T, lwd = 0.5) read = reader(f) del = triangulate(15, filter = keep_ground()) bound = hulls(del) ans = processor(read+del+bound)  par(mar = c(2, 2, 1, 1)) plot(ans, axes = T, lwd = 0.5, col = \"gray\")"},{"path":"/articles/lasR2.html","id":"sampling-voxel","dir":"Articles","previous_headings":"","what":"Sampling Voxel","title":"2. Tutorial","text":"sampling_voxel() summarise() functions operate similarly algorithms, output depends position pipeline:","code":"read = reader(f) pipeline = read + summarise() + sampling_voxel(4) + summarise() ans = processor(pipeline) print(head(ans[[1]])) #>  - npoints : 73403  #>  - nsingle : 31294  #>  - nwithheld : 0  #>  - nsynthetic : 0  #>  - npoints_per_return : 53538 15828 3569 451 16 1  #>  - npoints_per_class : 61347 8159 3897 print(head(ans[[2]])) #>  - npoints : 12745  #>  - nsingle : 5027  #>  - nwithheld : 0  #>  - nsynthetic : 0  #>  - npoints_per_return : 9401 2626 618 92 7 1  #>  - npoints_per_class : 10792 1573 380"},{"path":"/articles/lasR2.html","id":"readers","dir":"Articles","previous_headings":"","what":"Readers","title":"2. Tutorial","text":"several readers hidden behind reader(): reader_coverage(): read files process entire point cloud. reader_rectangles(): read rectangular regions interest coverage process sequentially. reader_circles(): read circular regions interest coverage process sequentially. following pipeline triangulates ground points, normalizes point cloud, computes metric interest file entire coverage. file loaded buffer triangulation performed without edge artifacts. Notice use drop_buffer = TRUE expose data.frame without buffer used perform triangulation normalization. following pipeline, contrary, works exactly operates circular plots. readers allow building ground inventory pipeline, example.","code":"my_metric_fun = function(data) { mean(data$Z) } tri <- triangulate(filter = keep_ground()) trans <- transform_with(tri) norm <- tri + trans metric <- callback(my_metric_fun, expose = \"z\", drop_buffer = TRUE) pipeline = reader(f) + norm + metric pipeline = reader_circles(f, xcenter, ycenter, 11.28) + tri + norm + metrics"},{"path":"/articles/lasR2.html","id":"plot-inventory","dir":"Articles","previous_headings":"","what":"Plot inventory","title":"2. Tutorial","text":"pipeline extracts plot inventory using shapefile non-normalized point cloud, normalizes plot, computes metrics plot, writes normalized non-normalized plot separate files. circular plot loaded buffer perform correct triangulation. plots exposed R without buffer using drop_buffer = TRUE.","code":"ofiles_plot <- paste0(tempdir(), \"/plot_*.las\") ofiles_plot_norm <- paste0(tempdir(), \"/plot_*_norm.las\")  my_metric_fun = function(data) { mean(data$Z) }  library(sf) inventory <- st_read(\"shapefile.shp\") coordinates <- st_coordinates(inventory) xcenter <- coordinates[,1] ycenter <- coordinates[,2]  read <- reader(f, xc = xcenter, yc = ycenter, r = 11.28)  tri <- triangulate(filter = keep_ground()) trans <- transform_with(tri) norm <- tri + trans metric <- callback(my_metric_fun, expose = \"z\", drop_buffer = TRUE) write1 <- write_las(ofiles_plot) write2 <- write_las(ofiles_plot_norm)  pipeline = read + trans + write1 + norm + write2"},{"path":"/articles/lasR2.html","id":"wildcard-usage","dir":"Articles","previous_headings":"","what":"Wildcard Usage","title":"2. Tutorial","text":"Usually, write_las() used wildcard ofile argument (see ) write one file per processed file. Otherwise, everything written single massive LAS file (might desired behaviour). contrary, rasterize() used without wildcard write everything single raster file, also accepts wildcard write results multiple files, useful reader_circles() avoid one massive raster mostly empty. Compare pipeline without wildcard. Without wildcard, output single raster covers entire point cloud two patches populated pixels.  wildcard, output contains two rasters cover regions interest.","code":"ofile = paste0(tempdir(), \"/chm.tif\")   # no wildcard  x = c(885100, 885100) y = c(629200, 629600)  pipeline = reader(f, xc = x, yc = y, r = 20) + rasterize(2, \"max\", ofile = ofile) r0 = processor(pipeline)  terra::plot(r0, col = col) # covers the entire collection of files ofile = paste0(tempdir(), \"/chm_*.tif\") # wildcard  x = c(885100, 885100) y = c(629200, 629600)  pipeline = reader(f, xc = x, yc = y, r = 20) + rasterize(2, \"max\", ofile = ofile) ans = processor(pipeline)  r1 = terra::rast(ans[1]) r2 = terra::rast(ans[2]) terra::plot(r1, col = col) terra::plot(r2, col = col)"},{"path":"/articles/lasR2.html","id":"compatibility-with-lidr","dir":"Articles","previous_headings":"","what":"Compatibility with lidR","title":"2. Tutorial","text":"lasR depends lidR compatibility . Instead providing paths files folder possible pass LAScatalog LAS object readers.","code":"library(lasR) library(lidR)  ctg = readLAScatalog(folder) pipeline = reader(ctg) + normalize() + write_las() ans = processor(pipeline)  las = readLAS(file) pipeline = reader(las) + normalize() + write_las() ans = processor(pipeline)"},{"path":[]},{"path":"/articles/lasR3.html","id":"normalize","dir":"Articles","previous_headings":"Pipeline factories","what":"Normalize","title":"3. Pipelines","text":"pipeline already installed package. can used way","code":"normalize = function(extrabytes = FALSE) {   tri <- triangulate(filter = keep_ground())   pipeline <- tri      if (extrabytes)   {     extra = add_extrabytes(\"int\", \"HAG\", \"Height Above Ground\")     trans = transform_with(tri, store_in_attribute = \"HAG\")     pipeline = pipeline + extra + trans   }   else   {     trans = transform_with(tri)     pipeline = pipeline + trans   }      return(pipeline) } pipeline = reader(f) + normalize() + write_las(o)"},{"path":[]},{"path":"/articles/lasR3.html","id":"with-csf","dir":"Articles","previous_headings":"Pipeline factories > Classify ground","what":"With CSF","title":"3. Pipelines","text":"","code":"ground_csf = function(smooth = FALSE, threshold = 0.5, resolution = 0.5, rigidness = 1L, iterations = 500L, step = 0.65) {   csf = function(data, smooth, threshold, resolution, rigidness, iterations, step)   {     id = RCSF::CSF(data, smooth, threshold, resolution, rigidness, iterations, step)     class = integer(nrow(data))     class[id] = 2L     data$Classification <- class     return(data)   }      classify = callback(csf, expose = \"xyz\", smooth = smooth, threshold = threshold, resolution = resolution, rigidness = rigidness, iterations = iterations, step = step)   return(classify) } pipeline = reader(f) + ground_csf() + write_las(o)"},{"path":"/articles/lasR3.html","id":"with-mcc","dir":"Articles","previous_headings":"Pipeline factories > Classify ground","what":"With MCC","title":"3. Pipelines","text":"pipelines use callback() exposes point cloud data.frame. One reasons lasR memory-efficient faster lidR expose point cloud data.frame. Thus, pipelines different classify_ground() function lidR. advantage using lasR ability pipe different stages.","code":"ground_mcc = function(s = 1.5, t = 0.3) {   csf = function(data, s, t)   {     id = RMCC::MCC(data, s, t)     class = integer(nrow(data))     class[id] = 2L     data$Classification <- class     return(data)   }      classify = callback(csf, expose = \"xyz\", s = s, t = t)   return(classify) } pipeline = reader(f) + ground_mcc() + write_las(o)"},{"path":"/articles/lasR3.html","id":"canopy-heigh-model","dir":"Articles","previous_headings":"Pipeline factories","what":"Canopy Heigh Model","title":"3. Pipelines","text":"two pipelines natively installed package name chm().","code":"chm_p2r = function(res, filter = \"\", ofile = tempfile(fileext = \".tif\")) {   return(rasterize(res, \"max\", filter = filter, ofile = ofile)) } chm_tin = function(res, ofile = tempfile(fileext = \".tif\")) {   tin = triangulate(filter = keep_first())   chm = rasterize(res, tin, ofile = ofile)   return(tin+chm) }"},{"path":"/articles/lasR3.html","id":"digital-terrain-model","dir":"Articles","previous_headings":"Pipeline factories","what":"Digital Terrain Model","title":"3. Pipelines","text":"one also natively installed package. add_class can used add class used ground 9 water.","code":"dtm = function(res, ofile = tempfile(fileext = \".tif\"), add_class = NULL) {   filter = keep_ground()   if (!is.null(add_class)) filter = filter + keep_class(add_class)   tin = triangulate(filter = filter)   chm = rasterize(res, tin, ofile = ofile)   return(tin+chm) }"},{"path":[]},{"path":"/articles/lasR3.html","id":"read-las","dir":"Articles","previous_headings":"Useful functions","what":"Read LAS","title":"3. Pipelines","text":"","code":"read_las = function(files, select = \"xyzi\", filter = \"\") {   load = function(data) { return(data) }   read = reader(files, filter = filter)   call = callback(load, expose = select, no_las_update = TRUE)   return(processor(read+call)) }"},{"path":"/articles/lasR3.html","id":"buffer-tiles","dir":"Articles","previous_headings":"Useful functions","what":"Buffer tiles","title":"3. Pipelines","text":"","code":"buffer_tiles = function(files, buffer, ofiles = paste0(tempdir(), \"/*_buffered.las\")) {   read = reader(files, buffer = buffer)   write = write_las(ofiles, keep_buffer = TRUE)   return(processor(read+write)) }"},{"path":"/articles/lasR3.html","id":"clip-circle","dir":"Articles","previous_headings":"Useful functions","what":"Clip circle","title":"3. Pipelines","text":"Writes LAS files returns data.frames. Supports sf objects input.","code":"clip_circle = function(files, geometry, radius, ofiles = paste0(tempdir(), \"/*_clipped.las\")) {   if (sf::st_geometry_type(geometry, FALSE) != \"POINT\")      stop(\"Expected POINT geometry type\")    coordinates <- sf::st_coordinates(geometry)   xcenter <- coordinates[,1]   ycenter <- coordinates[,2]      read = reader(files, xc = xcenter, yc = ycenter, r = radius)      if (length(ofiles) == 1L && ofiles == \"\")     stage = callback(function(data) { return(data) }, expose = \"*\", no_las_update = T)   else     stage = write_las(ofiles)      ans = processor(read+stage)   return(ans) }"},{"path":"/articles/lasR3.html","id":"crs","dir":"Articles","previous_headings":"Useful functions","what":"CRS","title":"3. Pipelines","text":"CRS sf object. cost applying hulls() virtually null.","code":"crs = function(files) {   pipeline = reader(files) + hulls()   ans = processor(pipeline)   return(sf::st_crs(ans)) }"},{"path":"/articles/lasR3.html","id":"inventory-metrics","dir":"Articles","previous_headings":"Useful functions","what":"Inventory metrics","title":"3. Pipelines","text":"Using sf object provide plot centers offering option normalize --fly. returns sf object extra attributes.","code":"inventory_metrics = function(f, geometry, radius, fun, normalize = FALSE) {   if (sf::st_geometry_type(geometry, FALSE) != \"POINT\")      stop(\"Expected POINT geometry type\")    coordinates <- sf::st_coordinates(geometry)   xcenter <- coordinates[,1]   ycenter <- coordinates[,2]      pipeline <- reader(f, xc = xcenter, yc = ycenter, r = radius)      if (normalize)   {     tri <- triangulate(filter = keep_ground())     trans <- transform_with(tri)     pipeline <- pipeline + tri + trans   }    pipeline <- pipeline + callback(fun, expose = \"*\")   ans <- processor(pipeline)   ans <- lapply(ans, as.data.frame)   ans <- do.call(rbind, ans)   return(cbind(geometry, ans)) }"},{"path":"/articles/lasR3.html","id":"virtual-point-cloud","dir":"Articles","previous_headings":"Useful functions","what":"Virtual point cloud","title":"3. Pipelines","text":"","code":"build_vpc = function(files, ofile) {   read = reader(files)   write = write_vpc(ofile)   processor(read+write) }"},{"path":[]},{"path":"/articles/lasR4.html","id":"lidr","dir":"Articles","previous_headings":"Canopy Height Model","what":"lidR","title":"4. Benchmarks of lasR vs. lidR","text":"","code":"chm = rasterize_canopy(ctg, 1, p2r())"},{"path":"/articles/lasR4.html","id":"lasr","dir":"Articles","previous_headings":"Canopy Height Model","what":"lasR","title":"4. Benchmarks of lasR vs. lidR","text":"","code":"pipeline = reader(ctg) + rasterize(1, \"max\") processor(pipeline)"},{"path":[]},{"path":[]},{"path":"/articles/lasR4.html","id":"lidr-1","dir":"Articles","previous_headings":"Digital Terrain Model","what":"lidR","title":"4. Benchmarks of lasR vs. lidR","text":"","code":"dtm = rasterize_terrain(ctg, 1, tin())"},{"path":"/articles/lasR4.html","id":"lasr-1","dir":"Articles","previous_headings":"Digital Terrain Model","what":"lasR","title":"4. Benchmarks of lasR vs. lidR","text":"","code":"tri = triangulate(filter = keep_ground()) pipeline = reader(ctg, filter = keep_ground()) + tri + rasterize(1, tri) processor(pipeline)"},{"path":[]},{"path":"/articles/lasR4.html","id":"multiple-raster","dir":"Articles","previous_headings":"","what":"Multiple raster","title":"4. Benchmarks of lasR vs. lidR","text":"gain terms computation time much significant running multiple stages single pipeline files read lasR multiple times lidR. , operations executed single pass C++ level, resulting efficient memory management.","code":""},{"path":"/articles/lasR4.html","id":"lidr-2","dir":"Articles","previous_headings":"Multiple raster","what":"lidR","title":"4. Benchmarks of lasR vs. lidR","text":"","code":"custom_function = function(z,i) { list(avgz = mean(z), avgi = mean(i)) } ctg = readLAScatalog(f) chm = rasterize_canopy(ctg, 1, p2r()) met = pixel_metrics(ctg, ~custom_function(Z, Intensity), 20) den = rasterize_density(ctg, 5)"},{"path":"/articles/lasR4.html","id":"lasr-2","dir":"Articles","previous_headings":"Multiple raster","what":"lasR","title":"4. Benchmarks of lasR vs. lidR","text":"","code":"custom_function = function(z,i) { list(avgz = mean(z), avgi = mean(i)) } read = reader(folder) chm = rasterize(1, \"max\") met = rasterize(20, custom_function(Z, Intensity)) den = rasterize(5, \"count\") pipeline = read + chm + met + den processor(pipeline)"},{"path":"/articles/lasR4.html","id":"complex-pipeline","dir":"Articles","previous_headings":"","what":"Complex Pipeline","title":"4. Benchmarks of lasR vs. lidR","text":"complex pipeline, point cloud normalized written new files. Digital Terrain Model (DTM) produced, Canopy Height Model (CHM) built, individual trees detected. detected trees used seeds region-growing algorithm segments trees. lasR pipeline can handle hundreds laser tiles, lidR may struggle apply pipeline, especially tree segmentation.","code":"read = reader(ctg) del = triangulate(filter = keep_ground()) norm = transform_with(del) dtm = rasterize(1, del) chm = rasterize(1, \"max\") seed = local_maximum(3) tree = region_growing(chm, seed) write = write_las() pipeline = read + del + norm + write + dtm + chm +  seed + tree ans = processor(pipeline, progress = TRUE)"},{"path":"/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Jean-Romain Roussel. Author, maintainer, copyright holder. Martin Isenburg. Copyright holder.            author LASlib LASzip libraries Benoît St-Onge. Copyright holder.            author 'chm_prep' function","code":""},{"path":"/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Roussel J (2024). lasR: Fast Pipable Airborne LiDAR Data Tools. R package version 0.2.1, https://github.com/r-lidar/lasR.","code":"@Manual{,   title = {lasR: Fast and Pipable Airborne LiDAR Data Tools},   author = {Jean-Romain Roussel},   year = {2024},   note = {R package version 0.2.1},   url = {https://github.com/r-lidar/lasR}, }"},{"path":"/index.html","id":"lasr","dir":"","previous_headings":"","what":"Fast and Pipable Airborne LiDAR Data Tools","title":"Fast and Pipable Airborne LiDAR Data Tools","text":"R Package Fast Airborne LiDAR Data Processing package early stage development. lasR package (pronounce laser) intent supersede lidR package, designed much efficient lidR common tasks like production CHM, DTM, tree detection segmentation large coverages. lidR intends tool box make data exploration innovation easy. lasR another hand focuses production, optimized memory speed makes trade aspects development. 📖 Read tutorial start lasR","code":""},{"path":"/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Fast and Pipable Airborne LiDAR Data Tools","text":"currently plan releasing lasR CRAN. must compile install package Github. Windows must install Rtools first able build package use package remotes install package: Users can’t rely CRAN versioning system RStudio update button get latest version lasR. loading lasR library(lasR), internal routine checks GitHub latest version prints message new version available. Updates frequent way.","code":"remotes::install_github(\"r-lidar/lasR\") library(lasR) #> lasR 0.1.3 is now available. You are using 0.1.1 #> remotes::install_github(\"r-lidar/lasR\")"},{"path":"/index.html","id":"benchmark","dir":"","previous_headings":"","what":"Benchmark","title":"Fast and Pipable Airborne LiDAR Data Tools","text":"following benchmark compares much time RAM memory takes lasR lidR produce DTM, CHM, raster two metrics derived Z intensity. test performed 120 million points stored 4 LAZ files. details benchmark vignette.","code":""},{"path":"/index.html","id":"main-differences-with-lidr","dir":"","previous_headings":"","what":"Main differences with lidR","title":"Fast and Pipable Airborne LiDAR Data Tools","text":"Introduces concept pipelines, missing lidR, chain multiple operations point cloud optimally. written exclusively C/C++ without single line R. Uses code lidR brings significant speed memory improvements. load point cloud data.frame. point cloud stored C++ structure exposed users. Uses GDAL instead relying terra sf flexibility C++ level. 2 strong dependencies: boost gdal. sf terra installed experience better. details corresponding vignette","code":""},{"path":"/index.html","id":"copyright-information","dir":"","previous_headings":"","what":"Copyright Information","title":"Fast and Pipable Airborne LiDAR Data Tools","text":"© 2007-2021 Martin Isenburg - http://rapidlasso.com Provided LGPL license modified R-compliant Jean-Romain Roussel. © 2023-2024 Jean-Romain Roussel Provided GPL-3 license. © 2008-2023 Benoît St-Onge - Geophoton-inc/chm_prep Provided GPL-3 license.","code":""},{"path":"/reference/add_extrabytes.html","id":null,"dir":"Reference","previous_headings":"","what":"Add attributes to a LAS file — add_extrabytes","title":"Add attributes to a LAS file — add_extrabytes","text":"According LAS specifications, LAS file contains core defined attributes, XYZ coordinates, intensity, return number, , point. possible add supplementary attributes. stages adds extra bytes attribute points. Values zeroed. edits point cloud returns nothing.","code":""},{"path":"/reference/add_extrabytes.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Add attributes to a LAS file — add_extrabytes","text":"","code":"add_extrabytes(data_type, name, description, scale = 1, offset = 0)"},{"path":"/reference/add_extrabytes.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Add attributes to a LAS file — add_extrabytes","text":"data_type character. data type extra bytes attribute. Can \"uchar\", \"char\", \"ushort\", \"short\", \"uint\", \"int\", \"uint64\", \"int64\", \"float\", \"double\". name character. name extra bytes attribute add file. description character. short description extra bytes attribute add file (32 characters). scale, offset numeric. scale offset data. See LAS specification.","code":""},{"path":"/reference/add_extrabytes.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Add attributes to a LAS file — add_extrabytes","text":"","code":"f <- system.file(\"extdata\", \"Example.las\", package = \"lasR\") fun <- function(data) { data$RAND <- runif(nrow(data), 0, 100); return(data) } pipeline <- reader(f) +   add_extrabytes(\"float\", \"RAND\", \"Random numbers\") +   callback(fun, expose = \"xyz\") processor(pipeline) #> NULL"},{"path":"/reference/callback.html","id":null,"dir":"Reference","previous_headings":"","what":"Call a user-defined function on the point cloud — callback","title":"Call a user-defined function on the point cloud — callback","text":"Call user-defined function point cloud. function receives data.frame point cloud. first input must point cloud. function returns anything data.frame number points, output stored returned end. However, output data.frame number points, updates point cloud. function can, therefore, used modify point cloud using user-defined function.","code":""},{"path":"/reference/callback.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Call a user-defined function on the point cloud — callback","text":"","code":"callback(fun, expose = \"xyz\", ..., drop_buffer = FALSE, no_las_update = FALSE)"},{"path":"/reference/callback.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Call a user-defined function on the point cloud — callback","text":"fun numeric. resolution raster. expose character. Expose attributes interest save memory (see details). ... parameters function fun drop_buffer bool. false, expose point buffer. no_las_update bool. user-defined function returns data.frame, supposed update point cloud. Can disabled.","code":""},{"path":"/reference/callback.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Call a user-defined function on the point cloud — callback","text":"lasR, point cloud exposed R data.frame like lidR. stored internally C++ structure seen modified directly users using R code. callback function stage allows direct interaction point cloud copying temporarily data.frame apply user-defined function.expose: 'expose' argument specifies data actually exposed R. example, 'xyzia' means x, y, z coordinates, intensity, scan angle loaded. supported entries t - gpstime, - scan angle, - intensity, n - number returns, r - return number, c - classification, s - synthetic flag, k - keypoint flag, w - withheld flag, o - overlap flag (format 6+), u - user data, p - point source ID, e - edge flight line flag, d - direction scan flag, R - red channel RGB color, G - green channel RGB color, B - blue channel RGB color, N - near-infrared channel, C - scanner channel (format 6+) Also numbers 1 9 extra bytes data numbers 1 9. 0 enables extra bytes loaded, '*' wildcard enables everything loaded LAS file.","code":""},{"path":[]},{"path":"/reference/callback.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Call a user-defined function on the point cloud — callback","text":"","code":"f <- system.file(\"extdata\", \"Topography.las\", package = \"lasR\")  # There is no function in lasR to read the data in R. Let's create one read_las <- function(f) {   load <- function(data) { return(data) }   read <- reader(f)   call <- callback(load, expose = \"xyzi\", no_las_update = TRUE)   return (processor(read + call)) } las <- read_las(f) head(las) #>          X       Y        Z Intensity #> 1 273357.1 5274360 806.5340      1340 #> 2 273357.2 5274359 806.5635       728 #> 3 273357.2 5274358 806.0248      1369 #> 4 273357.2 5274510 809.6303       589 #> 5 273357.2 5274509 809.3880      1302 #> 6 273357.2 5274508 809.4847       123  convert_intensity_in_range <- function(data, min, max) {   i <- data$Intensity   i <- ((i - min(i)) / (max(i) - min(i))) * (max - min) + min   i[i < min] <- min   i[i > max] <- max   data$Intensity <- as.integer(i)   return(data) }  read <- reader(f) call <- callback(convert_intensity_in_range, expose = \"xyzi\", min = 0, max = 255) write <- write_las() pipeline <- read + call + write ans <- processor(pipeline)  las <- read_las(ans) head(las) #>          X       Y        Z Intensity #> 1 273357.1 5274360 806.5340       137 #> 2 273357.2 5274359 806.5635        72 #> 3 273357.2 5274358 806.0248       140 #> 4 273357.2 5274510 809.6303        57 #> 5 273357.2 5274509 809.3880       133 #> 6 273357.2 5274508 809.4847         7"},{"path":"/reference/chm.html","id":null,"dir":"Reference","previous_headings":"","what":"Canopy Height Model — chm","title":"Canopy Height Model — chm","text":"Create Canopy Height Model using triangulate rasterize.","code":""},{"path":"/reference/chm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Canopy Height Model — chm","text":"","code":"chm(res = 1, tin = FALSE, ofile = tempfile(fileext = \".tif\"))"},{"path":"/reference/chm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Canopy Height Model — chm","text":"res numeric. resolution raster. tin bool. default CHM point--raster based methods .e. pixel assigned elevation highest point. tin = TRUE CHM triangulation-based model. first returns triangulated interpolated. ofile character. Full outputs always stored disk. ofile = \"\" stage store result disk return nothing. however hold partial output results temporarily memory. useful stage intermediate stage.","code":""},{"path":[]},{"path":"/reference/chm.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Canopy Height Model — chm","text":"","code":"f <- system.file(\"extdata\", \"Topography.las\", package=\"lasR\") pipeline <- reader(f) + chm()"},{"path":"/reference/classify_isolated_points.html","id":null,"dir":"Reference","previous_headings":"","what":"Classify isolated points — classify_isolated_points","title":"Classify isolated points — classify_isolated_points","text":"stage identifies points points surrounding 3 x 3 x 3 = 27 voxels edits points assign target classification. Used class 18, classifies points noise similar lasnoise LAStools. stage modifies point cloud pipeline produce output.","code":""},{"path":"/reference/classify_isolated_points.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Classify isolated points — classify_isolated_points","text":"","code":"classify_isolated_points(res = 5, n = 6L, class = 18L)"},{"path":"/reference/classify_isolated_points.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Classify isolated points — classify_isolated_points","text":"res numeric. Resolution voxels. n integer. maximal number 'points' 27 voxels. class integer. class assign points match condition.","code":""},{"path":"/reference/dtm.html","id":null,"dir":"Reference","previous_headings":"","what":"Digital Terrain Model — dtm","title":"Digital Terrain Model — dtm","text":"Create Digital Terrain Model using triangulate rasterize.","code":""},{"path":"/reference/dtm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Digital Terrain Model — dtm","text":"","code":"dtm(res = 1, add_class = NULL, ofile = tempfile(fileext = \".tif\"))"},{"path":"/reference/dtm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Digital Terrain Model — dtm","text":"res numeric. resolution raster. add_class integer. default triangulates using ground points (class 2). possible provide additional classes 9 water. ofile character. Full outputs always stored disk. ofile = \"\" stage store result disk return nothing. however hold partial output results temporarily memory. useful stage intermediate stage.","code":""},{"path":[]},{"path":"/reference/dtm.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Digital Terrain Model — dtm","text":"","code":"f <- system.file(\"extdata\", \"Topography.las\", package=\"lasR\") pipeline <- reader(f) + dtm()"},{"path":"/reference/filters.html","id":null,"dir":"Reference","previous_headings":"","what":"Point filters — filters","title":"Point filters — filters","text":"lasR uses LASlib/LASzip, library developed Martin Isenburg read write LAS/LAZ files. Thus, flags available LAStools also available lasR. Filters strings put filter arguments lasR algorithms. list available strings accessible filter_usage. convenience, useful filters associated function returns corresponding string.","code":""},{"path":"/reference/filters.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Point filters — filters","text":"","code":"keep_class(x)  drop_class(x)  keep_first()  drop_first()  keep_ground()  drop_ground()  keep_noise()  drop_noise()  keep_z_above(x)  drop_z_above(x)  keep_z_below(x)  drop_z_below(x)  drop_duplicates()  filter_usage()  # S3 method for laslibfilter print(x, ...)  # S3 method for laslibfilter +(e1, e2)"},{"path":"/reference/filters.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Point filters — filters","text":"x numeric integer function filter used. ... Unused. e1, e2 lasR objects.","code":""},{"path":"/reference/filters.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Point filters — filters","text":"","code":"f <- system.file(\"extdata\", \"Topography.las\", package=\"lasR\") filter_usage() #> Filter points based on their coordinates. #>   -keep_tile 631000 4834000 1000 (ll_x ll_y size) #>   -keep_circle 630250.00 4834750.00 100 (x y radius) #>   -keep_xy 630000 4834000 631000 4836000 (min_x min_y max_x max_y) #>   -drop_xy 630000 4834000 631000 4836000 (min_x min_y max_x max_y) #>   -keep_x 631500.50 631501.00 (min_x max_x) #>   -drop_x 631500.50 631501.00 (min_x max_x) #>   -drop_x_below 630000.50 (min_x) #>   -drop_x_above 630500.50 (max_x) #>   -keep_y 4834500.25 4834550.25 (min_y max_y) #>   -drop_y 4834500.25 4834550.25 (min_y max_y) #>   -drop_y_below 4834500.25 (min_y) #>   -drop_y_above 4836000.75 (max_y) #>   -keep_z 11.125 130.725 (min_z max_z) #>   -drop_z 11.125 130.725 (min_z max_z) #>   -drop_z_below 11.125 (min_z) #>   -drop_z_above 130.725 (max_z) #>   -keep_xyz 620000 4830000 100 621000 4831000 200 (min_x min_y min_z max_x max_y max_z) #>   -drop_xyz 620000 4830000 100 621000 4831000 200 (min_x min_y min_z max_x max_y max_z) #>   -drop_duplicates #> Filter points based on their return numbering. #>   -keep_first -first_only -drop_first #>   -keep_last -last_only -drop_last #>   -keep_second_last -drop_second_last #>   -keep_first_of_many -keep_last_of_many #>   -drop_first_of_many -drop_last_of_many #>   -keep_middle -drop_middle #>   -keep_return 1 2 3 #>   -drop_return 3 4 #>   -keep_single -drop_single #>   -keep_double -drop_double #>   -keep_triple -drop_triple #>   -keep_quadruple -drop_quadruple #>   -keep_number_of_returns 5 #>   -drop_number_of_returns 0 #> Filter points based on the scanline flags. #>   -drop_scan_direction 0 #>   -keep_scan_direction_change #>   -keep_edge_of_flight_line #> Filter points based on their intensity. #>   -keep_intensity 20 380 #>   -drop_intensity_below 20 #>   -drop_intensity_above 380 #>   -drop_intensity_between 4000 5000 #> Filter points based on classifications or flags. #>   -keep_class 1 3 7 #>   -drop_class 4 2 #>   -keep_extended_class 43 #>   -drop_extended_class 129 135 #>   -drop_synthetic -keep_synthetic #>   -drop_keypoint -keep_keypoint #>   -drop_withheld -keep_withheld #>   -drop_overlap -keep_overlap #> Filter points based on their user data. #>   -keep_user_data 1 #>   -drop_user_data 255 #>   -keep_user_data_below 50 #>   -keep_user_data_above 150 #>   -keep_user_data_between 10 20 #>   -drop_user_data_below 1 #>   -drop_user_data_above 100 #>   -drop_user_data_between 10 40 #> Filter points based on their point source ID. #>   -keep_point_source 3 #>   -keep_point_source_between 2 6 #>   -drop_point_source 27 #>   -drop_point_source_below 6 #>   -drop_point_source_above 15 #>   -drop_point_source_between 17 21 #> Filter points based on their scan angle. #>   -keep_scan_angle -15 15 #>   -drop_abs_scan_angle_above 15 #>   -drop_abs_scan_angle_below 1 #>   -drop_scan_angle_below -15 #>   -drop_scan_angle_above 15 #>   -drop_scan_angle_between -25 -23 #> Filter points based on their gps time. #>   -keep_gps_time 11.125 130.725 #>   -drop_gps_time_below 11.125 #>   -drop_gps_time_above 130.725 #>   -drop_gps_time_between 22.0 48.0 #> Filter points based on their RGB/CIR/NIR channels. #>   -keep_RGB_red 1 1 #>   -drop_RGB_red 5000 20000 #>   -keep_RGB_green 30 100 #>   -drop_RGB_green 2000 10000 #>   -keep_RGB_blue 0 0 #>   -keep_RGB_nir 64 127 #>   -keep_RGB_greenness 200 65535 #>   -keep_NDVI 0.2 0.7 -keep_NDVI_from_CIR -0.1 0.5 #>   -keep_NDVI_intensity_is_NIR 0.4 0.8 -keep_NDVI_green_is_NIR -0.2 0.2 #> Filter points based on their wavepacket. #>   -keep_wavepacket 0 #>   -drop_wavepacket 3 #> Filter points based on extra attributes. #>   -keep_attribute_above 0 5.0 #>   -drop_attribute_below 1 1.5 #> Filter points with simple thinning. #>   -keep_every_nth 2 -drop_every_nth 3 #>   -keep_random_fraction 0.1 #>   -keep_random_fraction 0.1 4711 #>   -thin_with_grid 1.0 #>   -thin_pulses_with_time 0.0001 #>   -thin_points_with_time 0.000001 #> Boolean combination of filters. #>   -filter_and gnd = keep_class(c(2,9)) reader(f, gnd) #>  ----------- #> reader_las (uid:2kGJgw) #>   files : Topography.las #>   filter : -keep_class 2 9  #>   buffer : 0  #>   output :   #> ----------- triangulate(filter = keep_ground()) #>  ----------- #> triangulate (uid:xHhiYO) #>   max_edge : 0  #>   filter : -keep_class 2  #>   output :   #>   use_attribute : Z  #> ----------- rasterize(1, \"max\", filter = \"-drop_z_below 5\") #>  ----------- #> rasterize (uid:68LSSy) #>   res : 1  #>   window : 1  #>   method : max  #>   filter : -drop_z_below 5  #>   output : /tmp/RtmpfWaf9O/file1f82112d0c40.tif  #> -----------"},{"path":"/reference/hulls.html","id":null,"dir":"Reference","previous_headings":"","what":"Contour of a point cloud — hulls","title":"Contour of a point cloud — hulls","text":"stage uses Delaunay triangulation computes contour. contour strict Delaunay triangulation convex hull, lasR, triangulation max_edge argument. Thus, contour might convex hull holes. Used without triangulation returns bouding box points.","code":""},{"path":"/reference/hulls.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Contour of a point cloud — hulls","text":"","code":"hulls(mesh = NULL, ofile = tempgpkg())"},{"path":"/reference/hulls.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Contour of a point cloud — hulls","text":"mesh NULL LASRalgorithm. triangulate stage. NULL take bounding box header file. ofile character. Full outputs always stored disk. ofile = \"\" stage store result disk return nothing. however hold partial output results temporarily memory. useful stage intermediate stage.","code":""},{"path":[]},{"path":"/reference/hulls.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Contour of a point cloud — hulls","text":"","code":"f <- system.file(\"extdata\", \"Topography.las\", package = \"lasR\") read <- reader(f) tri <- triangulate(20, filter = keep_ground()) contour <- hulls(tri) pipeline <- read + tri + contour ans <- processor(pipeline) plot(ans)"},{"path":"/reference/lasR-package.html","id":null,"dir":"Reference","previous_headings":"","what":"lasR: airborne LiDAR for forestry applications — lasR-package","title":"lasR: airborne LiDAR for forestry applications — lasR-package","text":"lasR provides set tools process efficiently airborne LiDAR data forestry contexts. package works .las .laz files. toolbox includes algorithms DSM, CHM, DTM, ABA, normalisation, tree detection, tree segmentation, tree delineation, colourization, validation tools, well processing engine process broad LiDAR coverage split many files efficiently.","code":""},{"path":[]},{"path":"/reference/lasR-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"lasR: airborne LiDAR for forestry applications — lasR-package","text":"Maintainer: Jean-Romain Roussel jean-romain.roussel.1@ulaval.ca [copyright holder] contributors: Martin Isenburg (author LASlib LASzip libraries) [copyright holder] Benoît St-Onge (author 'chm_prep' function) [copyright holder]","code":""},{"path":"/reference/local_maximum.html","id":null,"dir":"Reference","previous_headings":"","what":"Local Maximum — local_maximum","title":"Local Maximum — local_maximum","text":"Local Maximum stage identifies points locally maximum. window size fixed circular. stage modify point cloud. produces derived product vector format.","code":""},{"path":"/reference/local_maximum.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Local Maximum — local_maximum","text":"","code":"local_maximum(   ws,   min_height = 2,   filter = \"\",   ofile = tempgpkg(),   use_attribute = \"Z\" )"},{"path":"/reference/local_maximum.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Local Maximum — local_maximum","text":"ws numeric. Diameter moving window used detect local maxima units input data (usually meters). min_height numeric. Minimum height local maximum. Threshold point local maximum. Default 2. filter 'filter' argument allows filtering point-cloud work points interest. available filters LASlib can found running filter_usage. given algorithm filter applied, points meet criteria processes. common strings \"-keep_first\", \"-keep_class 2\", \"drop_z_below 2\". details see filters. ofile character. Full outputs always stored disk. ofile = \"\" stage store result disk return nothing. however hold partial output results temporarily memory. useful stage intermediate stage. use_attribute character. default local maximum performed coordinate Z. Can also name extra bytes attribute 'HAG' exists. Can also 'Intensity' probably use case one.","code":""},{"path":"/reference/local_maximum.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Local Maximum — local_maximum","text":"","code":"f <- system.file(\"extdata\", \"MixedConifer.las\", package = \"lasR\") read <- reader(f) lmf <- local_maximum(3) ans <- processor(read + lmf) ans #> Simple feature collection with 297 features and 5 fields #> Geometry type: POINT #> Dimension:     XYZ #> Bounding box:  xmin: 481260 ymin: 3812921 xmax: 481349.8 ymax: 3813011 #> z_range:       zmin: 2.16 zmax: 32.07 #> Projected CRS: NAD83 / UTM zone 12N #> # A tibble: 297 × 6 #>    Intensity gpstime ReturnNumber Classification ScanAngle #>        <int>   <dbl>        <int>          <int>     <dbl> #>  1       141 151388.            1              1        -4 #>  2        81 151388.            1              1        -6 #>  3       115 149929.            1              1        16 #>  4       136 149930.            1              1        16 #>  5       107 151388.            1              1        -5 #>  6         3 151388.            1              1        -5 #>  7        13 149930.            1              1        17 #>  8        18 149930.            1              1        17 #>  9       116 151388.            1              1        -8 #> 10       137 149930.            1              1        17 #> # ℹ 287 more rows #> # ℹ 1 more variable: geom <POINT [m]>"},{"path":"/reference/ncores.html","id":null,"dir":"Reference","previous_headings":"","what":"Determine the number of available cores — ncores","title":"Determine the number of available cores — ncores","text":"function returns number available CPU cores can utilized.","code":""},{"path":"/reference/ncores.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Determine the number of available cores — ncores","text":"","code":"ncores()  half_cores()"},{"path":"/reference/ncores.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Determine the number of available cores — ncores","text":"integer representing number available CPU cores.","code":""},{"path":"/reference/normalize.html","id":null,"dir":"Reference","previous_headings":"","what":"Normalize the point cloud — normalize","title":"Normalize the point cloud — normalize","text":"Normalize point cloud using `triangulate()` `transform_with()`","code":""},{"path":"/reference/normalize.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Normalize the point cloud — normalize","text":"","code":"normalize(extrabytes = FALSE)"},{"path":"/reference/normalize.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Normalize the point cloud — normalize","text":"extrabytes bool. FALSE coordinate Z point cloud modified becomes height ground (HAG). TRUE coordinate Z modified new extrabytes attribute named 'HAG' added point cloud.","code":""},{"path":[]},{"path":"/reference/normalize.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Normalize the point cloud — normalize","text":"","code":"f <- system.file(\"extdata\", \"Topography.las\", package=\"lasR\") pipeline <- reader(f) + normalize() + write_las()"},{"path":"/reference/pit_fill.html","id":null,"dir":"Reference","previous_headings":"","what":"Pits and spikes filling — pit_fill","title":"Pits and spikes filling — pit_fill","text":"Pits spikes filling raster. Typically used post-processing CHM. algorithm St-Onge 2008 (see reference).","code":""},{"path":"/reference/pit_fill.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Pits and spikes filling — pit_fill","text":"","code":"pit_fill(   raster,   lap_size = 3L,   thr_lap = 0.1,   thr_spk = -0.1,   med_size = 3L,   dil_radius = 0L,   ofile = temptif() )"},{"path":"/reference/pit_fill.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Pits and spikes filling — pit_fill","text":"raster LASRalgorithm. stage produces raster. lap_size integer. Size Laplacian filter kernel (integer value, pixels). thr_lap numeric. Threshold Laplacian value detecting cavity (values value considered cavity). positive value. thr_spk numeric. Threshold Laplacian value detecting spike (values value considered spike). negative value. med_size integer. Size median filter kernel (integer value, pixels). dil_radius integer. Dilation radius (integer value, pixels). ofile character. Full outputs always stored disk. ofile = \"\" stage store result disk return nothing. however hold partial output results temporarily memory. useful stage intermediate stage.","code":""},{"path":"/reference/pit_fill.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Pits and spikes filling — pit_fill","text":"St-Onge, B., 2008. Methods improving quality true orthomosaic Vexcel UltraCam images created using alidar digital surface model, Proceedings Silvilaser 2008, Edinburgh, 555-562. https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=81365288221f3ac34b51a82e2cfed8d58defb10e","code":""},{"path":"/reference/pit_fill.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Pits and spikes filling — pit_fill","text":"","code":"f <- system.file(\"extdata\", \"MixedConifer.las\", package=\"lasR\")  reader <- reader(f, filter = keep_first()) tri <- triangulate() chm <- rasterize(0.25, tri) pit <- pit_fill(chm) u <- processor(reader + tri + chm + pit)  chm <- u[[1]] sto <- u[[2]]  #terra::plot(c(chm, sto), col = lidR::height.colors(25))"},{"path":"/reference/processor.html","id":null,"dir":"Reference","previous_headings":"","what":"Process the pipeline — processor","title":"Process the pipeline — processor","text":"Process pipeline. Every functions nothing. function must called pipeline actually process point-cloud","code":""},{"path":"/reference/processor.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Process the pipeline — processor","text":"","code":"processor(pipeline, ncores = half_cores(), progress = FALSE, ...)"},{"path":"/reference/processor.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Process the pipeline — processor","text":"pipeline LASRpipeline. serie algorithms called order ncores integer. Number cores use. stages steps stages parallelised overall one file process time. progress boolean. Displays progress bar. ... unused","code":""},{"path":"/reference/processor.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Process the pipeline — processor","text":"","code":"if (FALSE) { f <- paste0(system.file(package=\"lasR\"), \"/extdata/bcts/\") f <- list.files(f, pattern = \"(?i)\\\\.la(s|z)$\", full.names = TRUE)  read <- reader(f, filter = \"\") tri <- triangulate(15) dtm <- rasterize(5, tri) lmf <- local_maximum(5) met <- rasterize(2, mean(Intensity)) pipeline <- read + tri + dtm + lmf + met ans <- processor(pipeline) }"},{"path":"/reference/rasterize.html","id":null,"dir":"Reference","previous_headings":"","what":"Rasterize a point cloud — rasterize","title":"Rasterize a point cloud — rasterize","text":"Rasterize point cloud using different approaches. stage modify point cloud. produces derived product raster format.","code":""},{"path":"/reference/rasterize.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Rasterize a point cloud — rasterize","text":"","code":"rasterize(res, operators = \"max\", filter = \"\", ofile = temptif())"},{"path":"/reference/rasterize.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Rasterize a point cloud — rasterize","text":"res numeric. resolution raster. Can vector two resolutions. case correspond x y resolution buffered rasterization. (see section 'Buffered' examples) operators Can character vector. \"min\", \"max\" \"count\" accepted well many others (see section 'Operators'). Can also rasterize triangulation input LASRalgorithm triangulation (see examples). Can also user-defined expression (see example section 'Operators'). filter 'filter' argument allows filtering point-cloud work points interest. available filters LASlib can found running filter_usage. given algorithm filter applied, points meet criteria processes. common strings \"-keep_first\", \"-keep_class 2\", \"drop_z_below 2\". details see filters. ofile character. Full outputs always stored disk. ofile = \"\" stage store result disk return nothing. however hold partial output results temporarily memory. useful stage intermediate stage.","code":""},{"path":"/reference/rasterize.html","id":"operators","dir":"Reference","previous_headings":"","what":"Operators","title":"Rasterize a point cloud — rasterize","text":"operators string vector strings, function employs internally optimized metrics. available metrics include \"zmax\", \"zmin\", \"zmean\", \"zmedian\", \"zsd\", \"zcv\", \"zpXX\" Z coordinates. , \"zpXX\" represents XXth percentile, instance, \"zp95\" signifies 95th percentile. Similarly, metrics accessible letter \"\" intensity, \"imax\" others. Additionally, \"count\" another available metric.  operators user-defined expression, function return either vector numbers list containing atomic numbers. assign band name raster, vector list must named accordingly. following valid operators:","code":"f = function(x) { return(mean(x)) } g = function(x,y) { return(c(avg = mean(x), med = median(y))) } h = function(x) { return(list(a = mean(x), b = median(x))) } rasterize(10, f(Intensity)) rasterize(10, g(Z, Intensity)) rasterize(10, h(Z))"},{"path":"/reference/rasterize.html","id":"buffered","dir":"Reference","previous_headings":"","what":"Buffered","title":"Rasterize a point cloud — rasterize","text":"argument res vector two numbers, first number represents resolution output raster, second number represents size windows used compute metrics. approach called Buffered Area Based Approach (BABA). classical rasterization, metrics computed independently pixel. example, predicting resource typically involves computing metrics 400 square meter pixel, resulting raster resolution 20 meters. possible achieve finer granularity method. However, buffered rasterization, possible compute raster resolution 10 meters (.e., computing metrics every 10 meters) using 20 x 20 windows metric computation. case, windows overlap, essentially creating moving window effect. option apply rasterizing triangulation, second value considered case.","code":""},{"path":"/reference/rasterize.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Rasterize a point cloud — rasterize","text":"","code":"f <- system.file(\"extdata\", \"Topography.las\", package=\"lasR\") read <- reader(f) tri  <- triangulate(filter = keep_ground()) dtm  <- rasterize(1, tri) # input is a triangulation stage avgi <- rasterize(10, mean(Intensity)) # input is a user expression chm  <- rasterize(2, \"max\") # input is a character vector pipeline <- read + tri + dtm + avgi + chm ans <- processor(pipeline) ans[[1]] #> class       : SpatRaster  #> dimensions  : 286, 286, 1  (nrow, ncol, nlyr) #> resolution  : 1, 1  (x, y) #> extent      : 273357, 273643, 5274357, 5274643  (xmin, xmax, ymin, ymax) #> coord. ref. : NAD83(CSRS) / MTM zone 7 (EPSG:2949)  #> source      : file1f82f23cbf8.tif  #> name        : file1f82f23cbf8  ans[[2]] #> class       : SpatRaster  #> dimensions  : 30, 30, 1  (nrow, ncol, nlyr) #> resolution  : 10, 10  (x, y) #> extent      : 273350, 273650, 5274350, 5274650  (xmin, xmax, ymin, ymax) #> coord. ref. : NAD83(CSRS) / MTM zone 7 (EPSG:2949)  #> source      : file1f826fdda0e0.tif  #> name        : file1f826fdda0e0  ans[[3]] #> class       : SpatRaster  #> dimensions  : 144, 144, 1  (nrow, ncol, nlyr) #> resolution  : 2, 2  (x, y) #> extent      : 273356, 273644, 5274356, 5274644  (xmin, xmax, ymin, ymax) #> coord. ref. : NAD83(CSRS) / MTM zone 7 (EPSG:2949)  #> source      : file1f824cde1ac7.tif  #> name        : max   # Demonstration of buffered rasterization  # A good resolution for computing point density is 5 meters. c0 <- rasterize(5, \"count\")  # Computing point density at too fine a resolution doesn't make sense since there is # either zero or one point per pixel. Therefore, producing a point density raster with # a 2 m resolution is not feasible with classical rasterization. c1 <- rasterize(2, \"count\")  # Using a buffered approach, we can produce a raster with a 2-meter resolution where # the metrics for each pixel are computed using a 5-meter window. c2  <- rasterize(c(2,5), \"count\")  pipeline = read + c0 + c1 + c2 res <- processor(pipeline) terra::plot(res[[1]]/25)  # divide by 25 to get the density  terra::plot(res[[2]]/4)   # divide by 4 to get the density  terra::plot(res[[3]]/25)  # divide by 25 to get the density"},{"path":"/reference/reader.html","id":null,"dir":"Reference","previous_headings":"","what":"Initialize the pipeline — reader","title":"Initialize the pipeline — reader","text":"first stage must called pipeline. specifies files must read. stage nothing returns nothing associated another processing stage. initializes pipeline. reader() main function dispatches functions. reader_*() reads LAS/LAZ files disk.reader_coverage() processes entire point cloud. reader_circles() reader_rectangles() read process selected regions interest.","code":""},{"path":"/reference/reader.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Initialize the pipeline — reader","text":"","code":"reader(x, filter = \"\", buffer = 0, ...)  reader_coverage(x, filter = \"\", buffer = 0, ...)  reader_circles(x, xc, yc, r, filter = \"\", buffer = 0, ...)  reader_rectangles(x, xmin, ymin, xmax, ymax, filter = \"\", buffer = 0, ...)"},{"path":"/reference/reader.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Initialize the pipeline — reader","text":"x Can paths files use, path folder files stored, path virtual point cloud file data.frame containing hte point cloud. supports also LAScatalog LAS objects lidR. filter 'filter' argument allows filtering point-cloud work points interest. available filters LASlib can found running filter_usage. given algorithm filter applied, points meet criteria processes. common strings \"-keep_first\", \"-keep_class 2\", \"drop_z_below 2\". details see filters. buffer numeric. file read buffer. default 0, mean file buffered. means internal routine knows buffer needed pick greatest value internal suggestion provided value. ... passed readers xc, yc, r numeric. Circle centres radius radii. xmin, ymin, xmax, ymax numeric. Coordinates rectangles","code":""},{"path":"/reference/reader.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Initialize the pipeline — reader","text":"","code":"f <- system.file(\"extdata\", \"Topography.las\", package = \"lasR\") read <- reader(f) ans <- processor(read)"},{"path":"/reference/region_growing.html","id":null,"dir":"Reference","previous_headings":"","what":"Region growing — region_growing","title":"Region growing — region_growing","text":"Region growing individual tree segmentation based Dalponte Coomes (2016) algorithm (see reference). Note stage strictly performs segmentation, original method described manuscript also performs pre- post-processing tasks. , tasks expected done user separate functions.","code":""},{"path":"/reference/region_growing.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Region growing — region_growing","text":"","code":"region_growing(   raster,   seeds,   th_tree = 2,   th_seed = 0.45,   th_cr = 0.55,   max_cr = 20,   ofile = temptif() )"},{"path":"/reference/region_growing.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Region growing — region_growing","text":"raster LASRalgoritm. stage producing raster. seeds LASRalgoritm. stage producing points used seeds. th_tree numeric. Threshold pixel tree. Default 2. th_seed numeric. Growing threshold 1. See reference Dalponte et al. 2016. pixel added region height greater tree height multiplied value. 0 1. Default 0.45. th_cr numeric. Growing threshold 2. See reference Dalponte et al. 2016. pixel added region height greater current mean height region multiplied value. 0 1. Default 0.55. max_cr numeric. Maximum value crown diameter detected tree (data units). Default 20. CAREFUL algorithm exists lidR package parameter pixels lidR. ofile character. Full outputs always stored disk. ofile = \"\" stage store result disk return nothing. however hold partial output results temporarily memory. useful stage intermediate stage.","code":""},{"path":"/reference/region_growing.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Region growing — region_growing","text":"Dalponte, M. Coomes, D. . (2016), Tree-centric mapping forest carbon density airborne laser scanning hyperspectral data. Methods Ecol Evol, 7: 1236–1245. doi:10.1111/2041-210X.12575.","code":""},{"path":"/reference/region_growing.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Region growing — region_growing","text":"","code":"f <- system.file(\"extdata\", \"MixedConifer.las\", package=\"lasR\")  reader <- reader(f, filter = keep_first()) reader <- reader(f, filter = keep_first()) chm <- rasterize(1, \"max\") lmx <- local_maximum(5) tree <- region_growing(chm, lmx, max_cr = 10) u <- processor(reader + chm + lmx + tree)"},{"path":"/reference/sampling.html","id":null,"dir":"Reference","previous_headings":"","what":"Sample the point cloud keeping one random point per units — sampling_voxel","title":"Sample the point cloud keeping one random point per units — sampling_voxel","text":"Sample point cloud, keeping one random point per pixel per voxel. stage modifies point cloud pipeline produce output.","code":""},{"path":"/reference/sampling.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Sample the point cloud keeping one random point per units — sampling_voxel","text":"","code":"sampling_voxel(res = 2, filter = \"\")  sampling_pixel(res = 2, filter = \"\")"},{"path":"/reference/sampling.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Sample the point cloud keeping one random point per units — sampling_voxel","text":"res numeric. voxel resolution filter 'filter' argument allows filtering point-cloud work points interest. available filters LASlib can found running filter_usage. given algorithm filter applied, points meet criteria processes. common strings \"-keep_first\", \"-keep_class 2\", \"drop_z_below 2\". details see filters.","code":""},{"path":"/reference/sampling.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Sample the point cloud keeping one random point per units — sampling_voxel","text":"","code":"f <- system.file(\"extdata\", \"Topography.las\", package=\"lasR\") read <- reader(f) vox <- sampling_voxel(5) write <- write_las() pipeline <- read + vox + write processor(pipeline) #> [1] \"/tmp/RtmpfWaf9O/Topography.las\""},{"path":"/reference/summarise.html","id":null,"dir":"Reference","previous_headings":"","what":"Summary — summarise","title":"Summary — summarise","text":"Summarize dataset counting number points, first returns, classes. also produces histogram Z Intensity. stage modify point cloud. produces summary `list`.","code":""},{"path":"/reference/summarise.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Summary — summarise","text":"","code":"summarise(zwbin = 2, iwbin = 25, filter = \"\")"},{"path":"/reference/summarise.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Summary — summarise","text":"zwbin, iwbin numeric. Width bins histograms Z Intensity. filter 'filter' argument allows filtering point-cloud work points interest. available filters LASlib can found running filter_usage. given algorithm filter applied, points meet criteria processes. common strings \"-keep_first\", \"-keep_class 2\", \"drop_z_below 2\". details see filters.","code":""},{"path":"/reference/summarise.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Summary — summarise","text":"","code":"f <- system.file(\"extdata\", \"Topography.las\", package=\"lasR\") read <- reader(f) pipeline <- read + summarise() ans <- processor(pipeline) ans #> $npoints #> [1] 73403 #>  #> $nsingle #> [1] 31294 #>  #> $nwithheld #> [1] 0 #>  #> $nsynthetic #> [1] 0 #>  #> $npoints_per_return #>     1     2     3     4     5     6  #> 53538 15828  3569   451    16     1  #>  #> $npoints_per_class #>     1     2     9  #> 61347  8159  3897  #>  #> $z_histogram #>   788   790   792   794   796   798   800   802   804   806   808   810   812  #>     1   163   265   470   596   694  1610  4955  5510 13833  9974  9865  8076  #>   814   816   818   820   822   824   826   828   830  #>  6643  4682  2958  1715   830   390   146    26     1  #>  #> $i_histogram #>   50   75  100  125  150  175  200  225  250  275  300  325  350  375  400  425  #>    8   51  168  422  485  677  697 1293 1453 1312 1337 1132 1145 1111 1003  962  #>  450  475  500  525  550  575  600  625  650  675  700  725  750  775  800  825  #> 1223 1286 1328 1198 1129 1108 1345 1293 1205 1218 1348 1399 1249 1212 1363 1327  #>  850  875  900  925  950  975 1000 1025 1050 1075 1100 1125 1150 1175 1200 1225  #> 1395 1469 1394 1419 1426 1571 1627 1564 1646 1734 1772 1827 1695 1709 1600 1411  #> 1250 1275 1300 1325 1350 1375 1400 1425 1450 1475 1500 1525 1550 1575 1600 1625  #> 1339 1192 1245 1409 1848 1887 1939 1428  966  544  250  132   91   58   54   52  #> 1650 1675 1700 1725 1750 1775 1800 1825 1850 1875 1900 1925 1950 1975 2000 2025  #>   46   40   30   29   14   14    5    6    5    7    4    6    6    1    4    1  #> 2050 2075 2100 2125 2150 2175 2200 2225 2250 2275 2300 2325 2350 2375 2400 2425  #>    0    1    0    0    0    0    2    1    0    0    0    0    0    0    0    0  #> 2450  #>    1  #>"},{"path":"/reference/temporary_files.html","id":null,"dir":"Reference","previous_headings":"","what":"Temporary files — temporary_files","title":"Temporary files — temporary_files","text":"Convenient functions create temporary file given extension.","code":""},{"path":"/reference/temporary_files.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Temporary files — temporary_files","text":"","code":"temptif()  tempgpkg()  tempshp()  templas()  templaz()"},{"path":"/reference/temporary_files.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Temporary files — temporary_files","text":"string. Path temporary file.","code":""},{"path":"/reference/tools.html","id":null,"dir":"Reference","previous_headings":"","what":"Tools inherited from base R — tools","title":"Tools inherited from base R — tools","text":"Tools inherited base R","code":""},{"path":"/reference/tools.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Tools inherited from base R — tools","text":"","code":"# S3 method for LASRalgorithm print(x, ...)  # S3 method for LASRpipeline print(x, ...)  # S3 method for LASRpipeline +(e1, e2)  # S3 method for LASRpipeline c(...)"},{"path":"/reference/tools.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Tools inherited from base R — tools","text":"x, e1, e2 lasR objects ... lasR objects. equivalent +","code":""},{"path":"/reference/tools.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Tools inherited from base R — tools","text":"","code":"algo1 <- rasterize(1, \"max\") algo2 <- rasterize(4, \"min\") print(algo1) #>  ----------- #> rasterize (uid:cqGOVP) #>   res : 1  #>   window : 1  #>   method : max  #>   filter :   #>   output : /tmp/RtmpfWaf9O/file1f821435e39d.tif  #> ----------- pipeline <- algo1 + algo2 print(pipeline) #>  ----------- #> rasterize (uid:cqGOVP) #>   res : 1  #>   window : 1  #>   method : max  #>   filter :   #>   output : /tmp/RtmpfWaf9O/file1f821435e39d.tif  #> ----------- #> rasterize (uid:PQurPA) #>   res : 4  #>   window : 4  #>   method : min  #>   filter :   #>   output : /tmp/RtmpfWaf9O/file1f826cc61d87.tif  #> -----------"},{"path":"/reference/transform_with.html","id":null,"dir":"Reference","previous_headings":"","what":"Transform a point cloud using another stage — transform_with","title":"Transform a point cloud using another stage — transform_with","text":"stage uses another stage produced Delaunay triangulation raster performs operation modify point cloud. can typically used build normalization stage stage modifies point cloud pipeline produce output.","code":""},{"path":"/reference/transform_with.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Transform a point cloud using another stage — transform_with","text":"","code":"transform_with(stage, operator = \"-\", store_in_attribute = \"\")"},{"path":"/reference/transform_with.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Transform a point cloud using another stage — transform_with","text":"stage LASRpipeline. stage produces triangulation raster. operator string. '-' '+' supported. store_in_attribute numeric. Use extra bytes attribute store result.","code":""},{"path":[]},{"path":"/reference/transform_with.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Transform a point cloud using another stage — transform_with","text":"","code":"f <- system.file(\"extdata\", \"Topography.las\", package=\"lasR\")  # There is a normalize pipeline in lasR but let's create one almost equivalent mesh  <- triangulate(filter = keep_ground()) trans <- transform_with(mesh) pipeline <- reader(f) + mesh + trans + write_las() ans <- processor(pipeline) #> Warning: 160 points outside delaunay triangulation were discarded"},{"path":"/reference/triangulate.html","id":null,"dir":"Reference","previous_headings":"","what":"Delaunay triangulation — triangulate","title":"Delaunay triangulation — triangulate","text":"Delaunay triangulation. Can used build DTM, CHM, normalize point cloud, application. stage typically used intermediate process without output file. stage modify point cloud.","code":""},{"path":"/reference/triangulate.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Delaunay triangulation — triangulate","text":"","code":"triangulate(max_edge = 0, filter = \"\", ofile = \"\", use_attribute = \"Z\")"},{"path":"/reference/triangulate.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Delaunay triangulation — triangulate","text":"max_edge numeric. Maximum edge length triangle Delaunay triangulation. triangle edge length greater value, removed. max_edge = 0, trimming done (see examples). filter 'filter' argument allows filtering point-cloud work points interest. available filters LASlib can found running filter_usage. given algorithm filter applied, points meet criteria processes. common strings \"-keep_first\", \"-keep_class 2\", \"drop_z_below 2\". details see filters. ofile character. Full outputs always stored disk. ofile = \"\" stage store result disk return nothing. however hold partial output results temporarily memory. useful stage intermediate stage. use_attribute character. default triangulation performed coordinate Z. Can also name extra bytes attribute 'HAG' exists. Can also 'Intensity'.","code":""},{"path":"/reference/triangulate.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Delaunay triangulation — triangulate","text":"","code":"f <- system.file(\"extdata\", \"Topography.las\", package=\"lasR\") read <- reader(f) tri1 <- triangulate(25, filter = keep_ground(), ofile = tempgpkg()) filter <- \"-keep_last -keep_random_fraction 0.1\" tri2 <- triangulate(filter = filter, ofile = tempgpkg()) pipeline <- read + tri1 + tri2 ans <- processor(pipeline) #plot(ans[[1]]) #plot(ans[[2]])"},{"path":"/reference/write_las.html","id":null,"dir":"Reference","previous_headings":"","what":"Write LAS or LAZ files — write_las","title":"Write LAS or LAZ files — write_las","text":"Write LAS LAZ file step pipeline (typically end). Unlike stages, output written single large file multiple tiled files corresponding original collection files.","code":""},{"path":"/reference/write_las.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Write LAS or LAZ files — write_las","text":"","code":"write_las(   ofile = paste0(tempdir(), \"/*.las\"),   filter = \"\",   keep_buffer = FALSE )"},{"path":"/reference/write_las.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Write LAS or LAZ files — write_las","text":"ofile character. Output file names. string must contain wildcard * wildcard can replaced name original tile preserve tiling pattern. wildcard omitted, everything written single file. may desired behavior circumstances, e.g., merge files. filter 'filter' argument allows filtering point-cloud work points interest. available filters LASlib can found running filter_usage. given algorithm filter applied, points meet criteria processes. common strings \"-keep_first\", \"-keep_class 2\", \"drop_z_below 2\". details see filters. keep_buffer bool. buffer removed write file can preserved.","code":""},{"path":"/reference/write_las.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Write LAS or LAZ files — write_las","text":"","code":"f <- system.file(\"extdata\", \"Topography.las\", package=\"lasR\") read <- reader(f) tri  <- triangulate(filter = keep_ground()) normalize <- tri + transform_with(tri) pipeline <- read + normalize + write_las(paste0(tempdir(), \"/*_norm.las\")) processor(pipeline) #> Warning: 160 points outside delaunay triangulation were discarded #> [1] \"/tmp/RtmpfWaf9O/Topography_norm.las\""},{"path":"/reference/write_vpc.html","id":null,"dir":"Reference","previous_headings":"","what":"Write a Virtual Point Cloud — write_vpc","title":"Write a Virtual Point Cloud — write_vpc","text":"Borrowing concept virtual rasters GDAL, VPC file format references point cloud files virtual point cloud (VPC)","code":""},{"path":"/reference/write_vpc.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Write a Virtual Point Cloud — write_vpc","text":"","code":"write_vpc(ofile)"},{"path":"/reference/write_vpc.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Write a Virtual Point Cloud — write_vpc","text":"ofile character. file path extnsion .vpc write virtual point cloud file","code":""},{"path":"/reference/write_vpc.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Write a Virtual Point Cloud — write_vpc","text":"https://www.lutraconsulting.co.uk/blog/2023/06/08/virtual-point-clouds/https://github.com/PDAL/wrench/blob/main/vpc-spec.md","code":""},{"path":"/reference/write_vpc.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Write a Virtual Point Cloud — write_vpc","text":"","code":"if (FALSE) { pipeline = reader(\"folder/\") + write_vpc(\"folder/dataset.vpc\") }"},{"path":"/news/index.html","id":"lasr-021-2024-03-05","dir":"Changelog","previous_headings":"","what":"lasR 0.2.1 (2024-03-05)","title":"lasR 0.2.1 (2024-03-05)","text":"Fix: callback() properly handles errors injected function New: handy functions tempxyz() generate temp files extension .xyz. New: rasterize() now parallelized internal metrics including buffered area based approach New: rasterize() gained progress bar internal metrics.","code":""},{"path":"/news/index.html","id":"lasr-020-2024-03-01","dir":"Changelog","previous_headings":"","what":"lasR 0.2.0 (2024-03-01)","title":"lasR 0.2.0 (2024-03-01)","text":"New: rasterize() gains ability perform multi-resolution buffered rasterization. See documentation. New: rasterize() gains numerous native metrics zmax, zmean, zmedian, imax, imean . New: internal engine gains ability skip processing files collection use files load buffer. feature works LAScatalog lidR respecting processed attribute used lidR Fix: loading package offline created bug R longer handles errors.","code":""},{"path":"/news/index.html","id":"lasr-012-2024-02-10","dir":"Changelog","previous_headings":"","what":"lasR 0.1.2 (2024-02-10)","title":"lasR 0.1.2 (2024-02-10)","text":"New: progress bar reading header files (LAScatalog) can enabled progress = TRUE Fix: progress bar starts appear earlier .e. 0%. pipeline affects feeling progress.","code":""},{"path":"/news/index.html","id":"lasr-011-2024-02-08","dir":"Changelog","previous_headings":"","what":"lasR 0.1.1 (2024-02-08)","title":"lasR 0.1.1 (2024-02-08)","text":"Doc: Corrected documentation argument ncores processor(), incorrectly mentioned supported. New: Added new functions ncores() half_cores(). Fix: Corrected reader progress bar display reading las file filter buffer. Fix: Fixed overall progress bar, delayed one file showing incorrect progress.","code":""}]
