[{"path":"/articles/lasR1.html","id":"rationnale-for-lasr-vs--lidr","dir":"Articles","previous_headings":"","what":"Rationnale for lasR vs.Â lidR","title":"1. Why lasR?","text":"need new package? short answer lies following graph. x-axis represents time perform three different rasterizations (CHM, DTM, density map), y-axis represents amount RAM memory used lidR lasR (details benchmark vignette). lasR intended much efficient lidR terms memory usage computation times.  second issue absence powerful pipeline engine lidR. Performing task simple extracting deriving metrics multiple inventory plots non-normalized collection files easy lidR. straightforward point cloud normalized, , users must write complex custom script. introduction real pipelines, lasR enables users complex tasks easier way (see tutorial vignette well pipeline vignette). Last least, almost decade additional experience R, C++, point cloud processing, lot feedback compared started creation lidR. simply technically capable writing lasR ten years ago!","code":""},{"path":[]},{"path":"/articles/lasR1.html","id":"pipeline","dir":"Articles","previous_headings":"Main differences between lasR and lidR","what":"Pipeline","title":"1. Why lasR?","text":"lasR introduces versatile pipeline engine, enabling creation complex processing pipelines. Users can simultaneously create ABA compute DTM one read pass, leading significant speed-.","code":""},{"path":"/articles/lasR1.html","id":"data-loading","dir":"Articles","previous_headings":"Main differences between lasR and lidR","what":"Data loading","title":"1. Why lasR?","text":"Unlike lidR, lasR load lidar data data.frame. designed efficient data processing, memory management C++ level. Consequently, read_las() function. Everything internally efficiently stored C++ structure keeps data compact memory. However, entry points available inject user-defined R code C++ pipeline.","code":""},{"path":"/articles/lasR1.html","id":"dependencies","dir":"Articles","previous_headings":"Main differences between lasR and lidR","what":"Dependencies","title":"1. Why lasR?","text":"lasR one dependency, boost. doesnâ€™t even depend Rcpp. lasR use terra sf R level reading writing spatial data; instead, links GDAL. terra sf installed, output files read packages. Due absence dependency R package non-loading data R objects, also dependency rgl, resulting interactive 3D viewer like lidR.","code":""},{"path":"/articles/lasR1.html","id":"code","dir":"Articles","previous_headings":"Main differences between lasR and lidR","what":"Code","title":"1. Why lasR?","text":"lasR written 100% C++ contains R code. utilizes source code lidR significant improvements. major improvements observed benchmark section much source code rather organization code, .e., longer using data.frame, memory management C++ rather R, processing R level, pipelines, .","code":""},{"path":"/articles/lasR1.html","id":"should-i-use-lidr-or-lasr","dir":"Articles","previous_headings":"","what":"Should I use lidR or lasR?","title":"1. Why lasR?","text":"question actually pretty simple answer. want explore, manipulate, test, try, retry, implement new ideas mind, use lidR. know want, want relatively common (raster metrics, DTM, CHM, tree location), especially want large coverage, use lasR.","code":""},{"path":"/articles/lasR1.html","id":"example-1","dir":"Articles","previous_headings":"Should I use lidR or lasR?","what":"Example 1","title":"1. Why lasR?","text":"received 500 kmÂ² data, want CHM DTM. â†’ Use lasR compute fast possible.","code":""},{"path":"/articles/lasR1.html","id":"example-2","dir":"Articles","previous_headings":"Should I use lidR or lasR?","what":"Example 2","title":"1. Why lasR?","text":"want segment trees, explore different methods, test different parameters small plots. Maybe integrate custom step, â€™s exploratory process. â†’ Use lidR.","code":""},{"path":"/articles/lasR1.html","id":"example-3","dir":"Articles","previous_headings":"Should I use lidR or lasR?","what":"Example 3","title":"1. Why lasR?","text":"want extract circular ground inventories compute metrics plot. dataset already normalized, can use either lasR lidR; pretty much equivalent. lidR easier use; lasR little bit efficient difficult use (yet pipeline vignette contains copy-pastable code ). dataset normalized, lasR much simpler case, thanks pipeline processor allows adding normalization stage computing metrics.","code":""},{"path":"/articles/lasR1.html","id":"example-4","dir":"Articles","previous_headings":"Should I use lidR or lasR?","what":"Example 4","title":"1. Why lasR?","text":"want create complex pipeline computes local shape points classify roofs wires point cloud. using shapefile, want classify water point cloud. finish, want write new classified LAS files. â†’ Use lidR. lasR many tools. lasR lidR; much efficient less versatile fewer tools.","code":""},{"path":"/articles/lasR1.html","id":"example-5","dir":"Articles","previous_headings":"Should I use lidR or lasR?","what":"Example 5","title":"1. Why lasR?","text":"want find segment trees common algorithm. Nothing fancy. want 100 kmÂ² . â†’ Use lasR. lidR probably fail .","code":""},{"path":"/articles/lasR2.html","id":"reader","dir":"Articles","previous_headings":"","what":"Reader","title":"2. Tutorial","text":"reader stage must first stage pipeline. takes input list files process. creating pipeline stage, files read, computation actually applied. points even read. result returned.","code":"read = reader(f) processor(read) #> NULL"},{"path":"/articles/lasR2.html","id":"triangulate","dir":"Articles","previous_headings":"","what":"Triangulate","title":"2. Tutorial","text":"first stage can try triangulate(). algorithm performs Delaunay triangulation points interest. Triangulating points useful task employed numerous processing tasks. Triangulating points interesting, usually want use filter argument triangulate specific points interest. following example, triangulate points classified 2 (.e., ground). produces meshed Digital Terrain Model. example, files read sequentially, points loaded one one stored build Delaunay triangulation. lasR, one file stored memory time. program stores point cloud Delaunay triangulation current processing file. data discarded load new file. users provide path output file store result, result lost. following pipeline, building triangulation ground points, get output everything lost. following pipeline triangulation stored geopackage file providing argument ofile:  can also triangulate first returns. produce meshed Digital Surface Model. can also perform triangulations pipeline. idea lasR execute tasks one pass using pipeline: Using triangulate() without stage pipeline usually useful. Typically, triangulate() employed without ofile argument intermediate step. instance, can used rasterize().","code":"read = reader(f) del = triangulate(filter = keep_ground()) ans = processor(read+del) ans #> NULL read = reader(f) del = triangulate(filter = keep_ground(), ofile = tempfile(fileext = \".gpkg\")) ans = processor(read+del) ans #> Simple feature collection with 1 feature and 0 fields #> Geometry type: MULTIPOLYGON #> Dimension:     XYZ #> Bounding box:  xmin: 273357.2 ymin: 5274357 xmax: 273642.9 ymax: 5274643 #> z_range:       zmin: 788.9932 zmax: 814.8322 #> Projected CRS: NAD83(CSRS) / MTM zone 7 #> # A tibble: 1 Ã— 1 #>                                                                             geom #>                                                               <MULTIPOLYGON [m]> #> 1 Z (((273357.5 5274626 805.9742, 273357.4 5274633 804.6458, 273357.4 5274634 8â€¦  par(mar = c(2, 2, 1, 1)) plot(ans, axes = T, lwd = 0.5) read = reader(f) del = triangulate(filter = keep_first(), ofile = tempfile(fileext = \".gpkg\")) ans = processor(read+del) read = reader(f) del1 = triangulate(filter = \"-keep_class 2\", ofile = tempfile(fileext = \".gpkg\")) del2 = triangulate(filter = \"-keep first\", ofile = tempfile(fileext = \".gpkg\")) pipeline = read + del1 + del2 ans = processor(pipeline)"},{"path":"/articles/lasR2.html","id":"rasterize","dir":"Articles","previous_headings":"","what":"Rasterize","title":"2. Tutorial","text":"rasterize() exactly users may expect even . three variations: Rasterize predefined operators. operators optimized internally, making operations fast possible, number registered operators low Rasterize injecting user-defined R expression. equivalent pixel_metrics() package lidR. user-defined function can mapped, making extremely versatile slower limited speed quality R code injected. Rasterize Delaunay triangulation. variations, users can build CHM, DTM, predictive model, anything else. Letâ€™s build DTM using triangulation ground points rasterize() stage. following pipeline, LAS files read, points loaded LAS file buffer, Delaunay triangulation ground points built, triangulation interpolated rasterized. default, rasterize() writes raster temporary file, result discarded. , processor() returns SpatRaster triangulate() returns nothing (NULL). Therefore, pipeline contains two elements, one returns something.  Notice , contrary lidR package, usually high-level function names like rasterize_terrain(). Instead, lasR made low-level functions versatile also challenging use. Letâ€™s build two CHMs: one based highest point per pixel resolution 2 meters, second based triangulation first returns resolution 50 cm. following pipeline, using two variations rasterize(): one capable rasterizing triangulation capable rasterizing point cloud predefined operator (max). output named list two SpatRaster.  simplicity package pre-installed pipelines named chm() dtm() explained . Last least, letâ€™s compute map median intensity injecting user-defined expression. Like lidR, attributes point cloud named: X, Y, Z, Intensity, gpstime, ReturnNumber, NumberOfreturns, Classification, UserData, PointSourceID, R, G, B, NIR. users familiar lidR package, note ScanAngleRank/ScanAngle; instead scanner angle always named ScanAngle numeric. Also flags named Withheld, Synthetic Keypoint.","code":"read = reader(f) del = triangulate(filter = \"-keep_class 2\") dtm = rasterize(1, del) pipeline = read + del + dtm ans = processor(pipeline) ans #> class       : SpatRaster  #> dimensions  : 286, 286, 1  (nrow, ncol, nlyr) #> resolution  : 1, 1  (x, y) #> extent      : 273357, 273643, 5274357, 5274643  (xmin, xmax, ymin, ymax) #> coord. ref. : NAD83(CSRS) / MTM zone 7 (EPSG:2949)  #> source      : file21b844e1c59c.tif  #> name        : file21b844e1c59c terra::plot(ans, col = gray.colors(25,0,1), mar = c(1, 1, 1, 3)) read <- reader(f) del <- triangulate(filter = \"-keep_first\") chm1 <- rasterize(2, \"max\") chm2 <- rasterize(0.5, del) pipeline <- read + del + chm1 + chm2 ans <- processor(pipeline)  terra::plot(ans[[1]], mar = c(1, 1, 1, 3), col = col) terra::plot(ans[[2]], mar = c(1, 1, 1, 3), col = col) read = reader(f) avgi = rasterize(10, median(Intensity)) pipeline = read + avgi ans = processor(pipeline)  terra::plot(ans, mar = c(1, 1, 1, 3), col = heat.colors(15))"},{"path":"/articles/lasR2.html","id":"transform-with-triangulation","dir":"Articles","previous_headings":"","what":"Transform with triangulation","title":"2. Tutorial","text":"Another way use Delaunay triangulation transform point cloud. can add subtract triangulation point cloud, effectively normalizing . Unlike lidR package, high-level function names like normalize_points(). Instead, lasR composed low-level functions offer versatility. Letâ€™s normalize point cloud using triangulation ground points (meshed DTM). following example, triangulation used transform_with_triangulation() modifies point cloud pipeline. triangulate() transform_with_triangulation() return nothing. output NULL. convenience pipeline pre-recorded package name normalize(). obtain meaningful output, necessary chain another stage. point cloud modified discarded. instance, can compute Canopy Height Model (CHM) normalized point cloud. following pipeline, first rasterization (chm1) applied normalization, second rasterization occurs transform_with_triangulation(), thus applied transformed point cloud.  performing normalization, users may want write normalized point cloud disk later use. case, can append write_las() stage pipeline.","code":"read = reader(f) del = triangulate(filter = \"-keep_class 2\") norm = transform_with_triangulation(del, \"-\") pipeline = read + del + norm ans = processor(pipeline) ans #> NULL read = reader(f) del = triangulate(filter = \"-keep_class 2\") norm = transform_with_triangulation(del, \"-\") chm1 = rasterize(2, \"max\") chm2 = rasterize(2, \"max\") pipeline = read + chm1 + del + norm + chm2 ans = processor(pipeline)  col = grDevices::colorRampPalette(c(\"blue\", \"cyan2\", \"yellow\", \"red\"))(15) terra::plot(c(ans[[1]], ans[[2]]), col = col)"},{"path":"/articles/lasR2.html","id":"write-las","dir":"Articles","previous_headings":"","what":"Write LAS","title":"2. Tutorial","text":"write_las() can called point pipeline. writes one file per input file, using name input files added prefixes suffixes. following pipeline, read files, write ground points files named original files suffix _ground, perform triangulation entire point cloud, followed normalization. Finally, write normalized point cloud suffix _normalized. crucial include wildcard * file path; otherwise, single large file created. behaviour may intentional. Letâ€™s consider creating file merge pipeline. following example, wildcard * used names LAS/LAZ files. input files read, points sequentially written single file dataset_merged.laz, naturally forming merge pipeline.","code":"read = reader(f) write1 = write_las(paste0(tempdir(), \"/*_ground\"), filter = \"-keep_class 2\") write2 = write_las(paste0(tempdir(), \"/*_normalized.laz\"), ) del = triangulate(filter = \"-keep_class 2\") norm = transform_with_triangulation(del, \"-\") pipeline = read + write1 + del + norm + write2 ans = processor(pipeline) ans #> $write_las #> [1] \"/tmp/RtmpkJZCz4/bcts_1_ground\" \"/tmp/RtmpkJZCz4/bcts_2_ground\" #>  #> $write_las #> [1] \"/tmp/RtmpkJZCz4/bcts_1_normalized.laz\" #> [2] \"/tmp/RtmpkJZCz4/bcts_2_normalized.laz\" read = reader(f) ofile = paste0(tempdir(), \"/dataset_merged.laz\") merge = write_las(ofile) ans = processor(read + merge) ans #> [1] \"/tmp/RtmpkJZCz4/dataset_merged.laz\""},{"path":"/articles/lasR2.html","id":"callback","dir":"Articles","previous_headings":"","what":"Callback","title":"2. Tutorial","text":"callback stage holds significant importance second last entry point inject R code pipeline, following rasterize(). familiar lidR package, initial step often involves reading data lidR::readLAS() expose point cloud data.frame object R. contrast, lasR loads point cloud optimally C++ without exposing directly R. However, callback, becomes possible expose point cloud data.frame executing specific R functions. Similar lidR, attributes point cloud lasR named: X, Y, Z, Intensity, gpstime, ReturnNumber, NumberOfreturns, Classification, UserData, PointSourceID, R, G, B, NIR. Notably, users accustomed lidR package, scanner angle consistently named ScanAngle numeric, opposed ScanAngleRank/ScanAngle. Additionally, flags named Withheld, Synthetic, Keypoint. Letâ€™s delve simple example. LAS file, callback loads point cloud data.frame invokes meanz() function data.frame. mindful , given LAS/LAZ file, point cloud may contain points original file file loaded buffer. clarification matter provided later. callback function versatile can also employed edit point cloud. user-defined function returns data.frame number rows original one, function edits underlying C++ dataset. enables users perform tasks assigning class specific point. physically removing points possible, users can flag points Withheld. cases, points processed subsequent stages observed, , callback explicitly return anything; however, edits point cloud without producing visible result. generate output, users must use write_las(). â€™s important note write_las() write point number 13 flagged withheld. Now, letâ€™s explore capabilities callback . First, letâ€™s create lidR-like read_las() function expose point cloud R. following example, user-defined function employed return data.frame . userâ€™s function returns data.frame number points original dataset, updates points C++ level. , use no_las_update = TRUE explicitly return result. Ground points can also classified using R function, one provided RCSF package:","code":"meanz = function(data){ return(mean(data$Z)) } read = reader(f) call = callback(meanz, expose = \"xyz\") ans = processor(read+call) ans #> $Topography #> [1] 809.0835 #>  #> $Megaplot #> [1] 13.27202 edit_points = function(data) {   data$Classification[5:7] = c(2L,2L,2L)   data$Withheld = FALSE   data$Withheld[12] = TRUE   return(data) }  read = reader(f) call = callback(edit_points, expose = \"xyzc\") ans = processor(read+call) ans #> NULL read_las = function(f, select = \"xyzi\", filter = \"\") {   load = function(data) { return(data) }   read = reader(f, filter = filter)   call = callback(load, expose = select, no_las_update = TRUE)   return (processor(read+call)) }  f <- system.file(\"extdata\", \"Topography.las\", package=\"lasR\") las = read_las(f) head(las) #>          X       Y        Z Intensity #> 1 273357.1 5274360 806.5340      1340 #> 2 273357.2 5274359 806.5635       728 #> 3 273357.2 5274358 806.0248      1369 #> 4 273357.2 5274510 809.6303       589 #> 5 273357.2 5274509 809.3880      1302 #> 6 273357.2 5274508 809.4847       123 csf = function(data) {   id = RCSF::CSF(data)   class = integer(nrow(data))   class[id] = 2L   data$Classification <- class   return(data) }  read = reader(f) classify = callback(csf, expose = \"xyz\") write = write_las() pipeline = read + classify + write processor(pipeline)"},{"path":"/articles/lasR2.html","id":"tree-segmentation","dir":"Articles","previous_headings":"","what":"Tree Segmentation","title":"2. Tutorial","text":"section presents complex pipeline tree segmentation using local_maximum() identify tree tops, region_growing() segment trees using seeds produced local_maximum(), Canopy Height Model (CHM) produced using Delaunay triangulation first returns. CHM post-processed pit_fill(), algorithm designed enhance CHM filling pits NAs. tutorial, pipeline tested one file render page faster. However, pipeline can applied number files produce contiguous output, managing buffer files. Every intermediate output can exported, tutorial, export everything display outputs.","code":"read = reader(f) del = triangulate(filter = \"-keep_first\") chm = rasterize(0.5, del) chm2 = pit_fill(chm) seed = local_maximum(3) tree = region_growing(chm2, seed) pipeline = read + del + chm + chm2 +  seed + tree ans = processor(pipeline) col = grDevices::colorRampPalette(c(\"blue\", \"cyan2\", \"yellow\", \"red\"))(25) terra::plot(ans$rasterize, col = col, mar = c(1, 1, 1, 3)) terra::plot(ans$pit_fill, col = col, mar = c(1, 1, 1, 3)) terra::plot(ans$region_growing, col = col[sample.int(25, 297, TRUE)], mar = c(1, 1, 1, 3)) plot(ans$local_maximum$geom, add = T, pch = 19, cex = 0.5)"},{"path":"/articles/lasR2.html","id":"buffer","dir":"Articles","previous_headings":"","what":"Buffer","title":"2. Tutorial","text":"Point clouds typically stored multiple contiguous files. avoid edge artifacts, file must loaded extra points coming neighbouring files. Everything handled automatically, except callback() algorithm. callback(), point cloud exposed data.frame buffer, providing user-defined function spatial context. callback used edit points, everything handled internally. However, R object returned, responsibility user handle buffer. example, following pipeline, processing two files, callback() used count number points. presence triangulate() implies file loaded buffer. Consequently, counting points callback() returns points summarise() summarise() knows deal buffer. can compare pipeline without triangulate(). case, reason use buffer, files buffered. counts equal. handle buffer, user can read attribute bbox data.frame. contains bounding box point cloud without buffer use column Buffer contains TRUE FALSE point. TRUE, point buffer. buffer exposed user includes letter 'b'. conclusion, hypothesis user-defined function returns something complex, two ways handle buffer: either using bounding box using Buffer flag. third option use drop_buffer. case users ensure receive data.frame include points buffer.","code":"count = function(data) { length(data$X) } read = reader(f) del = triangulate(filter = \"-keep_class 2\") npts = callback(count, expose = \"x\") sum = summarise() ans = processor(read + del + npts + sum) ans$callback #> $bcts_1 #> [1] 682031 #>  #> $bcts_2 #> [1] 931581 ans$callback[[1]]+ ans$callback[[2]] #> [1] 1613612 ans$summary$npoints #> [1] 1355607 ans = processor(read + npts + sum) ans$callback[[1]]+ ans$callback[[2]] #> [1] 1355607 ans$summary$npoints #> [1] 1355607 count_buffer_aware = function(data) {   bbox = attr(data, \"bbox\")   npoints = sum(!data$Buffer)   return(list(bbox = bbox, npoints = npoints)) }  read = reader(f) del = triangulate(filter = \"-keep_class 2\") npts = callback(count_buffer_aware, expose = \"b\") # b for buffer sum = summarise() ans = processor(read + del + npts + sum) ans$callback #> $bcts_1 #> $bcts_1$bbox #> [1] 885022.4 629157.2 885210.2 629400.0 #>  #> $bcts_1$npoints #> [1] 531662 #>  #>  #> $bcts_2 #> $bcts_2$bbox #> [1] 885024.1 629400.0 885217.1 629700.0 #>  #> $bcts_2$npoints #> [1] 823945 ans$callback[[1]]$npoints+ ans$callback[[2]]$npoints #> [1] 1355607 ans$summary$npoints #> [1] 1355607"},{"path":"/articles/lasR2.html","id":"hulls","dir":"Articles","previous_headings":"","what":"Hulls","title":"2. Tutorial","text":"Delaunay triangulation defines convex polygon, represents convex hull points. However, dense point clouds, removing triangles large edges due absence points results complex structure.  hulls() algorithm computes contour mesh, producing convex hull holes:  However hulls() likely used without triangulation. case returns bounding box LAS/LAZ file read header. used triangulate(0) returns convex hull inefficient way get convex hull.","code":"read = reader(f) del = triangulate(15, filter = keep_ground(), ofile = tempfile(fileext = \".gpkg\")) ans = processor(read+del)  par(mar = c(2, 2, 1, 1)) plot(ans, axes = T, lwd = 0.5) read = reader(f) del = triangulate(15, filter = keep_ground()) bound = hulls(del) ans = processor(read+del+bound) #> Warning in processor(read + del + bound): GDAL Message 1: Hole lies outside #> shell at or near point 273359.81374999997 5274519.7364999996  par(mar = c(2, 2, 1, 1)) plot(ans, axes = T, lwd = 0.5, col = \"gray\")"},{"path":"/articles/lasR2.html","id":"sampling-voxel","dir":"Articles","previous_headings":"","what":"Sampling Voxel","title":"2. Tutorial","text":"sampling_voxel() summarise() functions operate similarly algorithms, output depends position pipeline:","code":"read = reader(f) pipeline = read + summarise() + sampling_voxel(4) + summarise() ans = processor(pipeline) head(ans[[1]]) #> $npoints #> [1] 73403 #>  #> $nsingle #> [1] 31294 #>  #> $nwithheld #> [1] 0 #>  #> $nsynthetic #> [1] 0 #>  #> $npoints_per_return #>     1     2     3     4     5     6  #> 53538 15828  3569   451    16     1  #>  #> $npoints_per_class #>     1     2     9  #> 61347  8159  3897 head(ans[[2]]) #> $npoints #> [1] 12745 #>  #> $nsingle #> [1] 5031 #>  #> $nwithheld #> [1] 0 #>  #> $nsynthetic #> [1] 0 #>  #> $npoints_per_return #>    1    2    3    4    5  #> 9410 2596  651   84    4  #>  #> $npoints_per_class #>     1     2     9  #> 10815  1554   376"},{"path":"/articles/lasR2.html","id":"readers","dir":"Articles","previous_headings":"","what":"Readers","title":"2. Tutorial","text":"several readers hidden behind reader(): reader_coverage(): read files process entire point cloud. reader_rectangles(): read rectangular regions interest coverage process sequentially. reader_circles(): read circular regions interest coverage process sequentially. following pipeline triangulates ground points, normalizes point cloud, computes metric interest file entire coverage. file loaded buffer triangulation performed without edge artifacts. Notice use drop_buffer = TRUE expose data.frame without buffer used perform triangulation normalization. following pipeline, contrary, works exactly operates circular plots. readers allow building ground inventory pipeline, example.","code":"my_metric_fun = function(data) { mean(data$Z) } tri <- triangulate(filter = keep_ground()) trans <- transform_with_triangulation(tri) norm <- tri + trans metric <- callback(my_metric_fun, expose = \"z\", drop_buffer = TRUE) pipeline = reader(f) + norm + metric pipeline = reader_circles(f, xcenter, ycenter, 11.28) + tri + norm + metrics"},{"path":"/articles/lasR2.html","id":"plot-inventory","dir":"Articles","previous_headings":"","what":"Plot inventory","title":"2. Tutorial","text":"pipeline extracts plot inventory using shapefile non-normalized point cloud, normalizes plot, computes metrics plot, writes normalized non-normalized plot separate files. circular plot loaded buffer perform correct triangulation. plots exposed R without buffer using drop_buffer = TRUE.","code":"ofiles_plot <- paste0(tempdir(), \"/plot_*.las\") ofiles_plot_norm <- paste0(tempdir(), \"/plot_*_norm.las\")  my_metric_fun = function(data) { mean(data$Z) }  library(sf) inventory <- st_read(\"shapefile.shp\") coordinates <- st_coordinates(inventory) xcenter <- coordinates[,1] ycenter <- coordinates[,2]  read <- reader(f, xc = xcenter, yc = ycenter, r = 11.28)  tri <- triangulate(filter = keep_ground()) trans <- transform_with_triangulation(tri) norm <- tri + trans metric <- callback(my_metric_fun, expose = \"z\", drop_buffer = TRUE) write1 <- write_las(ofiles_plot) write2 <- write_las(ofiles_plot_norm)  pipeline = read + trans + write1 + norm + write2"},{"path":"/articles/lasR2.html","id":"wildcard-usage","dir":"Articles","previous_headings":"","what":"Wildcard Usage","title":"2. Tutorial","text":"Usually, write_las() used wildcard ofile argument (see ) write one file per processed file. Otherwise, everything written single massive LAS file (might desired behaviour). contrary, rasterize() used without wildcard write everything single raster file, also accepts wildcard write results multiple files, useful reader_circles() avoid one massive raster mostly empty. Compare pipeline without wildcard. Without wildcard, output single raster covers entire point cloud two patches populated pixels.  wildcard, output contains two rasters cover regions interest.","code":"ofile = paste0(tempdir(), \"/chm.tif\")   # no wildcard  x = c(885100, 885100) y = c(629200, 629600)  pipeline = reader(f, xc = x, yc = y, r = 20) + rasterize(2, \"max\", ofile = ofile) r0 = processor(pipeline)  terra::plot(r0, col = col) # covers the entire collection of files ofile = paste0(tempdir(), \"/chm_*.tif\") # wildcard  x = c(885100, 885100) y = c(629200, 629600)  pipeline = reader(f, xc = x, yc = y, radius = 20) + rasterize(2, \"max\", ofile = ofile) ans = processor(pipeline)  r1 = terra::rast(ans[1]) r2 = terra::rast(ans[2]) terra::plot(r1, col = col) terra::plot(r2, col = col)"},{"path":"/articles/lasR2.html","id":"compatibility-with-lidr","dir":"Articles","previous_headings":"","what":"Compatibility with lidR","title":"2. Tutorial","text":"lasR depends lidR compatibility . Instead providing paths files folder possible pass LAScatalog LAS object readers.","code":"library(lasR) library(lidR)  ctg = readLAScatalog(folder) pipeline = reader(ctg) + normalize() + write_las() ans = processor(pipeline)  las = readLAS(file) pipeline = reader(las) + normalize() + write_las() ans = processor(pipeline)"},{"path":[]},{"path":"/articles/lasR3.html","id":"normalize","dir":"Articles","previous_headings":"Pipeline factories","what":"Normalize","title":"3. Pipelines","text":"pipeline already installed package. can used way","code":"normalize = function(extrabytes = FALSE) {   tri <- triangulate(filter = keep_ground())   pipeline <- tri      if (extrabytes)   {     extra = add_extrabytes(\"int\", \"HAG\", \"Height Above Ground\")     trans = transform_with_triangulation(tri, store_in_attribute = \"HAG\")     pipeline = pipeline + extra + trans   }   else   {     trans = transform_with_triangulation(tri)     pipeline = pipeline + trans   }      return(pipeline) } pipeline = reader(f) + normalize() + write_las(o)"},{"path":"/articles/lasR3.html","id":"classify-ground-with-csf","dir":"Articles","previous_headings":"Pipeline factories","what":"Classify ground with CSF","title":"3. Pipelines","text":"","code":"ground_csf = function(smooth = FALSE, threshold = 0.5, resolution = 0.5, rigidness = 1L, iterations = 500L, step = 0.65) {   csf = function(data, smooth, threshold, resolution, rigidness, iterations, step)   {     id = RCSF::CSF(data, smooth, threshold, resolution, rigidness, iterations, step)     class = integer(nrow(data))     class[id] = 2L     data$Classification <- class     return(data)   }      classify = callback(csf, expose = \"xyz\", smooth = smooth, threshold = threshold, resolution = resolution, rigidness = rigidness, iterations = iterations, step = step)   return(classify) } pipeline = reader(f) + ground_csf() + write_las(o)"},{"path":"/articles/lasR3.html","id":"classify-ground-with-mcc","dir":"Articles","previous_headings":"Pipeline factories","what":"Classify ground with MCC","title":"3. Pipelines","text":"","code":"ground_mcc = function(s = 1.5, t = 0.3) {   csf = function(data, s, t)   {     id = RMCC::MCC(data, s, t)     class = integer(nrow(data))     class[id] = 2L     data$Classification <- class     return(data)   }      classify = callback(csf, expose = \"xyz\", s = s, t = t)   return(classify) } pipeline = reader(f) + ground_mcc() + write_las(o)"},{"path":"/articles/lasR3.html","id":"canopy-heigh-model","dir":"Articles","previous_headings":"Pipeline factories","what":"Canopy Heigh Model","title":"3. Pipelines","text":"two pipelines natively installed package name chm().","code":"chm_p2r = function(res, filter = \"\", ofile = tempfile(fileext = \".tif\")) {   return(rasterize(res, \"max\", filter = filter, ofile = ofile)) } chm_tin = function(res, ofile = tempfile(fileext = \".tif\")) {   tin = triangulate(filter = keep_first())   chm = rasterize(res, tin, ofile = ofile)   return(tin+chm) }"},{"path":"/articles/lasR3.html","id":"digital-terrain-model","dir":"Articles","previous_headings":"Pipeline factories","what":"Digital Terrain Model","title":"3. Pipelines","text":"one also natively installed package. add_class can used add class used ground 9 water.","code":"dtm = function(res, ofile = tempfile(fileext = \".tif\"), add_class = NULL) {   filter = keep_ground()   if (!is.null(add_class)) filter = filter + keep_class(add_class)   tin = triangulate(filter = filter)   chm = rasterize(res, tin, ofile = ofile)   return(tin+chm) }"},{"path":[]},{"path":"/articles/lasR3.html","id":"read-las","dir":"Articles","previous_headings":"Useful functions","what":"Read LAS","title":"3. Pipelines","text":"","code":"read_las = function(files, select = \"xyzi\", filter = \"\") {   load = function(data) { return(data) }   read = reader(files, filter = filter)   call = callback(load, expose = select, no_las_update = TRUE)   return(processor(read+call)) }"},{"path":"/articles/lasR3.html","id":"buffer-tiles","dir":"Articles","previous_headings":"Useful functions","what":"Buffer tiles","title":"3. Pipelines","text":"","code":"buffer_tiles = function(files, buffer, ofiles = paste0(tempdir(), \"/*_buffered.las\")) {   read = reader(files, buffer = buffer)   write = write_las(ofiles, keep_buffer = TRUE)   return(processor(read+write)) }"},{"path":"/articles/lasR3.html","id":"clip-circle","dir":"Articles","previous_headings":"Useful functions","what":"Clip circle","title":"3. Pipelines","text":"Writes LAS files returns data.frames","code":"clip_circle = function(files, geometry, radius, ofiles = paste0(tempdir(), \"/*_clipped.las\")) {   if (sf::st_geometry_type(geometry, FALSE) != \"POINT\")      stop(\"Expected POINT geometry type\")    coordinates <- sf::st_coordinates(geometry)   xcenter <- coordinates[,1]   ycenter <- coordinates[,2]      read = reader(files, xc = xcenter, yc = ycenter, r = radius)      if (length(ofiles) == 1L && ofiles == \"\")     stage = callback(function(data) { return(data) }, expose = \"*\", no_las_update = T)   else     stage = write_las(ofiles)      ans = processor(read+stage)   return(ans) }"},{"path":"/articles/lasR3.html","id":"crs","dir":"Articles","previous_headings":"Useful functions","what":"CRS","title":"3. Pipelines","text":"CRS sf object. cost applying hulls() virtually null.","code":"crs = function(files) {   pipeline = reader(files) + hulls()   ans = processor(pipeline)   return(sf::st_crs(ans)) }"},{"path":"/articles/lasR3.html","id":"inventory-metrics","dir":"Articles","previous_headings":"Useful functions","what":"Inventory metrics","title":"3. Pipelines","text":"Using sf object provide plot centers offering option normalize --fly returns sf object extra attributes","code":"inventory_metrics = function(f, geometry, radius, fun, normalize = FALSE) {   if (sf::st_geometry_type(geometry, FALSE) != \"POINT\")      stop(\"Expected POINT geometry type\")    coordinates <- sf::st_coordinates(geometry)   xcenter <- coordinates[,1]   ycenter <- coordinates[,2]      pipeline <- reader(f, xc = xcenter, yc = ycenter, r = radius)      if (normalize)   {     tri <- triangulate(filter = keep_ground())     trans <- transform_with_triangulation(tri, store_in_attribute = \"HAG\")     pipeline <- pipeline + tri + trans   }    pipeline <- pipeline + callback(fun, expose = \"*\")   ans <- processor(pipeline)   ans <- lapply(ans, as.data.frame)   ans <- do.call(rbind, ans)   return(cbind(geometry, ans)) }"},{"path":[]},{"path":"/articles/lasR4.html","id":"lidr","dir":"Articles","previous_headings":"Canopy Height Model","what":"lidR","title":"4. Benchmarks of lasR vs. lidR","text":"","code":"chm = rasterize_canopy(ctg, 1, p2r())"},{"path":"/articles/lasR4.html","id":"lasr","dir":"Articles","previous_headings":"Canopy Height Model","what":"lasR","title":"4. Benchmarks of lasR vs. lidR","text":"","code":"pipeline = reader(ctg) + rasterize(1, \"max\") processor(pipeline)"},{"path":[]},{"path":[]},{"path":"/articles/lasR4.html","id":"lidr-1","dir":"Articles","previous_headings":"Digital Terrain Model","what":"lidR","title":"4. Benchmarks of lasR vs. lidR","text":"","code":"dtm = rasterize_terrain(ctg, 1, tin())"},{"path":"/articles/lasR4.html","id":"lasr-1","dir":"Articles","previous_headings":"Digital Terrain Model","what":"lasR","title":"4. Benchmarks of lasR vs. lidR","text":"","code":"tri = triangulate(filter = keep_ground()) pipeline = reader(ctg, filter = keep_ground()) + tri + rasterize(1, tri) processor(pipeline)"},{"path":[]},{"path":"/articles/lasR4.html","id":"multiple-raster","dir":"Articles","previous_headings":"","what":"Multiple raster","title":"4. Benchmarks of lasR vs. lidR","text":"gain terms computation time much significant running multiple stages single pipeline files read lasR multiple times lidR. , operations executed single pass C++ level, resulting efficient memory management.","code":""},{"path":"/articles/lasR4.html","id":"lidr-2","dir":"Articles","previous_headings":"Multiple raster","what":"lidR","title":"4. Benchmarks of lasR vs. lidR","text":"","code":"custom_function = function(z,i) { list(avgz = mean(z), avgi = mean(i)) } ctg = readLAScatalog(f) chm = rasterize_canopy(ctg, 1, p2r()) met = pixel_metrics(ctg, ~custom_function(Z, Intensity), 20) den = rasterize_density(ctg, 5)"},{"path":"/articles/lasR4.html","id":"lasr-2","dir":"Articles","previous_headings":"Multiple raster","what":"lasR","title":"4. Benchmarks of lasR vs. lidR","text":"","code":"custom_function = function(z,i) { list(avgz = mean(z), avgi = mean(i)) } read = reader(folder) chm = rasterize(1, \"max\") met = rasterize(20, custom_function(Z, Intensity)) den = rasterize(5, \"count\") pipeline = read + chm + met + den processor(pipeline)"},{"path":"/articles/lasR4.html","id":"complex-pipeline","dir":"Articles","previous_headings":"","what":"Complex Pipeline","title":"4. Benchmarks of lasR vs. lidR","text":"complex pipeline, point cloud normalized written new files. Digital Terrain Model (DTM) produced, Canopy Height Model (CHM) built, individual trees detected. detected trees used seeds region-growing algorithm segments trees. lasR pipeline can handle hundreds laser tiles, lidR may struggle apply pipeline, especially tree segmentation.","code":"read = reader(ctg) del = triangulate(filter = \"-keep_class 2\") norm = transform_with_triangulation(del) dtm = rasterize(1, del) chm = rasterize(1, \"max\") seed = local_maximum(3) tree = region_growing(chm, seed) write = write_las() pipeline = read + del + norm + write + dtm + chm +  seed + tree ans = processor(pipeline, progress = TRUE)"},{"path":"/articles/lasR5.html","id":"add_attribute","dir":"Articles","previous_headings":"","what":"add_attribute","title":"5. How lidR functions map to lasR functions","text":"lidR, function add_attribute enables adding column data.frame column writable LAS file. Since data.frame lasR, package processes -disk files produces -disk files, one--one equivalent add_attribute(). closest equivalent function something like: adds extrabytes attribute, attribute zeroed. idea attributes can used later stages, , pre-built function stages.","code":"add_dimension = function(ifile, data_type, name, description, ofile = tempfile(fileext = \".las\")) {   pipeline <- reader(f) + add_extrabytes(data_type, name, description) + write_las(ofile)   processor(pipeline) }"},{"path":"/articles/lasR5.html","id":"catalog_apply","dir":"Articles","previous_headings":"","what":"catalog_apply","title":"5. How lidR functions map to lasR functions","text":"equivalent since catalog_apply() catalog_map() functions designed create pipelines lidR. lasR pipeline engine need functions. pipeline mechanism lidR difficult use.","code":""},{"path":"/articles/lasR5.html","id":"catalog_retile","dir":"Articles","previous_headings":"","what":"catalog_retile","title":"5. How lidR functions map to lasR functions","text":"currently full equivalent lasR works files create arbitrary chunks. Yet, catalog_retile() versatile, behaviours can reproduced, writing buffered tiles.","code":"buffer_tiles = function(f, buffer, ofiles = paste0(tempdir(), \"/*_buffered.las\")) {   read = reader(f, buffer = buffer)   write = write_las(ofiles, keep_buffer = TRUE)   processor(read+write) }"},{"path":"/articles/lasR5.html","id":"classify_ground","dir":"Articles","previous_headings":"","what":"classify_ground","title":"5. How lidR functions map to lasR functions","text":"lasR include built-ground classification algorithm yet, callback stage can used leverage functions R packages. lidR csf() package RCSF mcc() package RMCC.","code":"classify_ground_csf = function(     files,      smooth = FALSE,      threshold = 0.5,      resolution = 0.5,      rigidness = 1L,      iterations = 500L,      step = 0.65,      ofiles = paste0(tempdir(), \"/*_classified.las\")) {   csf = function(data, smooth, threshold, resolution, rigidness, iterations, step)   {     id = RCSF::CSF(data, smooth, threshold, resolution, rigidness, iterations, step)     class = integer(nrow(data))     class[id] = 2L     data$Classification <- class     return(data)   }      read = reader(files, buffer = 25)   classify = callback(csf, expose = \"xyz\", smooth = smooth, threshold = threshold, resolution = resolution, rigidness = rigidness, iterations = iterations, step = step)   write = write_las(ofiles)   pipeline = read + classify + write   processor(pipeline) }"},{"path":"/articles/lasR5.html","id":"classify_noise","dir":"Articles","previous_headings":"","what":"classify_noise","title":"5. How lidR functions map to lasR functions","text":", worth noting pre-built functions, possible perform ground classification noise classification, self-built pipeline, possible combine multiple stages pipeline.","code":"classify_noise_ivf = function(files, res =  5, n = 6, ofiles = paste0(tempdir(), \"/*_classified.las\")) {   processor(reader(files) + classify_isolated_points(res, n) +  write_las(ofile)) } read = reader(f)  ground = callback(csf, expose = \"xyz\", threshold = 0.4, iterations = 200) noise = classify_isolated_points() write = write_las() pipeline = read + ground + noise + write"},{"path":"/articles/lasR5.html","id":"classify_poi","dir":"Articles","previous_headings":"","what":"classify_poi","title":"5. How lidR functions map to lasR functions","text":"current equivalent","code":""},{"path":"/articles/lasR5.html","id":"clip","dir":"Articles","previous_headings":"","what":"clip","title":"5. How lidR functions map to lasR functions","text":", function implemented using sf package supports multiple options, returning R memory writing files.","code":"clip_circle = function(files, geometry, radius, ofiles = paste0(tempdir(), \"/*_clipped.las\")) {   if (sf::st_geometry_type(geometry, FALSE) != \"POINT\") stop(\"Expected POINT geometry type\")    coordinates <- sf::st_coordinates(geometry)   xcenter <- coordinates[,1]   ycenter <- coordinates[,2]      read = reader(files, xc = xcenter, yc = ycenter, r = radius)      if (length(ofiles) == 1L && ofiles == \"\")     stage = callback(function(data) { return(data) }, expose = \"*\", no_las_update = T)   else     stage = write_las(ofiles)      ans = processor(read+stage)   return(ans) }"},{"path":"/articles/lasR5.html","id":"decimate_points","dir":"Articles","previous_headings":"","what":"decimate_points","title":"5. How lidR functions map to lasR functions","text":"","code":"decimate_with_voxels = function(f, res = 2, ofiles = paste0(tempdir(), \"/*_decimated.las\")) {   read = reader(f)   sample = sampling_voxel(res)   write = write_las(ofiles)   processor(read+sample+write) } decimate_with_pixel = function(f, res = 2, ofiles = paste0(tempdir(), \"/*_decimated.las\")) {   read = reader(f)   sample = sampling_pixel(res)   write = write_las(ofiles)   processor(read+sample+write) }"},{"path":"/articles/lasR5.html","id":"filter","dir":"Articles","previous_headings":"","what":"filter","title":"5. How lidR functions map to lasR functions","text":"lasR parse R expression ReturnNumber != x thus one must use LASlib filters. stage pipeline filter. See ?lasR::filters.","code":""},{"path":"/articles/lasR5.html","id":"filter_duplicates","dir":"Articles","previous_headings":"","what":"filter_duplicates","title":"5. How lidR functions map to lasR functions","text":"lasR added -drop_duplicates command LASlib.","code":"read = reader(f, filter = drop_duplicates())"},{"path":"/articles/lasR5.html","id":"las_check","dir":"Articles","previous_headings":"","what":"las_check","title":"5. How lidR functions map to lasR functions","text":"currently implementation added later","code":""},{"path":"/articles/lasR5.html","id":"locate_trees","dir":"Articles","previous_headings":"","what":"locate_trees","title":"5. How lidR functions map to lasR functions","text":"","code":"tree_tops = function(f, ...) {   read = reader(f)   ttops = local_maximum(...)   processor(read+ttops) }"},{"path":"/articles/lasR5.html","id":"normalize_height","dir":"Articles","previous_headings":"","what":"normalize_height","title":"5. How lidR functions map to lasR functions","text":"implementation leaves choice use extrabytes attributes store height ground.","code":"normalize_elevation = function(f, extrabytes = FALSE, ofiles = paste0(tempdir(), \"/*_normalized.las\")) {   read = reader(f)   tri = triangulate()      pipeline = read + tri      if (extrabytes)   {     extra = add_extrabytes(\"int\", \"HAG\", \"Height Above Ground\")     norm = transform_with_triangulation(tri, store_in_attribute = \"HAG\")     pipeline = pipeline + extra + norm   }   else   {     norm = transform_with_triangulation(tri)     pipeline = pipeline + norm   }      pipeline = pipeline + write_las(ofiles)   processor(pipeline) }"},{"path":"/articles/lasR5.html","id":"pixel_metrics","dir":"Articles","previous_headings":"","what":"pixel_metrics","title":"5. How lidR functions map to lasR functions","text":"lasR rasterize(), exact equivalent pixel_metrics(). hood, uses aggregate(). Due non-standard evaluation, function bit trickier use inside function.","code":"raster_metrics = function(f, res, fun) {   call = substitute(fun)    env = new.env(parent=parent.frame())   read = reader(f)   rast = lasR:::aggregate_q(res, call, filter = \"\", ofile = tempfile(fileext = \".tif\"), env = env)   processor(read+rast) }"},{"path":"/articles/lasR5.html","id":"plot_metrics","dir":"Articles","previous_headings":"","what":"plot_metrics","title":"5. How lidR functions map to lasR functions","text":"computes custom metrics per inventory plots.","code":"circle_metrics = function(f, xcenter, ycenter, radius, fun) {   read = reader(f, xc = xcenter, yc = ycenter, r = radius)   metrics = callback(fun, expose = \"*\")   processor(read+metrics) }"},{"path":"/articles/lasR5.html","id":"rasterize_canopy","dir":"Articles","previous_headings":"","what":"rasterize_canopy","title":"5. How lidR functions map to lasR functions","text":", interesting notice limitation pre-built function. dataset normalized, pipeline builds Digital Elevation Model (similar lidR::rasterize_canopy()). way obtain Canopy Height Model, just like lidR. However, using low-level functions build custom pipeline, can normalize --fly pipeline.","code":"rasterize_chm_p2r = function(f, res) {   read = reader(f)   chm = rasterize(res, \"max\")   return(processor(read+chm)) }  rasterize_chm_tin = function(f, res) {   read = reader(f)   tin = triangulate()   chm = rasterize(res, tin)   return(processor(read+tin+chm)) }"},{"path":"/articles/lasR5.html","id":"rasterize_terrain","dir":"Articles","previous_headings":"","what":"rasterize_terrain","title":"5. How lidR functions map to lasR functions","text":"","code":"rasterize_dtm = function(f, res) {   read = reader(f)   tri = triangulate(filter = keep_ground())   dtm = rasterize(res, tri)   return(processor(read+tri+dtm)) }"},{"path":"/articles/lasR5.html","id":"rasterize_density","dir":"Articles","previous_headings":"","what":"rasterize_density","title":"5. How lidR functions map to lasR functions","text":"","code":"rasterize_chm_p2r = function(f, res) {   read = reader(f)   den = rasterize(res, \"count\")   return(processor(read+den)) }"},{"path":"/articles/lasR5.html","id":"readlas","dir":"Articles","previous_headings":"","what":"readLAS","title":"5. How lidR functions map to lasR functions","text":"","code":"read_las = function(f, select = \"xyzi\", filter = \"\") {   load = function(data) { return(data) }   read = reader(f)   call = callback(load, expose = select, filter, no_las_update = TRUE)   return(processor(read+call)) }"},{"path":"/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Jean-Romain Roussel. Author, maintainer, copyright holder. Martin Isenburg. Copyright holder.            author LASlib LASzip libraries BenoÃ®t St-Onge. Copyright holder.            author 'chm_prep' function","code":""},{"path":"/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Roussel J (2023). lasR: Fast Pipable Airborne LiDAR Data Tools. R package version 0.1.0, https://github.com/r-lidar/lasR.","code":"@Manual{,   title = {lasR: Fast and Pipable Airborne LiDAR Data Tools},   author = {Jean-Romain Roussel},   year = {2023},   note = {R package version 0.1.0},   url = {https://github.com/r-lidar/lasR}, }"},{"path":"/index.html","id":"lasr","dir":"","previous_headings":"","what":"Fast and Pipable Airborne LiDAR Data Tools","title":"Fast and Pipable Airborne LiDAR Data Tools","text":"R Package Fast Airborne LiDAR Data Processing package early stage development. Features functions can change without warning lasR package (pronounce laser) intent supersede lidR package, designed much efficient lidR common tasks like production CHM, DTM, tree detection segmentation large coverages. lidR intends tool box make data exploration innovation easy. lasR another hand focuses exclusively memory speed optimization common tasks make trade aspects development. ðŸ“– Read tutorial start lasR","code":""},{"path":"/index.html","id":"benchmark","dir":"","previous_headings":"","what":"Benchmark","title":"Fast and Pipable Airborne LiDAR Data Tools","text":"following benchmark compares much time RAM memory takes lasR lidR produce DTM, CHM, raster two metrics derived Z intensity. test performed 120 million points stored 4 LAZ files. details benchmark vignettes.","code":""},{"path":"/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Fast and Pipable Airborne LiDAR Data Tools","text":"currently plan releasing lasR CRAN. must compile install package Github.","code":"remotes::install_github(\"r-lidar/lasR\")"},{"path":"/index.html","id":"main-differences-with-lidr","dir":"","previous_headings":"","what":"Main differences with lidR","title":"Fast and Pipable Airborne LiDAR Data Tools","text":"Introduces concept pipelines chain multiple operations point cloud optimally missing lidR written exclusively C/C++ without single line R. Uses code lidR brings significant speed memory improvements. load point cloud data.frame. point cloud stored C++ structure exposed users. Uses GDAL instead relying terra sf flexibility C++ level. 2 strong dependencies: boost gdal. sf terra installed experience better.","code":""},{"path":"/index.html","id":"copyright-information","dir":"","previous_headings":"","what":"Copyright Information","title":"Fast and Pipable Airborne LiDAR Data Tools","text":"Â© 2007-2021 martin.isenburg@rapidlasso.com - http://rapidlasso.com Provided LGPL license modified R-compliant Jean-Romain Roussel. Â© 2023-2014 Jean-Romain Roussel Provided GPL-3 license. Â© 2008-2023 BenoÃ®t St-Onge Provided GPL-3 license.","code":""},{"path":"/reference/add_extrabytes.html","id":null,"dir":"Reference","previous_headings":"","what":"Add attributes to a LAS file â€” add_extrabytes","title":"Add attributes to a LAS file â€” add_extrabytes","text":"According LAS specifications, LAS file contains core defined attributes, XYZ coordinates, intensity, return number, , point. possible add supplementary attributes. algorithm adds extra bytes attribute points. Values zeroed. edits point cloud returns nothing.","code":""},{"path":"/reference/add_extrabytes.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Add attributes to a LAS file â€” add_extrabytes","text":"","code":"add_extrabytes(data_type, name, description, scale = 1, offset = 0)"},{"path":"/reference/add_extrabytes.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Add attributes to a LAS file â€” add_extrabytes","text":"data_type character. data type extra bytes attribute. Can \"uchar\", \"char\", \"ushort\", \"short\", \"uint\", \"int\", \"uint64\", \"int64\", \"float\", \"double\". name character. name extra bytes attribute add file. description character. short description extra bytes attribute add file (32 characters). scale, offset numeric. scale offset data. See LAS specification.","code":""},{"path":"/reference/add_extrabytes.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Add attributes to a LAS file â€” add_extrabytes","text":"","code":"f <- system.file(\"extdata\", \"Example.las\", package = \"lasR\") fun <- function(data) { data$RAND <- runif(nrow(data), 0, 100); return(data) } pipeline <- reader(f) +   add_extrabytes(\"float\", \"RAND\", \"Random numbers\") +   callback(fun, expose = \"xyz\") processor(pipeline) #> NULL"},{"path":"/reference/callback.html","id":null,"dir":"Reference","previous_headings":"","what":"Call a user-defined function on the point cloud â€” callback","title":"Call a user-defined function on the point cloud â€” callback","text":"Call user-defined function point cloud. function receives data.frame point cloud. first input must point cloud. function returns anything data.frame number points, output stored returned end. However, output data.frame number points, updates point cloud. function can, therefore, used modify point cloud using user-defined function.","code":""},{"path":"/reference/callback.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Call a user-defined function on the point cloud â€” callback","text":"","code":"callback(fun, expose = \"xyz\", ..., drop_buffer = FALSE, no_las_update = FALSE)"},{"path":"/reference/callback.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Call a user-defined function on the point cloud â€” callback","text":"fun numeric. resolution raster. expose character. Expose attributes interest save memory (see details). ... parameters function fun drop_buffer bool. false, expose point buffer. no_las_update bool. user-defined function returns data.frame, supposed update point cloud. Can disabled.","code":""},{"path":"/reference/callback.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Call a user-defined function on the point cloud â€” callback","text":"lasR, point cloud exposed R data.frame like lidR. stored internally C++ structure seen modified directly users using R code. callback function algorithm allows direct interaction point cloud copying temporarily data.frame apply user-defined function.expose: 'expose' argument specifies data actually exposed R. example, 'xyzia' means x, y, z coordinates, intensity, scan angle loaded. supported entries t - gpstime, - scan angle, - intensity, n - number returns, r - return number, c - classification, s - synthetic flag, k - keypoint flag, w - withheld flag, o - overlap flag (format 6+), u - user data, p - point source ID, e - edge flight line flag, d - direction scan flag, R - red channel RGB color, G - green channel RGB color, B - blue channel RGB color, N - near-infrared channel, C - scanner channel (format 6+) Also numbers 1 9 extra bytes data numbers 1 9. 0 enables extra bytes loaded, '*' wildcard enables everything loaded LAS file.","code":""},{"path":[]},{"path":"/reference/callback.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Call a user-defined function on the point cloud â€” callback","text":"","code":"f <- system.file(\"extdata\", \"Topography.las\", package = \"lasR\")  # There is no function in lasR to read the data in R. Let's create one read_las <- function(f) {   load <- function(data) { return(data) }   read <- reader(f)   call <- callback(load, expose = \"xyzi\", no_las_update = TRUE)   return (processor(read + call)) } las <- read_las(f) head(las) #>          X       Y        Z Intensity #> 1 273357.1 5274360 806.5340      1340 #> 2 273357.2 5274359 806.5635       728 #> 3 273357.2 5274358 806.0248      1369 #> 4 273357.2 5274510 809.6303       589 #> 5 273357.2 5274509 809.3880      1302 #> 6 273357.2 5274508 809.4847       123  convert_intensity_in_range <- function(data, min, max) {   i <- data$Intensity   i <- ((i - min(i)) / (max(i) - min(i))) * (max - min) + min   i[i < min] <- min   i[i > max] <- max   data$Intensity <- as.integer(i)   return(data) }  read <- reader(f) call <- callback(convert_intensity_in_range, expose = \"xyzi\", min = 0, max = 255) write <- write_las() pipeline <- read + call + write ans <- processor(pipeline)  las <- read_las(ans) head(las) #>          X       Y        Z Intensity #> 1 273357.1 5274360 806.5340       137 #> 2 273357.2 5274359 806.5635        72 #> 3 273357.2 5274358 806.0248       140 #> 4 273357.2 5274510 809.6303        57 #> 5 273357.2 5274509 809.3880       133 #> 6 273357.2 5274508 809.4847         7"},{"path":"/reference/chm.html","id":null,"dir":"Reference","previous_headings":"","what":"Canopy Height Model â€” chm","title":"Canopy Height Model â€” chm","text":"Create Canopy Height Model using triangulate rasterize.","code":""},{"path":"/reference/chm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Canopy Height Model â€” chm","text":"","code":"chm(res = 1, tin = FALSE, ofile = tempfile(fileext = \".tif\"))"},{"path":"/reference/chm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Canopy Height Model â€” chm","text":"res numeric. resolution raster. tin bool. default CHM point--raster based methods .e. pixel assigned elevation highest point. tin = TRUE CHM triangulation-based model. first returns triangulated interpolated. ofile character. Full outputs always stored disk. ofile = \"\" algorithm store result disk return nothing. however hold partial output results temporarily memory. useful algorithm intermediate stage.","code":""},{"path":[]},{"path":"/reference/chm.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Canopy Height Model â€” chm","text":"","code":"f <- system.file(\"extdata\", \"Topography.las\", package=\"lasR\") pipeline <- reader(f) + chm()"},{"path":"/reference/classify_isolated_points.html","id":null,"dir":"Reference","previous_headings":"","what":"Classify isolated points â€” classify_isolated_points","title":"Classify isolated points â€” classify_isolated_points","text":"algorithm identifies points points surrounding 3 x 3 x 3 = 27 voxels edits points assign target classification. Used class 18, classifies points noise similar lasnoise lastools. algorithm modifies point cloud pipeline produce output.","code":""},{"path":"/reference/classify_isolated_points.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Classify isolated points â€” classify_isolated_points","text":"","code":"classify_isolated_points(res = 5, n = 6L, class = 18L)"},{"path":"/reference/classify_isolated_points.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Classify isolated points â€” classify_isolated_points","text":"res numeric. Resolution voxels. n integer. maximal number 'points' 27 voxels. class integer. class assign points match condition.","code":""},{"path":"/reference/dtm.html","id":null,"dir":"Reference","previous_headings":"","what":"Digital Terrain Model â€” dtm","title":"Digital Terrain Model â€” dtm","text":"Create Digital Terrain Model using triangulate rasterize.","code":""},{"path":"/reference/dtm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Digital Terrain Model â€” dtm","text":"","code":"dtm(res = 1, add_class = NULL, ofile = tempfile(fileext = \".tif\"))"},{"path":"/reference/dtm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Digital Terrain Model â€” dtm","text":"res numeric. resolution raster. add_class integer. default triangulates using ground points (class 2). possible provide additional classes 9 water. ofile character. Full outputs always stored disk. ofile = \"\" algorithm store result disk return nothing. however hold partial output results temporarily memory. useful algorithm intermediate stage.","code":""},{"path":[]},{"path":"/reference/dtm.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Digital Terrain Model â€” dtm","text":"","code":"f <- system.file(\"extdata\", \"Topography.las\", package=\"lasR\") pipeline <- reader(f) + dtm()"},{"path":"/reference/filters.html","id":null,"dir":"Reference","previous_headings":"","what":"Point filters â€” filters","title":"Point filters â€” filters","text":"lasR uses LASlib/LASzip, library developed Martin Isenburg read write LAS/LAZ files. Thus, flags available LAStools also available lasR. Filters strings put filter arguments lasR algorithms. list available strings accessible filter_usage. convenience, useful filters associated function returns corresponding string.","code":""},{"path":"/reference/filters.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Point filters â€” filters","text":"","code":"keep_class(x)  drop_class(x)  keep_first()  drop_first()  keep_ground()  drop_ground()  keep_noise()  drop_noise()  keep_z_above(x)  drop_z_above(x)  keep_z_below(x)  drop_z_below(x)  drop_duplicates()  filter_usage()  # S3 method for laslibfilter print(x, ...)  # S3 method for laslibfilter +(e1, e2)"},{"path":"/reference/filters.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Point filters â€” filters","text":"x numeric integer function filter used. ... Unused. e1, e2 lasR objects.","code":""},{"path":"/reference/filters.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Point filters â€” filters","text":"","code":"f <- system.file(\"extdata\", \"Topography.las\", package=\"lasR\") filter_usage() #> Filter points based on their coordinates. #>   -keep_tile 631000 4834000 1000 (ll_x ll_y size) #>   -keep_circle 630250.00 4834750.00 100 (x y radius) #>   -keep_xy 630000 4834000 631000 4836000 (min_x min_y max_x max_y) #>   -drop_xy 630000 4834000 631000 4836000 (min_x min_y max_x max_y) #>   -keep_x 631500.50 631501.00 (min_x max_x) #>   -drop_x 631500.50 631501.00 (min_x max_x) #>   -drop_x_below 630000.50 (min_x) #>   -drop_x_above 630500.50 (max_x) #>   -keep_y 4834500.25 4834550.25 (min_y max_y) #>   -drop_y 4834500.25 4834550.25 (min_y max_y) #>   -drop_y_below 4834500.25 (min_y) #>   -drop_y_above 4836000.75 (max_y) #>   -keep_z 11.125 130.725 (min_z max_z) #>   -drop_z 11.125 130.725 (min_z max_z) #>   -drop_z_below 11.125 (min_z) #>   -drop_z_above 130.725 (max_z) #>   -keep_xyz 620000 4830000 100 621000 4831000 200 (min_x min_y min_z max_x max_y max_z) #>   -drop_xyz 620000 4830000 100 621000 4831000 200 (min_x min_y min_z max_x max_y max_z) #>   -drop_duplicates #> Filter points based on their return numbering. #>   -keep_first -first_only -drop_first #>   -keep_last -last_only -drop_last #>   -keep_second_last -drop_second_last #>   -keep_first_of_many -keep_last_of_many #>   -drop_first_of_many -drop_last_of_many #>   -keep_middle -drop_middle #>   -keep_return 1 2 3 #>   -drop_return 3 4 #>   -keep_single -drop_single #>   -keep_double -drop_double #>   -keep_triple -drop_triple #>   -keep_quadruple -drop_quadruple #>   -keep_number_of_returns 5 #>   -drop_number_of_returns 0 #> Filter points based on the scanline flags. #>   -drop_scan_direction 0 #>   -keep_scan_direction_change #>   -keep_edge_of_flight_line #> Filter points based on their intensity. #>   -keep_intensity 20 380 #>   -drop_intensity_below 20 #>   -drop_intensity_above 380 #>   -drop_intensity_between 4000 5000 #> Filter points based on classifications or flags. #>   -keep_class 1 3 7 #>   -drop_class 4 2 #>   -keep_extended_class 43 #>   -drop_extended_class 129 135 #>   -drop_synthetic -keep_synthetic #>   -drop_keypoint -keep_keypoint #>   -drop_withheld -keep_withheld #>   -drop_overlap -keep_overlap #> Filter points based on their user data. #>   -keep_user_data 1 #>   -drop_user_data 255 #>   -keep_user_data_below 50 #>   -keep_user_data_above 150 #>   -keep_user_data_between 10 20 #>   -drop_user_data_below 1 #>   -drop_user_data_above 100 #>   -drop_user_data_between 10 40 #> Filter points based on their point source ID. #>   -keep_point_source 3 #>   -keep_point_source_between 2 6 #>   -drop_point_source 27 #>   -drop_point_source_below 6 #>   -drop_point_source_above 15 #>   -drop_point_source_between 17 21 #> Filter points based on their scan angle. #>   -keep_scan_angle -15 15 #>   -drop_abs_scan_angle_above 15 #>   -drop_abs_scan_angle_below 1 #>   -drop_scan_angle_below -15 #>   -drop_scan_angle_above 15 #>   -drop_scan_angle_between -25 -23 #> Filter points based on their gps time. #>   -keep_gps_time 11.125 130.725 #>   -drop_gps_time_below 11.125 #>   -drop_gps_time_above 130.725 #>   -drop_gps_time_between 22.0 48.0 #> Filter points based on their RGB/CIR/NIR channels. #>   -keep_RGB_red 1 1 #>   -drop_RGB_red 5000 20000 #>   -keep_RGB_green 30 100 #>   -drop_RGB_green 2000 10000 #>   -keep_RGB_blue 0 0 #>   -keep_RGB_nir 64 127 #>   -keep_RGB_greenness 200 65535 #>   -keep_NDVI 0.2 0.7 -keep_NDVI_from_CIR -0.1 0.5 #>   -keep_NDVI_intensity_is_NIR 0.4 0.8 -keep_NDVI_green_is_NIR -0.2 0.2 #> Filter points based on their wavepacket. #>   -keep_wavepacket 0 #>   -drop_wavepacket 3 #> Filter points based on extra attributes. #>   -keep_attribute_above 0 5.0 #>   -drop_attribute_below 1 1.5 #> Filter points with simple thinning. #>   -keep_every_nth 2 -drop_every_nth 3 #>   -keep_random_fraction 0.1 #>   -keep_random_fraction 0.1 4711 #>   -thin_with_grid 1.0 #>   -thin_pulses_with_time 0.0001 #>   -thin_points_with_time 0.000001 #> Boolean combination of filters. #>   -filter_and gnd = keep_class(c(2,9)) reader(f, gnd) #>  ----------- #> reader_las (uid:g0wrv3) #>   files : Topography.las #>   filter : -keep_class 2 9  #>   buffer : 0  #>   output :   #> ----------- triangulate(filter = keep_ground()) #>  ----------- #> triangulate (uid:BGFFye) #>   max_edge : 0  #>   filter : -keep_class 2  #>   output :   #>   use_attribute : Z  #> ----------- rasterize(1, \"max\", filter = \"-drop_z_below 5\") #>  ----------- #> rasterize (uid:Lcy06K) #>   res : 1  #>   method : 1  #>   filter : -drop_z_below 5  #>   output : /tmp/RtmpNYJVKt/file2009473b84ae.tif  #> -----------"},{"path":"/reference/hulls.html","id":null,"dir":"Reference","previous_headings":"","what":"Contour of a Delaunay triangulation â€” hulls","title":"Contour of a Delaunay triangulation â€” hulls","text":"algorithm uses Delaunay triangulation computes contour. contour strict Delaunay triangulation convex hull, lasR, triangulation max_edge argument. Thus, contour might convex hull holes.","code":""},{"path":"/reference/hulls.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Contour of a Delaunay triangulation â€” hulls","text":"","code":"hulls(mesh = NULL, ofile = tempfile(fileext = \".gpkg\"))"},{"path":"/reference/hulls.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Contour of a Delaunay triangulation â€” hulls","text":"mesh NULL LASRalgorithm. triangulate algorithm. NULL take bounding box header file. ofile character. Full outputs always stored disk. ofile = \"\" algorithm store result disk return nothing. however hold partial output results temporarily memory. useful algorithm intermediate stage.","code":""},{"path":[]},{"path":"/reference/hulls.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Contour of a Delaunay triangulation â€” hulls","text":"","code":"f <- system.file(\"extdata\", \"Topography.las\", package = \"lasR\") read <- reader(f) tri <- triangulate(20, filter = \"-keep_class 2\") contour <- hulls(tri) pipeline <- read + tri + contour ans <- processor(pipeline) #> Warning: GDAL Message 1: Hole lies outside shell at or near point 273465.17200000002 5274357.2455000002 plot(ans)"},{"path":"/reference/lasR-package.html","id":null,"dir":"Reference","previous_headings":"","what":"lasR: airborne LiDAR for forestry applications â€” lasR-package","title":"lasR: airborne LiDAR for forestry applications â€” lasR-package","text":"lasR provides set tools process efficiently airborne LiDAR data forestry contexts. package works .las .laz files. toolbox includes algorithms DSM, CHM, DTM, ABA, normalisation, tree detection, tree segmentation, tree delineation, colourization, validation tools, well processing engine process broad LiDAR coverage split many files efficiently.","code":""},{"path":[]},{"path":"/reference/lasR-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"lasR: airborne LiDAR for forestry applications â€” lasR-package","text":"Maintainer: Jean-Romain Roussel jean-romain.roussel.1@ulaval.ca [copyright holder] contributors: Martin Isenburg (author LASlib LASzip libraries) [copyright holder] BenoÃ®t St-Onge (author 'chm_prep' function) [copyright holder]","code":""},{"path":"/reference/local_maximum.html","id":null,"dir":"Reference","previous_headings":"","what":"Local Maximum â€” local_maximum","title":"Local Maximum â€” local_maximum","text":"Local Maximum algorithm identifies points locally maximum. window size fixed circular. algorithm modify point cloud. produces derived product vector format.","code":""},{"path":"/reference/local_maximum.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Local Maximum â€” local_maximum","text":"","code":"local_maximum(   ws,   min_height = 2,   filter = \"\",   ofile = tempfile(fileext = \".gpkg\"),   use_attribute = \"Z\" )"},{"path":"/reference/local_maximum.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Local Maximum â€” local_maximum","text":"ws numeric. Diameter moving window used detect local maxima units input data (usually meters). min_height numeric. Minimum height local maximum. Threshold point local maximum. Default 2. filter 'filter' argument allows filtering point-cloud work points interest. available filters LASlib can found running `print_filters()`. given algorithm filter applied, points meet criteria processes. common strings \"-keep_first\", \"-keep_class 2\", \"drop_z_below 2\". details see filters. ofile character. Full outputs always stored disk. ofile = \"\" algorithm store result disk return nothing. however hold partial output results temporarily memory. useful algorithm intermediate stage. use_attribute character. default local maximum performed coordinate Z. Can also name extra bytes attribute 'HAG' exists. Can also 'Intensity' probably use case one.","code":""},{"path":"/reference/local_maximum.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Local Maximum â€” local_maximum","text":"","code":"f <- system.file(\"extdata\", \"MixedConifer.las\", package = \"lasR\") read <- reader(f) lmf <- local_maximum(3) ans <- processor(read + lmf) ans #> Simple feature collection with 297 features and 5 fields #> Geometry type: POINT #> Dimension:     XYZ #> Bounding box:  xmin: 481260 ymin: 3812921 xmax: 481349.8 ymax: 3813011 #> z_range:       zmin: 2.16 zmax: 32.07 #> Projected CRS: NAD83 / UTM zone 12N #> # A tibble: 297 Ã— 6 #>    Intensity gpstime ReturnNumber Classification ScanAngle #>        <int>   <dbl>        <int>          <int>     <dbl> #>  1       115 149929.            1              1        16 #>  2       136 149930.            1              1        16 #>  3        13 149930.            1              1        17 #>  4        18 149930.            1              1        17 #>  5       137 149930.            1              1        17 #>  6       152 149930.            1              1        17 #>  7       117 149930.            1              1        16 #>  8       133 149930.            1              1        16 #>  9       113 149930.            1              1        17 #> 10         1 149930.            1              1        17 #> # â„¹ 287 more rows #> # â„¹ 1 more variable: geom <POINT [m]>"},{"path":"/reference/normalize.html","id":null,"dir":"Reference","previous_headings":"","what":"Normalize the point cloud â€” normalize","title":"Normalize the point cloud â€” normalize","text":"Normalize point cloud using `triangulate()` `transform_with_triangulation()`","code":""},{"path":"/reference/normalize.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Normalize the point cloud â€” normalize","text":"","code":"normalize(extrabytes = FALSE)"},{"path":"/reference/normalize.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Normalize the point cloud â€” normalize","text":"extrabytes bool. FALSE coordinate Z point cloud modified becomes height ground (HAG). TRUE coordinate Z modified new extrabytes attribute named 'HAG' added point cloud.","code":""},{"path":[]},{"path":"/reference/normalize.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Normalize the point cloud â€” normalize","text":"","code":"f <- system.file(\"extdata\", \"Topography.las\", package=\"lasR\") pipeline <- reader(f) + normalize() + write_las()"},{"path":"/reference/pit_fill.html","id":null,"dir":"Reference","previous_headings":"","what":"Pits and spikes filling â€” pit_fill","title":"Pits and spikes filling â€” pit_fill","text":"Pits spikes filling raster. Typically used post-processing CHM. algorithm St-Onge 2008 (see reference).","code":""},{"path":"/reference/pit_fill.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Pits and spikes filling â€” pit_fill","text":"","code":"pit_fill(   raster,   lap_size = 3L,   thr_lap = 0.1,   thr_spk = -0.1,   med_size = 3L,   dil_radius = 0L,   ofile = tempfile(fileext = \".tif\") )"},{"path":"/reference/pit_fill.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Pits and spikes filling â€” pit_fill","text":"raster LASRalgorithm. algorithm produces raster. lap_size integer. Size Laplacian filter kernel (integer value, pixels). thr_lap numeric. Threshold Laplacian value detecting cavity (values value considered cavity). positive value. thr_spk numeric. Threshold Laplacian value detecting spike (values value considered spike). negative value. med_size integer. Size median filter kernel (integer value, pixels). dil_radius integer. Dilation radius (integer value, pixels). ofile character. Full outputs always stored disk. ofile = \"\" algorithm store result disk return nothing. however hold partial output results temporarily memory. useful algorithm intermediate stage.","code":""},{"path":"/reference/pit_fill.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Pits and spikes filling â€” pit_fill","text":"St-Onge, B., 2008. Methods improving quality true orthomosaic Vexcel UltraCam images created using alidar digital surface model, Proceedings Silvilaser 2008, Edinburgh, 555-562. https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=81365288221f3ac34b51a82e2cfed8d58defb10e","code":""},{"path":"/reference/pit_fill.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Pits and spikes filling â€” pit_fill","text":"","code":"f <- system.file(\"extdata\", \"MixedConifer.las\", package=\"lasR\")  reader <- reader(f, filter = \"-keep_first\") tri <- triangulate() chm <- rasterize(0.25, tri) pit <- pit_fill(chm) u <- processor(reader + tri + chm + pit)  chm <- u[[1]] sto <- u[[2]]  #terra::plot(c(chm, sto), col = lidR::height.colors(25))"},{"path":"/reference/processor.html","id":null,"dir":"Reference","previous_headings":"","what":"Process the pipeline â€” processor","title":"Process the pipeline â€” processor","text":"Process pipeline. Every functions nothing. function must called pipeline actually process point-cloud","code":""},{"path":"/reference/processor.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Process the pipeline â€” processor","text":"","code":"processor(pipeline, ncores = 1L, progress = FALSE, ...)"},{"path":"/reference/processor.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Process the pipeline â€” processor","text":"pipeline LASRpipeline. serie algorithms called order ncores integer. yet supported. progress boolean. Displays progress bar. ... unused","code":""},{"path":"/reference/processor.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Process the pipeline â€” processor","text":"","code":"if (FALSE) { f <- paste0(system.file(package=\"lasR\"), \"/extdata/bcts/\") f <- list.files(f, pattern = \"(?i)\\\\.la(s|z)$\", full.names = TRUE)  read <- reader(f, filter = \"\") tri <- triangulate(15) dtm <- rasterize(5, tri) lmf <- local_maximum(5) met <- rasterize(2, mean(Intensity)) pipeline <- read + tri + dtm + lmf + met ans <- processor(pipeline) }"},{"path":"/reference/rasterize.html","id":null,"dir":"Reference","previous_headings":"","what":"Rasterize a point cloud â€” rasterize","title":"Rasterize a point cloud â€” rasterize","text":"Rasterize point cloud using different approaches. algorithm modify point cloud. produces derived product raster format.","code":""},{"path":"/reference/rasterize.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Rasterize a point cloud â€” rasterize","text":"","code":"rasterize(   res,   operators = \"max\",   filter = \"\",   ofile = tempfile(fileext = \".tif\") )"},{"path":"/reference/rasterize.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Rasterize a point cloud â€” rasterize","text":"res numeric. resolution raster. operators Can character vector. \"min\", \"max\" \"count\" accepted. Can also rasterize triangulation input LASRalgorithm triangulation (see examples). Can also user-defined expression (see example details). filter 'filter' argument allows filtering point-cloud work points interest. available filters LASlib can found running `print_filters()`. given algorithm filter applied, points meet criteria processes. common strings \"-keep_first\", \"-keep_class 2\", \"drop_z_below 2\". details see filters. ofile character. Full outputs always stored disk. ofile = \"\" algorithm store result disk return nothing. however hold partial output results temporarily memory. useful algorithm intermediate stage.","code":""},{"path":"/reference/rasterize.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Rasterize a point cloud â€” rasterize","text":"operators user-defined expression, function must return either vector numbers list atomic numbers. assign band name raster vector list must named. valid operators:","code":"f = function(x) { return(mean(x)) } g = function(x,y) { return(c(avg = mean(x), med = median(y))) } h = function(x) { return(list(a = mean(x), b = median(x))) } rasterize(10, f(Intensity)) rasterize(10, g(Z, Intensity)) rasterize(10, h(Z))"},{"path":"/reference/rasterize.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Rasterize a point cloud â€” rasterize","text":"","code":"f <- system.file(\"extdata\", \"Topography.las\", package=\"lasR\") read <- reader(f) tri  <- triangulate(filter = \"-keep_class 2\") dtm  <- rasterize(1, tri) # input is a triangulation algorithm avgi <- rasterize(10, mean(Intensity)) # input is a user expression chm  <- rasterize(2, \"max\") # input is a character vector pipeline <- read + tri + dtm + avgi + chm ans <- processor(pipeline) ans[[1]] #> class       : SpatRaster  #> dimensions  : 286, 286, 1  (nrow, ncol, nlyr) #> resolution  : 1, 1  (x, y) #> extent      : 273357, 273643, 5274357, 5274643  (xmin, xmax, ymin, ymax) #> coord. ref. : NAD83(CSRS) / MTM zone 7 (EPSG:2949)  #> source      : file2009680b4117.tif  #> name        : file2009680b4117  ans[[2]] #> class       : SpatRaster  #> dimensions  : 30, 30, 1  (nrow, ncol, nlyr) #> resolution  : 10, 10  (x, y) #> extent      : 273350, 273650, 5274350, 5274650  (xmin, xmax, ymin, ymax) #> coord. ref. : NAD83(CSRS) / MTM zone 7 (EPSG:2949)  #> source      : file200949acb5ce.tif  #> name        : file200949acb5ce  ans[[3]] #> class       : SpatRaster  #> dimensions  : 144, 144, 1  (nrow, ncol, nlyr) #> resolution  : 2, 2  (x, y) #> extent      : 273356, 273644, 5274356, 5274644  (xmin, xmax, ymin, ymax) #> coord. ref. : NAD83(CSRS) / MTM zone 7 (EPSG:2949)  #> source      : file2009786893f1.tif  #> name        : max"},{"path":"/reference/reader.html","id":null,"dir":"Reference","previous_headings":"","what":"Initialize the pipeline â€” reader","title":"Initialize the pipeline â€” reader","text":"first stage must called pipeline. specifies files must read. stage nothing returns nothing assiciated another processing stage. initializes pipeline. reader() main function dispatches functions. reader_file_*() reads LAS/LAZ files disk. reader_dataframe() can put R data.frame pipeline. read_*_coverage processed entire point cloud. reader_*_circles() read_*_rectangles() read process selected regions interest.","code":""},{"path":"/reference/reader.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Initialize the pipeline â€” reader","text":"","code":"reader(x, filter = \"\", buffer = 0, ...)  reader_coverage(x, filter = \"\", buffer = 0)  reader_circles(x, xc, yc, r, filter = \"\", buffer = 0)  reader_rectangles(x, xmin, ymin, xmax, ymax, filter = \"\", buffer = 0)"},{"path":"/reference/reader.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Initialize the pipeline â€” reader","text":"x path files use. path folder files stored. data.frame. Supports also LAScatalog LAS objects lidR. filter 'filter' argument allows filtering point-cloud work points interest. available filters LASlib can found running `print_filters()`. given algorithm filter applied, points meet criteria processes. common strings \"-keep_first\", \"-keep_class 2\", \"drop_z_below 2\". details see filters. buffer numeric. file read buffer. default 0, mean file buffered. means internal routine knows buffer needed pick greatest value internal suggestion provided value. ... passed readers xc, yc, r numeric. Circle centres radius radii. xmin, ymin, xmax, ymax numeric. Coordinates rectangles","code":""},{"path":"/reference/reader.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Initialize the pipeline â€” reader","text":"","code":"f <- system.file(\"extdata\", \"Topography.las\", package = \"lasR\") read <- reader(f) ans <- processor(read)"},{"path":"/reference/region_growing.html","id":null,"dir":"Reference","previous_headings":"","what":"Region growing â€” region_growing","title":"Region growing â€” region_growing","text":"Region growing individual tree segmentation based Dalponte Coomes (2016) algorithm (see reference). Note algorithm strictly performs segmentation, original method described manuscript also performs pre- post-processing tasks. , tasks expected done user separate functions.","code":""},{"path":"/reference/region_growing.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Region growing â€” region_growing","text":"","code":"region_growing(   raster,   seeds,   th_tree = 2,   th_seed = 0.45,   th_cr = 0.55,   max_cr = 20,   ofile = tempfile(fileext = \".tif\") )"},{"path":"/reference/region_growing.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Region growing â€” region_growing","text":"raster LASRalgoritm algorithm producing raster. seeds LASRalgoritm algorithm producing points used seeds. th_tree numeric. Threshold pixel tree. Default 2. th_seed numeric. Growing threshold 1. See reference Dalponte et al. 2016. pixel added region height greater tree height multiplied value. 0 1. Default 0.45. th_cr numeric. Growing threshold 2. See reference Dalponte et al. 2016. pixel added region height greater current mean height region multiplied value. 0 1. Default 0.55. max_cr numeric. Maximum value crown diameter detected tree (data units). Default 20. CAREFUL algorithm exists lidR package parameter pixels lidR. ofile character. Full outputs always stored disk. ofile = \"\" algorithm store result disk return nothing. however hold partial output results temporarily memory. useful algorithm intermediate stage.","code":""},{"path":"/reference/region_growing.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Region growing â€” region_growing","text":"Dalponte, M. Coomes, D. . (2016), Tree-centric mapping forest carbon density airborne laser scanning hyperspectral data. Methods Ecol Evol, 7: 1236â€“1245. doi:10.1111/2041-210X.12575.","code":""},{"path":"/reference/region_growing.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Region growing â€” region_growing","text":"","code":"f <- system.file(\"extdata\", \"MixedConifer.las\", package=\"lasR\")  reader <- reader(f, filter = \"-keep_first\") reader <- reader(f, filter = \"-keep_first\") chm <- rasterize(1, \"max\") lmx <- local_maximum(5) tree <- region_growing(chm, lmx, max_cr = 10) u <- processor(reader + chm + lmx + tree)"},{"path":"/reference/sampling.html","id":null,"dir":"Reference","previous_headings":"","what":"Sample the point cloud keeping one random point per units â€” sampling_voxel","title":"Sample the point cloud keeping one random point per units â€” sampling_voxel","text":"Sample point cloud, keeping one random point per pixel per voxel. algorithm modifies point cloud pipeline produce output.","code":""},{"path":"/reference/sampling.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Sample the point cloud keeping one random point per units â€” sampling_voxel","text":"","code":"sampling_voxel(res = 2, filter = \"\")  sampling_pixel(res = 2, filter = \"\")"},{"path":"/reference/sampling.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Sample the point cloud keeping one random point per units â€” sampling_voxel","text":"res numeric. voxel resolution filter 'filter' argument allows filtering point-cloud work points interest. available filters LASlib can found running `print_filters()`. given algorithm filter applied, points meet criteria processes. common strings \"-keep_first\", \"-keep_class 2\", \"drop_z_below 2\". details see filters.","code":""},{"path":"/reference/sampling.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Sample the point cloud keeping one random point per units â€” sampling_voxel","text":"","code":"f <- system.file(\"extdata\", \"Topography.las\", package=\"lasR\") read <- reader(f) vox <- sampling_voxel(5) write <- write_las() pipeline <- read + vox + write processor(pipeline) #> [1] \"/tmp/RtmpNYJVKt/Topography.las\""},{"path":"/reference/summarise.html","id":null,"dir":"Reference","previous_headings":"","what":"Summary â€” summarise","title":"Summary â€” summarise","text":"Summarize dataset counting number points, first returns, classes. also produces histogram Z Intensity. algorithm modify point cloud. produces summary `list`.","code":""},{"path":"/reference/summarise.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Summary â€” summarise","text":"","code":"summarise(zwbin = 2, iwbin = 25, filter = \"\")"},{"path":"/reference/summarise.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Summary â€” summarise","text":"zwbin, iwbin numeric. Width bins histograms Z Intensity. filter 'filter' argument allows filtering point-cloud work points interest. available filters LASlib can found running `print_filters()`. given algorithm filter applied, points meet criteria processes. common strings \"-keep_first\", \"-keep_class 2\", \"drop_z_below 2\". details see filters.","code":""},{"path":"/reference/summarise.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Summary â€” summarise","text":"","code":"f <- system.file(\"extdata\", \"Topography.las\", package=\"lasR\") read <- reader(f) pipeline <- read + summarise() ans <- processor(pipeline) ans #> $npoints #> [1] 73403 #>  #> $nsingle #> [1] 31294 #>  #> $nwithheld #> [1] 0 #>  #> $nsynthetic #> [1] 0 #>  #> $npoints_per_return #>     1     2     3     4     5     6  #> 53538 15828  3569   451    16     1  #>  #> $npoints_per_class #>     1     2     9  #> 61347  8159  3897  #>  #> $z_histogram #>   788   790   792   794   796   798   800   802   804   806   808   810   812  #>     1   163   265   470   596   694  1610  4955  5510 13833  9974  9865  8076  #>   814   816   818   820   822   824   826   828   830  #>  6643  4682  2958  1715   830   390   146    26     1  #>  #> $i_histogram #>   50   75  100  125  150  175  200  225  250  275  300  325  350  375  400  425  #>    8   51  168  422  485  677  697 1293 1453 1312 1337 1132 1145 1111 1003  962  #>  450  475  500  525  550  575  600  625  650  675  700  725  750  775  800  825  #> 1223 1286 1328 1198 1129 1108 1345 1293 1205 1218 1348 1399 1249 1212 1363 1327  #>  850  875  900  925  950  975 1000 1025 1050 1075 1100 1125 1150 1175 1200 1225  #> 1395 1469 1394 1419 1426 1571 1627 1564 1646 1734 1772 1827 1695 1709 1600 1411  #> 1250 1275 1300 1325 1350 1375 1400 1425 1450 1475 1500 1525 1550 1575 1600 1625  #> 1339 1192 1245 1409 1848 1887 1939 1428  966  544  250  132   91   58   54   52  #> 1650 1675 1700 1725 1750 1775 1800 1825 1850 1875 1900 1925 1950 1975 2000 2025  #>   46   40   30   29   14   14    5    6    5    7    4    6    6    1    4    1  #> 2050 2075 2100 2125 2150 2175 2200 2225 2250 2275 2300 2325 2350 2375 2400 2425  #>    0    1    0    0    0    0    2    1    0    0    0    0    0    0    0    0  #> 2450  #>    1  #>"},{"path":"/reference/tools.html","id":null,"dir":"Reference","previous_headings":"","what":"Tools inherited from base R â€” tools","title":"Tools inherited from base R â€” tools","text":"Tools inherited base R","code":""},{"path":"/reference/tools.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Tools inherited from base R â€” tools","text":"","code":"# S3 method for LASRalgorithm print(x, ...)  # S3 method for LASRpipeline print(x, ...)  # S3 method for LASRpipeline +(e1, e2)  # S3 method for LASRpipeline c(...)"},{"path":"/reference/tools.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Tools inherited from base R â€” tools","text":"x, e1, e2 lasR objects ... lasR objects. equivalent +","code":""},{"path":"/reference/tools.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Tools inherited from base R â€” tools","text":"","code":"algo1 <- rasterize(1, \"max\") algo2 <- rasterize(4, \"min\") print(algo1) #>  ----------- #> rasterize (uid:jVqMW7) #>   res : 1  #>   method : 1  #>   filter :   #>   output : /tmp/RtmpNYJVKt/file2009269b677c.tif  #> ----------- pipeline <- algo1 + algo2 print(pipeline) #>  ----------- #> rasterize (uid:jVqMW7) #>   res : 1  #>   method : 1  #>   filter :   #>   output : /tmp/RtmpNYJVKt/file2009269b677c.tif  #> ----------- #> rasterize (uid:Ry3CY9) #>   res : 4  #>   method : 2  #>   filter :   #>   output : /tmp/RtmpNYJVKt/file20095c737929.tif  #> -----------"},{"path":"/reference/transform_with_triangulation.html","id":null,"dir":"Reference","previous_headings":"","what":"Transform a point cloud using a triangulation â€” transform_with_triangulation","title":"Transform a point cloud using a triangulation â€” transform_with_triangulation","text":"algorithm uses Delaunay triangulation , point, linearly interpolates triangulation retrieve value mesh exact location point. performs operation value modify point cloud. can typically used build normalization algorithm. algorithm modifies point cloud pipeline produce output.","code":""},{"path":"/reference/transform_with_triangulation.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Transform a point cloud using a triangulation â€” transform_with_triangulation","text":"","code":"transform_with_triangulation(   triangulator,   operator = \"-\",   store_in_attribute = \"\" )"},{"path":"/reference/transform_with_triangulation.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Transform a point cloud using a triangulation â€” transform_with_triangulation","text":"triangulator LASRpipeline. 'triangulate' algorithm. operator string. '-' '+' supported. store_in_attribute numeric. Use extra bytes attribute store result.","code":""},{"path":[]},{"path":"/reference/transform_with_triangulation.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Transform a point cloud using a triangulation â€” transform_with_triangulation","text":"","code":"f <- system.file(\"extdata\", \"Topography.las\", package=\"lasR\")  # There is a normalize pipeline in lasR but let's create one almost equivalent mesh  <- triangulate(filter = keep_ground()) trans <- transform_with_triangulation(mesh) pipeline <- reader(f) + mesh + trans + write_las() ans <- processor(pipeline) #> Warning: 160 points outside delaunay triangulation were discarded"},{"path":"/reference/triangulate.html","id":null,"dir":"Reference","previous_headings":"","what":"Delaunay triangulation â€” triangulate","title":"Delaunay triangulation â€” triangulate","text":"Delaunay triangulation. Can used build DTM, CHM, normalize point cloud, application. algorithm typically used intermediate process without output file. algorithm modify point cloud.","code":""},{"path":"/reference/triangulate.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Delaunay triangulation â€” triangulate","text":"","code":"triangulate(max_edge = 0, filter = \"\", ofile = \"\", use_attribute = \"Z\")"},{"path":"/reference/triangulate.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Delaunay triangulation â€” triangulate","text":"max_edge numeric. Maximum edge length triangle Delaunay triangulation. triangle edge length greater value, removed. max_edge = 0, trimming done (see examples). filter 'filter' argument allows filtering point-cloud work points interest. available filters LASlib can found running `print_filters()`. given algorithm filter applied, points meet criteria processes. common strings \"-keep_first\", \"-keep_class 2\", \"drop_z_below 2\". details see filters. ofile character. Full outputs always stored disk. ofile = \"\" algorithm store result disk return nothing. however hold partial output results temporarily memory. useful algorithm intermediate stage. use_attribute character. default triangulation performed coordinate Z. Can also name extra bytes attribute 'HAG' exists. Can also 'Intensity'.","code":""},{"path":"/reference/triangulate.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Delaunay triangulation â€” triangulate","text":"","code":"f <- system.file(\"extdata\", \"Topography.las\", package=\"lasR\") read <- reader(f) tri1 <- triangulate(25, filter = \"-keep_class 2\", ofile = tempfile(fileext = \".gpkg\")) filter <- \"-keep_last -keep_random_fraction 0.1\" tri2 <- triangulate(filter = filter, ofile = tempfile(fileext = \".gpkg\")) pipeline <- read + tri1 + tri2 ans <- processor(pipeline) #plot(ans[[1]]) #plot(ans[[2]])"},{"path":"/reference/write_las.html","id":null,"dir":"Reference","previous_headings":"","what":"Write LAS or LAZ files â€” write_las","title":"Write LAS or LAZ files â€” write_las","text":"Write LAS LAZ file step pipeline (typically end). Unlike algorithms, output written single large file multiple tiled files corresponding original collection files.","code":""},{"path":"/reference/write_las.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Write LAS or LAZ files â€” write_las","text":"","code":"write_las(   ofile = paste0(tempdir(), \"/*.las\"),   filter = \"\",   keep_buffer = FALSE )"},{"path":"/reference/write_las.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Write LAS or LAZ files â€” write_las","text":"ofile character. Output file names. string must contain wildcard * wildcard can replaced algorithm name original tile preserve tiling pattern. wildcard omitted, everything written single file. may desired behaviour circumstances, e.g., merge files. filter 'filter' argument allows filtering point-cloud work points interest. available filters LASlib can found running `print_filters()`. given algorithm filter applied, points meet criteria processes. common strings \"-keep_first\", \"-keep_class 2\", \"drop_z_below 2\". details see filters. keep_buffer bool. buffer removed write file can preserved.","code":""},{"path":"/reference/write_las.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Write LAS or LAZ files â€” write_las","text":"","code":"f <- system.file(\"extdata\", \"Topography.las\", package=\"lasR\") read <- reader(f) tri  <- triangulate(filter = \"-keep_class 2\") normalize <- tri + transform_with_triangulation(tri) pipeline <- read + normalize + write_las(paste0(tempdir(), \"/*_norm.las\")) processor(pipeline) #> Warning: 160 points outside delaunay triangulation were discarded #> [1] \"/tmp/RtmpNYJVKt/Topography_norm.las\""}]
